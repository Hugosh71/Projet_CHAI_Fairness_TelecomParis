# ::id 0
# ::snt In the ensuing discussion, we note the limitations and assess the implications of this ethical framework for future efforts to create laws, rules, technical standards, and best practices for ethical AI in a wide range of contexts.Keywords: accountability, autonomy, artificial intelligence, beneficence, ethics, explicability, fairness, intelligibility, justice, non-maleficence.1.
(a / and
      :li 1
      :op1 (n / note-01
            :ARG0 (w / we)
            :ARG1 (l / limit-01
                  :ARG0 (f / framework
                        :mod (e / ethics)
                        :mod (t / this))))
      :op2 (a2 / assess-01
            :ARG0 w
            :ARG1 (ii / implicate-01
                  :ARG1 f
                  :ARG2 (e2 / effort-01
                        :ARG1 (c / create-01
                              :ARG1 (a3 / and
                                    :op1 (l2 / law)
                                    :op2 (r / rule)
                                    :op3 (s / standard
                                          :mod (t2 / technical))
                                    :op4 (p / practice-01
                                          :ARG1 (ii2 / intelligent-01
                                                :mod (a4 / artificial))
                                          :ARG1-of (h / have-degree-91
                                                :ARG2 (g / good-02
                                                      :ARG1 p)
                                                :ARG3 (m / most))))
                              :location (c2 / context
                                    :quant (r2 / range
                                          :ARG1-of (w2 / wide-02)))))))
      :time (d / discuss-01
            :ARG1-of (e3 / ensue-01))
      :example (a5 / and
            :op1 (a6 / accountable-02)
            :op2 (a7 / autonomy)
            :op3 ii2
            :op4 (b / beneficence)
            :op5 (e4 / ethics)
            :op6 (p2 / possible-01
                  :ARG1 (e5 / explain-01))
            :op7 (f2 / fair-01)
            :op8 (ii3 / intelligible)
            :op9 (j / justice)
            :op10 (m2 / malice
                  :polarity -)))


# ::id 0
# ::snt It also warns against the risk of bias in datasets used to train AI systems, and – unique among the documents – argues for the need to defend against threats to “solidarity,” including “systems of mutual assistance such as in social insurance and healthcare.” Elsewhere ‘justice’ has still other meanings (especially in the sense of fairness), variously relating to the use of AI to correct past wrongs such as eliminating unfair discrimination, promoting diversity, and preventing the rise of new threats to justice.
(m / multi-sentence
      :snt1 (a / and
            :op1 (w / warn-01
                  :ARG0 (ii / it)
                  :ARG1 (r / risk-01
                        :ARG1 (b / bias-01
                              :ARG1 (d / dataset
                                    :ARG1-of (u / use-01
                                          :ARG2 (t / train-01
                                                :ARG1 (s / system
                                                      :mod (ii2 / intelligent-01
                                                            :mod (a2 / artificial))))))))
                  :mod (a3 / also))
            :op2 (a4 / argue-01
                  :ARG0 ii
                  :ARG1 (n / need-01
                        :ARG1 (d2 / defend-01
                              :ARG3 (t2 / threaten-01
                                    :ARG1 (s2 / solidarity)
                                    :ARG2-of (ii3 / include-01
                                          :ARG1 (s3 / system
                                                :ARG0-of (a5 / assist-01
                                                      :manner (m2 / mutual))
                                                :example (a6 / and
                                                      :op1 (ii4 / insure-02
                                                            :mod (s4 / social))
                                                      :op2 (h / healthcare)))))))
                  :ARG1-of (u2 / unique-01
                        :ARG2 (d3 / document))))
      :snt2 (m3 / mean-01
            :ARG1 (j / justice)
            :ARG2 (o / other)
            :mod (s5 / still)
            :ARG1-of (r2 / relate-01
                  :ARG2 (u3 / use-01
                        :ARG1 (ii5 / intelligent-01
                              :mod a2))
                  :ARG2 (c / correct-01
                        :ARG1 (w2 / wrong-01
                              :time (p / past)
                              :example (a7 / and
                                    :op1 (e / eliminate-01
                                          :ARG1 (d4 / discriminate-02
                                                :ARG1-of (f / fair-01
                                                      :polarity -)))
                                    :op2 (p2 / promote-02
                                          :ARG1 (d5 / diversity))
                                    :op3 (p3 / prevent-01
                                          :ARG1 (r3 / rise-01
                                                :ARG1 (t3 / threaten-01
                                                      :ARG1 j)
                                                :ARG1-of (n2 / new-01)))))))
            :manner (v / various))
      :location (e2 / elsewhere))


# ::id 12
# ::snt Considering ethical  concepts such as justice, fairness, transparency, and accountability allows for valuable debate about the  societal impacts of AI, and the role of AI in our lives.52  There is also an academic research community  devoted to addressing ethical issues.53  Ethics have helped those researching and developing AI to define  boundaries for themselves.
(m / multi-sentence
      :snt1 (a / allow-01
            :li 52
            :ARG0 (c / consider-02
                  :ARG1 (c2 / concept
                        :mod (e / ethics)
                        :example (a2 / and
                              :op1 (j / justice)
                              :op2 (f / fairness)
                              :op3 (t / transparency)
                              :op4 (a3 / accountable-02))))
            :ARG1 (d / debate-01
                  :ARG1 (a4 / and
                        :op1 (ii / impact-01
                              :ARG0 (ii2 / intelligent-01
                                    :mod (a5 / artificial))
                              :ARG1 (s / society))
                        :op2 (r / role
                              :poss ii2
                              :topic (l / live-01
                                    :ARG0 (w / we))))
                  :ARG1-of (v / value-02)))
      :snt2 (d2 / devote-01
            :li 53
            :ARG1 (c3 / community
                  :mod (r2 / research-01
                        :mod (a6 / academia)))
            :ARG2 (a7 / address-02
                  :ARG0 c3
                  :ARG1 (ii3 / issue-02
                        :ARG0 (e2 / ethics)))
            :mod (a8 / also))
      :snt3 (h / help-01
            :li 53
            :ARG0 (e3 / ethics)
            :ARG1 (d3 / define-01
                  :ARG0 (p / person
                        :ARG0-of (r3 / research-01
                              :ARG1 (ii4 / intelligent-01
                                    :mod a5)))
                  :ARG0-of (d4 / develop-02
                        :ARG1 ii4))
            :ARG1 (b / boundary)
            :beneficiary p)
      :ARG2 p)


# ::id 12
# ::snt 52  https://www.considerati.com/publications/blog/marrying-ethics-human-rights-ai-scrutiny/ 53  The Fairness, Accountability and Transparency in Machine Learning initiative: https://www.fatml.org/ 54  For each policy see -- Microsoft: https://www.microsoft.com/en-us/ai/our-approach-to-ai, Google: https://www.blog.google/technology/ai/aiprinciples/, DeepMind: https://deepmind.com/applied/deepmind-ethics-society/principles/ 
accessnow.org18 HUMAN RIGHTS IN THE AGE OF ARTIFICIAL INTELLIGENCEHOW AI IMPACTS HUMAN RIGHTS The role of AI in facilitating discrimination is well documented, and is one of the key issues in the ethics  debate today.
(m / multi-sentence
      :snt1 (a / and
            :li 52
            :op1 (u / url-entity
                  :value "https://www.considerati.com/publications/blog/marrying-ethics-human-rights-ai-scrutiny/")
            :op2 (u2 / url-entity
                  :value 53
                  :topic (ii / initiative
                        :name (n / name
                              :op1 "The"
                              :op2 "Fairness"
                              :op3 ","
                              :op4 "Accountability"
                              :op5 "and"
                              :op6 "Transparency"
                              :op7 "in"
                              :op8 "Machine"
                              :op9 "Learning"))))
      :snt2 (s / see-01
            :mode imperative
            :ARG0 (y / you)
            :ARG1 (a2 / and
                  :op1 (c / company
                        :name (n2 / name
                              :op1 "Microsoft")
                        :medium (u3 / url-entity
                              :value "https://www.microsoft.com/en-us/ai/our-approach-to-ai"))
                  :op2 c
                  :name (n3 / name
                        :op1 "DeepMind")
                  :medium (u4 / url-entity
                        :value "https://www.google/technology/ai/aiprinciples/"))
            :op3 (u5 / url-entity
                  :value "https://www.fatml.com/applied/deepmind-ethics-society/principles/"))
      :topic (p / policy-01
            :mod (e / each))
      :snt3 (r / right-05
            :ARG1 (h / human)
            :time (a3 / age
                  :mod (a4 / artificial))
            :manner (t / thing
                  :manner-of (ii2 / impact-01
                        :ARG0 (ii3 / intelligent-01
                              :mod a4))
                  :ARG1 r))
      :ARG1-of (d / document-01
            :ARG1-of (w / well-09))
      :ARG1-of (ii4 / include-91
            :ARG2 (ii5 / issue-02
                  :ARG1-of (k / key-02
                        :ARG2 (d2 / debate-01
                              :ARG1 (e2 / ethics)
                              :time (t2 / today))))))


# ::id 12
# ::snt 28 This is broadly known at the FATML community, “Fairness, Accountability and Transparency for Machine Learning.” See https://www.fatml.org/ for              more info.
(m / multi-sentence
      :snt1 (k / know-02
            :li 28
            :ARG0 (c / community
                  :mod (o / organization
                        :name (n / name
                              :op1 "FATML")))
            :ARG1 (t / this
                  :ARG1-of (m2 / mean-01
                        :ARG2 (a / and
                              :op1 (f / fairness)
                              :op2 (a2 / accountable-02)
                              :op3 (t2 / transparency)
                              :beneficiary (l / learn-01
                                    :mod (m3 / machine)))))
            :manner (b / broad))
      :snt2 (s / see-01
            :mode imperative
            :ARG0 (y / you)
            :ARG1 (u / url-entity
                  :value "https://www.fatml.org/")
            :ARG2 (ii / information
                  :mod (m4 / more))))


# ::id 12
# ::snt 24  https://en.oxforddictionaries.com/definition/bias 25  For a deeper discussion on statistical bias and fairness issues in AI, see talk by Princeton Computer Scientist Arving Narayanan: https://www.youtube.
(s / see-01
      :li 24
      :ARG0 (y / you)
      :ARG1 (t / talk-01
            :ARG0 (p / person
                  :name (n / name
                        :op1 "Arving"
                        :op2 "Narayanan")
                  :ARG0-of (h / have-org-role-91
                        :ARG1 (u / university
                              :name (n2 / name
                                    :op1 "Princeton"))
                        :ARG2 (s2 / scientist
                              :mod (c / computer)))))
      :medium (u2 / url-entity
            :value "https://en.oxforddictionaries.com/definition/bias-25")
      :purpose (d / discuss-01
            :ARG1 (a / and
                  :op1 (b / bias-01
                        :mod (s3 / statistics))
                  :op2 (ii / issue-02
                        :ARG0 (f / fairness))
                  :topic (ii2 / intelligent-01
                        :mod (a2 / artificial)))
            :ARG1-of (h2 / have-degree-91
                  :ARG2 (d2 / deep-03
                        :ARG1 d)
                  :ARG3 (m / more))))


# ::id 14
# ::snt Diversity, non-discrimination, and fairness  6.
(a / and
      :li 6
      :op1 (d / diversity)
      :op2 (d2 / discriminate-01
            :polarity -)
      :op3 (f / fairness))


# ::id 14
# ::snt A  number of these documents mention trustworthiness as a  desirable characteristic of AI system, and according to ​ Reuters ​,  the “Trump administration said agencies should ‘promote  trustworthy AI’ and ‘must consider fairness,  non-discrimination, openness, transparency, safety, and  security’”​29 ​.
(a / and
      :li 29
      :op1 (m / mention-01
            :ARG0 (d / document
                  :mod (t / this)
                  :quant (n / number))
            :ARG1 (t2 / trustworthy
                  :ARG2-of (c / characteristic-02
                        :ARG1 (s / system
                              :mod (ii / intelligent-01
                                    :mod (a2 / artificial)))
                        :ARG1-of (d2 / desirable-02))))
      :op2 (s2 / say-01
            :ARG0 (p / publication
                  :name (n2 / name
                        :op1 "Reuters"))
            :ARG1 s2
            :ARG0 (g / government-organization
                  :ARG0-of (a3 / administrate-01)
                  :mod (p2 / person
                        :name (n3 / name
                              :op1 "Trump")))
            :ARG1 (a4 / and
                  :op1 (r / recommend-01
                        :ARG1 (p3 / promote-02
                              :ARG0 (a5 / agency)
                              :ARG1 (t3 / trustworthy
                                    :domain ii))
                        :ARG2 a5)
                  :op2 (o / obligate-01
                        :ARG1 a5
                        :ARG2 (c2 / consider-02
                              :ARG0 a5
                              :ARG1 (a6 / and
                                    :op1 (f / fairness)
                                    :op2 (d3 / discriminate-01
                                          :polarity -)
                                    :op3 (o2 / openness)
                                    :op4 (t4 / transparency)
                                    :op5 (s3 / safe-01)
                                    :op6 (s4 / security)))))))


# ::id 14
# ::snt Fujitsu, as part of its focus on human-centric ICT, has outlined principles for  human-centric AI, which mention that the company “will seek trustworthy AI through  considering fairness and safety to prevent discrimination and harm”.
(o / outline-01
      :ARG0 (c / company
            :name (n / name
                  :op1 "Fujitsu"))
      :ARG1 (p / principle
            :topic (ii / intelligent-01
                  :mod (a / artificial)
                  :ARG1-of (c2 / center-01
                        :ARG2 (h / human)))
            :ARG0-of (m / mention-01
                  :ARG1 (s / seek-01
                        :ARG0 c
                        :ARG1 ii
                        :manner (c3 / consider-02
                              :ARG0 c
                              :ARG1 (a2 / and
                                    :op1 (f / fairness)
                                    :op2 (s2 / safe-01))
                              :purpose (p2 / prevent-01
                                    :ARG0 c
                                    :ARG1 (a3 / and
                                          :op1 (d / discriminate-02)
                                          :op2 (h2 / harm-01))))
                        :mod (t / trustworthy))))
      :part-of (f2 / focus-01
            :ARG0 c
            :ARG2 (t2 / technology
                  :mod (ii2 / information)
                  :mod (c4 / cyber))))


# ::id 14
# ::snt In July 2019, Vodafone also launched its Artificial  Intelligence Framework ​43 ​, which is based on the idea of Trustworthy AI, and aims to promote:  transparency and accountability; ethics and fairness; privacy and security; human rights,  diversity, and inclusion; and to ensure an equitable transition to AI and contribute to building  an inclusive digital society.
(l / launch-01
      :ARG0 (c / company
            :name (n / name
                  :op1 "Vodafone"))
      :ARG1 (o / organization
            :name (n2 / name
                  :op1 "Artificial"
                  :op2 "Intelligence"
                  :op3 "Framework"
                  :op4 43)
            :ARG1-of (b / base-02
                  :ARG2 (ii / idea
                        :topic (ii2 / intelligent-01
                              :mod (a / artificial)
                              :ARG1-of (d / deserve-01))))
            :ARG0-of (a2 / aim-01
                  :ARG1 (a3 / and
                        :op1 (p / promote-02
                              :ARG0 o
                              :ARG1 (a4 / and
                                    :op1 (t / transparency)
                                    :op2 (a5 / accountable-02)
                                    :op3 (e / ethics)
                                    :op4 (f / fairness)
                                    :op5 (p2 / privacy)
                                    :op6 (s / security)
                                    :op7 (r / right-05
                                          :ARG1 (h / human))
                                    :op8 (d2 / diversity)
                                    :op9 (ii3 / include-01))))
                  :op2 (e2 / ensure-01
                        :ARG0 o
                        :ARG1 (t2 / transition-01
                              :ARG2 ii2
                              :mod (e3 / equitable)))
                  :op3 (c2 / contribute-01
                        :ARG0 o
                        :ARG2 (b2 / build-01
                              :ARG0 o
                              :ARG1 (s2 / society
                                    :mod (d3 / digital)
                                    :ARG0-of (ii4 / include-01))))))
      :mod (a6 / also)
      :time (d4 / date-entity
            :month 7
            :year 2019))


# ::id 14
# ::snt In a study from Harvard  University’s Berkman Klein Center for Internet & Society, ​Principled Artificial Intelligence:  Mapping Consensus in Ethical and Rights-based Approaches to Principles for AI ​105 ​, the authors  analysed a sample of 36 sets of AI principles, and note a “convergence” around eight key  themes the various sets of principles share:  ●Privacy  ●Accountability  ●Safety and Security  ●Transparency and Explainability  ●Fairness and Non-discrimination  ●Human Control of Technology  ●Professional Responsibility  ●Promotion of Human Values  Although the authors note that these principles could be seen to represent a “normative”  core, they caution against drawing any overly optimistic conclusions from this apparent  convergence, noting that the impact of any set of AI ethics principles is not likely to be very  great, given that it will largely “depend on how it is embedded in a larger governance  ecosystem, including for instance relevant policies (e.g.
(a / and
      :op1 (a2 / analyze-01
            :ARG0 (p / person
                  :ARG0-of (a3 / author-01))
            :ARG1 (s / sample-01
                  :ARG1 (s2 / set
                        :quant 36
                        :consist-of (p2 / principle
                              :mod (a4 / artificial)))))
      :op2 (n / note-01
            :ARG0 p
            :ARG1 (c / converge-01
                  :location (a5 / around
                        :op1 (t / theme
                              :quant 8
                              :ARG1-of (s3 / share-01
                                    :ARG0 (s4 / set
                                          :mod (v / various)
                                          :consist-of (p3 / principle)
                                          :mod a4))
                              :ARG1-of (k / key-02)))))
      :op3 (a6 / and
            :op1 (p4 / privacy)
            :op2 (s5 / safe-01)
            :op3 (s6 / security)
            :op4 (t2 / transparency)
            :op5 (p5 / possible-01
                  :ARG1 (e / explain-01)))
      :op6 (c2 / control-01
            :ARG0 (h / human)
            :ARG1 (t3 / technology))
      :op7 (r / responsible-03
            :ARG0 (p6 / professional))
      :op8 (p7 / promote-02
            :ARG1 (v2 / value
                  :mod (h2 / human)))
      :concession (n2 / note-01
            :ARG0 p
            :ARG1 (p8 / possible-01
                  :ARG1 (s7 / see-01
                        :ARG1 (c3 / core
                              :ARG1-of (n3 / normal-02)
                              :domain p2)))
            :ARG1-of (c4 / cause-01
                  :ARG0 (d / depend-01
                        :ARG0 (ii / impact-01
                              :ARG0 (s8 / set
                                    :mod (a7 / any)
                                    :consist-of (p9 / principle
                                          :mod a4)))
                        :ARG1 (t4 / thing
                              :manner-of (e2 / embed-01
                                    :ARG1 s8
                                    :ARG2 (e3 / ecosystem
                                          :mod (g / govern-01)
                                          :ARG1-of (h3 / have-degree-91
                                                :ARG2 (l / large)
                                                :ARG3 (m / more))
                                          :example (p10 / policy-01
                                                :ARG1-of (r2 / relevant-01)))))
                        :degree (g2 / great
                              :polarity -
                              :degree (v3 / very)))))
      :medium (p11 / publication
            :ARG1-of (c5 / cite-01
                  :ARG2 105)
            :source (r3 / research-institute
                  :name (n4 / name
                        :op1 "Berkman"
                        :op2 "Klein"
                        :op3 "Center"
                        :op4 "for"
                        :op5 "Internet"
                        :op6 "&"
                        :op7 "Society")
                  :part-of (u / university
                        :name (n5 / name
                              :op1 "Harvard")))))


# ::id 14
# ::snt As the authors of  the ​ Principled Artificial Intelligence ​ study noted ​109 ​, the established mechanisms of the human  rights framework could help in cases where ethical principles such as “fairness” are subject to  conflicting interpretations, and provide “solutions for complex situations in which separate  principles within a single document are in tension with one another”.
(p / possible-01
      :ARG1 (a / and
            :op1 (h / help-01
                  :ARG0 (m / mechanism
                        :ARG1-of (e / establish-01)
                        :part-of (f / framework
                              :mod (r / right-05
                                    :ARG1 (h2 / human))))
                  :ARG1 (c / case-04
                        :ARG1 (s / subject-02
                              :ARG1 (p2 / principle
                                    :mod (e2 / ethics)
                                    :example (f2 / fairness))
                              :ARG2 (ii / interpret-01
                                    :ARG1 p2
                                    :ARG0-of (c2 / conflict-01)))))
            :op2 (s2 / solve-01
                  :ARG0 m
                  :ARG1 (s3 / situation
                        :mod (c3 / complex)
                        :subevent (t / tension
                              :mod (b / between
                                    :op1 (p3 / principle
                                          :ARG1-of (s4 / separate-02)
                                          :location (d / document
                                                :ARG1-of (s5 / single-02)))
                                    :op2 p3)))))
      :ARG1-of (n / note-01
            :ARG0 (p4 / person
                  :ARG0-of (a2 / author-01
                        :ARG1 (s6 / study
                              :name (n2 / name
                                    :op1 "Principled"
                                    :op2 "Artificial"
                                    :op3 "Intelligence"))))
            :ARG1-of (c4 / cite-01
                  :ARG2 109)))


# ::id 14
# ::snt Further, she notes that ethical  assessments and tweaking of algorithms to comply with fairness criteria will not be enough in  certain cases, and that “in some cases the discriminatory effect of digital technologies will  require their outright prohibition”.
(n / note-01
      :ARG0 (s / she)
      :ARG1 (a / and
            :op1 (h / have-quant-91
                  :polarity -
                  :ARG1 (a2 / and
                        :op1 (a3 / assess-01
                              :mod (e / ethics))
                        :op2 (t / tweak-01
                              :ARG1 (a4 / algorithm)))
                  :ARG3 (e2 / enough)
                  :ARG6 (c / comply-01
                        :ARG0 a2
                        :ARG1 (c2 / criteria
                              :topic (f / fair-01)))
                  :time (c3 / case-04
                        :mod (c4 / certain)))
            :op2 (r / require-01
                  :ARG0 (a5 / affect-01
                        :ARG0 (t2 / technology
                              :mod (d / digital))
                        :ARG2 (d2 / discriminate-02))
                  :ARG1 (p / prohibit-01
                        :ARG1 t2
                        :mod (o / outright))
                  :time (c5 / case-04
                        :mod (s2 / some))))
      :mod (f2 / further))


# ::id 15
# ::snt ” Harvard Journal of Law and Technology, 35( 10), 78, (2021), https://papers.ssrn.com/sol3/papers.cfm?abstract_id =3867634 .34Ibid ., 26.33Ibid ., 28.32Emanuel Moss et al., “Assembling Accountability: Alg orithmic Impact Assessment for the Public Interest, ”Data & Society, June 29, 2021, https://datasociety.net/library/assembling-accounta bility-algorithmic-impact-assessment-for-the-public -interest/31Jacob Metcalf et al.. "Algorithmic impact assessmen ts and accountability: The co-construction of impact s." In Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Tra nsparency , pp.
(p / publication-91
      :ARG0 (a / and
            :op1 (p2 / person
                  :name (n / name
                        :op1 "Emanuel"
                        :op2 "Moss"))
            :op2 (p3 / person
                  :mod (o / other)))
      :ARG1 (p4 / publication
            :name (n2 / name
                  :op1 "Assembling"
                  :op2 "Accountability"
                  :op3 ":"
                  :op4 "Alg"
                  :op5 "orithmic"
                  :op6 "Impact"
                  :op7 "Assessment"
                  :op8 "for"
                  :op9 "the"
                  :op10 "Public"
                  :op11 "Interest"))
      :ARG4 (j / journal
            :name (n3 / name
                  :op1 "Harvard"
                  :op2 "Journal"
                  :op3 "of"
                  :op4 "Law"
                  :op5 "and"
                  :op6 "Technology"))
      :ARG7 (v / value-interval
            :op1 35
            :op2 78)
      :time (d / date-entity
            :year 2021
            :month 6
            :day 29)
      :medium (u / url-entity
            :value "https://www.ssrn.com/library/assembling-accounta bility-algorithmic-impact-assessment-for-the-public-interest/"))


# ::id 15
# ::snt ”67Within two years, NIST is required to develop an AI risk management framework that enables the assessment of “trustworthy” AI and iden tification of appropriate risk mitigation strategies on a voluntary basis in the public and p rivate sectors.68NIST is to establish common definitions and characterizations for AI principles , such as explainability, transparency, and fairness.
(r / require-01
      :li 67
      :ARG1 (d / develop-02
            :ARG0 (r2 / research-institute
                  :name (n / name
                        :op1 "NIST"))
            :ARG1 (f / framework
                  :ARG0-of (e / enable-01
                        :ARG1 (a / and
                              :op1 (a2 / assess-01
                                    :ARG1 (ii / intelligent-01
                                          :mod (a3 / artificial))
                                    :ARG2 (d2 / deserve-01
                                          :ARG0 ii))
                              :op2 (ii2 / identify-01
                                    :ARG1 (s / strategy
                                          :ARG0-of (m / mitigate-01
                                                :ARG1 (r3 / risk-01))
                                          :ARG1-of (a4 / appropriate-02))
                                    :ARG1-of (b / base-02
                                          :ARG2 (a5 / and
                                                :op1 (s2 / sector
                                                      :ARG1-of (p / public-02))
                                                :op2 (s3 / sector
                                                      :ARG1-of (p2 / private-03)))
                                          :ARG1-of (v / voluntary-02)))))
                  :purpose (m2 / manage-01
                        :ARG1 r3)))
      :ARG2 r2
      :time (a6 / after
            :op1 (n2 / now)
            :quant (u / up-to
                  :op1 (t / temporal-quantity
                        :quant 2
                        :unit (y / year))))
      :purpose (e2 / establish-01
            :ARG0 r2
            :ARG1 (a7 / and
                  :op1 (d3 / define-01
                        :ARG1 (p3 / principle
                              :mod ii)
                        :mod (c / common)
                        :example (a8 / and
                              :op1 (e3 / explain-01
                                    :ARG1-of (p4 / possible-01))
                              :op2 (t2 / transparency)
                              :op3 (f2 / fairness)))
                  :op2 (c2 / characterize-01
                        :ARG1 p3))))


# ::id 15
# ::snt If a developer implements the ECPAI S standards, it can add a quality assurance mark to its products and services with the intent t o raise consumer trust and market power.65 The ISO and the International Electrotechnical Comm ission (IEC) are advancing a conformity assessment standard for AI risk management through the work of a joint committee on artificial intelligence (ISO/IEC JTC1/SC 42).66The proposed ISO/EC 42001 - Artificial Intelligence Management System (AIMS) standard will enable organ izations to show they have implemented and continually work on improving processes to addr ess bias, fairness, inclusiveness, safety, security, privacy, accountability, applicability, a nd transparency in AI.
(m / multi-sentence
      :li 65
      :snt1 (p / possible-01
            :ARG1 (a / add-02
                  :ARG0 (p2 / person
                        :ARG0-of (d / develop-02))
                  :ARG1 (m2 / mark
                        :mod (a2 / assure-01
                              :ARG2 (q / quality)))
                  :ARG2 (a3 / and
                        :op1 (p3 / product
                              :poss p2)
                        :op2 (s / serve-01
                              :ARG0 p2))
                  :purpose (ii / intend-01
                        :ARG0 p2
                        :ARG1 (r / raise-01
                              :ARG0 p2
                              :ARG1 (a4 / and
                                    :op1 (t / trust-01
                                          :ARG0 (p4 / person
                                                :ARG0-of (c / consume-01)))
                                    :op2 (p5 / power
                                          :mod (m3 / market)))))))
      :snt2 (a5 / advance-01
            :ARG0 (a6 / and
                  :op1 (o / organization
                        :name (n / name
                              :op1 "ISO"))
                  :op2 (o2 / organization
                        :name (n2 / name
                              :op1 "International"
                              :op2 "Electrotechnical"
                              :op3 "Commission")))
            :ARG1 (s2 / standard
                  :mod (a7 / assess-01
                        :ARG1 (c2 / conform-01))
                  :topic (m4 / manage-01
                        :ARG1 (ii2 / intelligent-01
                              :mod (a8 / artificial)))
                  :ARG1-of (p6 / propose-01))
            :manner (w / work-01
                  :ARG0 (c3 / committee
                        :mod (j / joint)
                        :topic ii2
                        :ARG1-of (m5 / mean-01
                              :ARG2 (a9 / and
                                    :op1 (s3 / standard
                                          :mod (o3 / organization
                                                :name (n3 / name
                                                      :op1 "ISO"))
                                          :mod (o4 / organization
                                                :name (n4 / name
                                                      :op1 "JTC1")))
                                    :op2 (s4 / standard
                                          :mod (o5 / organization
                                                :name (n5 / name
                                                      :op1 "SC"
                                                      :op2 42)))))))))


# ::id 15
# ::snt "Algorithmic Impact Assessment s and Accountability: The Co-Construction of Impact s." In Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Tra nsparency , pp.
(p / publication-91
      :ARG1 (p2 / publication
            :name (n / name
                  :op1 "Algorithmic"
                  :op2 "Impact"
                  :op3 "Assessment"
                  :op4 "and"
                  :op5 "Accountability"
                  :op6 "The"
                  :op7 "Co-Construction"
                  :op8 "of"
                  :op9 "Impact"
                  :op10 "S"))
      :ARG4 (c / conference
            :name (n2 / name
                  :op1 "ACM"
                  :op2 "Conference"
                  :op3 "on"
                  :op4 "Fairness"
                  :op5 "and"
                  :op6 "Accountability"
                  :op7 "and"
                  :op8 "Tra"
                  :op9 "Nsparency")
            :time (d / date-entity
                  :year 2021)
            :ARG2-of (p3 / proceed-01))
      :ARG7 (v / value-interval
            :op1 "pp")
      :op2 "pp")


# ::id 15
# ::snt One of the current trends associated with AIAs is to focus on assessing the sociotechnical aspects (e.g., the potential for bias, fairness, or explainability of the system) and their immediately foreseeable and measurable risks or consequences.
(f / focus-01
      :ARG1 (a / assess-01
            :ARG1 (a2 / aspect
                  :mod (s / sociology)
                  :example (p / potential
                        :domain (o / or
                              :op1 (b / bias-01
                                    :ARG1 (s2 / system))
                              :op2 (f2 / fairness
                                    :poss s2)
                              :op3 (e / explain-01
                                    :ARG1 s2))))
            :ARG2 (a3 / and
                  :op1 (p2 / possible-01
                        :ARG1 (f3 / foresee-01
                              :ARG1 (o2 / or
                                    :op1 (r / risk-01)
                                    :op2 (c / consequence-03)))
                        :time (ii / immediate))
                  :op2 (p3 / possible-01
                        :ARG1 (m / measure-01
                              :ARG1 o2))))
      :ARG1-of (ii2 / include-91
            :ARG2 (t / trend-01
                  :time (c2 / current)
                  :ARG1-of (a4 / associate-01
                        :ARG2 (p4 / product
                              :name (n / name
                                    :op1 "AIA"))))))


# ::id 15
# ::snt Canadaʼs Directive on Automated Decision-Making In 2019, the Canadian government released its Direc tive on Automated Decision-Making (the Directive).39The Directiveʼs principal objectives were to ensure the incorporation of ADS into external public service delivery respects “core adm inistrative law principles such as transparency, accountability, legality, and procedural fairness” and to ensure harmful eﬀects of algorithms on administrative decisions are assessed and reduced.40To this end, the Directive includes an accompanying impact assessment tool in the form of a questionnaire that must be completed prior to the development of any ADS.
(m / multi-sentence
      :snt1 (r / release-01
            :li 39
            :ARG0 (g / government-organization
                  :ARG0-of (g2 / govern-01
                        :ARG1 (c / country
                              :name (n / name
                                    :op1 "Canada"))))
            :ARG1 (l / law
                  :name (n2 / name
                        :op1 "Directive"
                        :op2 "on"
                        :op3 "Automated"
                        :op4 "Decision-Making")
                  :poss g)
            :time (d / date-entity
                  :year 2019))
      :snt2 (ii / include-01
            :li 40
            :ARG1 (t / tool
                  :ARG0-of (a / assess-01
                        :ARG1 (ii2 / impact-01))
                  :ARG0-of (a2 / accompany-01)
                  :consist-of (q / questionnaire
                        :ARG1-of (c2 / complete-01
                              :ARG2-of (o / obligate-01)
                              :time (p / prior
                                    :op1 (d2 / develop-02
                                          :ARG1 (p2 / product
                                                :name (n3 / name
                                                      :op1 "ADS")
                                                :mod (a3 / any)))))))
            :ARG2 (o2 / objective
                  :mod (p3 / principal)
                  :poss l
                  :domain (a4 / and
                        :op1 (e / ensure-01
                              :ARG0 l
                              :ARG1 (r2 / respect-01
                                    :ARG0 l
                                    :ARG1 (p4 / principle
                                          :mod (l2 / law
                                                :mod (a5 / accountable))
                                          :example (a6 / and
                                                :op1 (t2 / transparency)
                                                :op2 (l3 / legal-02)
                                                :op3 (f / fairness
                                                      :mod (p5 / procedure)))
                                          :mod (c3 / core))))
                        :op2 (e2 / ensure-01
                              :ARG0 l
                              :ARG1 (a7 / and
                                    :op1 (a8 / assess-01
                                          :ARG1 (a9 / affect-01
                                                :ARG0 (a10 / algorithm)
                                                :ARG1 (d3 / decide-01
                                                      :ARG3 (a11 / administrate-01))
                                                :ARG1-of (h / harmful-02)))
                                    :op2 (r3 / reduce-01
                                          :ARG1 a9)))))))


# ::id 15
# ::snt In Proceedings of the conference on fairness, accounta bility, and transparency , pp.
(b / be-located-at-91
      :ARG2 (p / publication-91
            :ARG1 (c / conference
                  :topic (a / and
                        :op1 (f / fairness)
                        :op2 (h / honesty)
                        :op3 (t / transparency)))
            :ARG7 (v / value-interval
                  :op1 1
                  :op2 2)))


# ::id 15
# ::snt ”34Typical sources of risk to be identified include the  presence of bias in datasets used to train an AI system, as well as the  fairness and explainability of the model; identification of potential impacts can include con textual considerations related to equity and justice, as well as the economic interests, health,  and well-being of users or populations potentially aﬀected by the proposed system.
(m / multi-sentence
      :snt1 (ii / include-01
            :li 34
            :ARG1 (a / and
                  :op1 (p / present-02
                        :ARG1 (b / bias-01)
                        :ARG2 (d / dataset
                              :ARG1-of (u / use-01
                                    :ARG2 (t / train-01
                                          :ARG1 (s / system
                                                :mod (ii2 / intelligent-01
                                                      :mod (a2 / artificial)))))))
                  :op2 (a3 / and
                        :op1 (f / fairness
                              :poss (m2 / model))
                        :op2 (e / explain-01
                              :ARG1 m2)))
            :ARG2 (s2 / source-02
                  :ARG1 (r / risk-01)
                  :ARG1-of (t2 / typical-02)
                  :ARG1-of (ii3 / identify-01)))
      :snt2 (p2 / possible-01
            :ARG1 (ii4 / include-01
                  :ARG1 (c / consider-02
                        :mod (t3 / text)
                        :ARG1-of (r2 / relate-01
                              :ARG2 (a4 / and
                                    :op1 (e2 / equity)
                                    :op2 (j / justice))))
                  :ARG2 (a5 / and
                        :op1 (ii5 / interest-01
                              :ARG1 (o / or
                                    :op1 (p3 / person
                                          :ARG0-of u))
                              :op2 (p4 / population))
                        :ARG2 (e3 / economy))
                  :op2 (h / healthy
                        :domain o)
                  :op3 (w / well-09
                        :ARG1 o)
                  :ARG1-of (ii6 / identify-01
                        :ARG0 (s3 / system
                              :ARG1-of (p5 / propose-01)))))
      :ARG3 (ii7 / impact-01
            :mod (p6 / potential)))


# ::id 15
# ::snt 4
Human rights impact assessments for AI: analysis an d recommendations AI GOVERNANCE AND HUMAN RIGHTS Over the last few years, at least 170 sets of ethic al or human-rights based AI principles, frameworks, and guidelines have been developed to s upport responsible AI development and deployment within the public and private sectors.11Research has shown that a growing consensus is forming around core principles, including the ne ed for accountability, privacy and security, transparency and explainability, fairness and non-d iscrimination, professional responsibility, human control, and the promotion of human values.12As these AI principles gain acceptance within the public and private sectors, the focus is  now shi
(m / multi-sentence
      :snt1 (a / assess-01
            :li 4
            :ARG1 (ii / impact-01
                  :ARG0 (ii2 / intelligent-01
                        :mod (a2 / artificial))
                  :ARG1 (r / right-05
                        :ARG1 (h / human)))
            :ARG2 (a3 / analyze-01)
            :ARG0-of (r2 / recommend-01
                  :ARG1 (a4 / and
                        :op1 (g / government-organization
                              :ARG0-of (g2 / govern-01)
                              :mod (a5 / artificial))
                        :op2 (r3 / right-05
                              :ARG1 (h2 / human)))))
      :snt2 (d / develop-02
            :ARG1 (s / set
                  :quant (a6 / at-least
                        :op1 170)
                  :consist-of (a7 / and
                        :op1 (p / principle
                              :ARG1-of (b / base-02
                                    :ARG2 (e / ethic)))
                        :op2 (f / framework)
                        :op3 (g3 / guideline)))
            :purpose (s2 / support-01
                  :ARG0 s
                  :ARG1 (a8 / and
                        :op1 d
                        :ARG1 (ii3 / intelligent-01
                              :mod (a9 / artificial))
                        :ARG1-of (r4 / responsible-02))
                  :op2 (d2 / deploy-01
                        :ARG1 ii3
                        :ARG2 (a10 / and
                              :op1 (s3 / sector
                                    :ARG1-of (p2 / public-02))
                              :op2 (s4 / sector
                                    :ARG1-of (p3 / private-03))))))
      :time (b2 / before
            :op1 (n / now)
            :duration (f2 / few
                  :op1 (t / temporal-quantity
                        :quant 1
                        :unit (y / year))))
      :snt3 (s5 / show-01
            :ARG0 (r5 / research-01)
            :ARG1 (f3 / form-01
                  :ARG1 (c / consensus
                        :ARG1-of (g4 / grow-01))
                  :ARG2 (p4 / principle
                        :mod (c2 / core)
                        :ARG2-of (ii4 / include-01
                              :ARG1 (a11 / and
                                    :op1 (a12 / accountable-02)
                                    :op2 (p5 / privacy)
                                    :op3 (s6 / security)
                                    :op4 (a13 / and
                                          :op1 (t2 / transparency)
                                          :op2 (e2 / explain-01))
                                    :op5 (f4 / fairness)
                                    :op6 (d3 / discriminate-02
                                          :polarity -)
                                    :op7 (c3 / control-01
                                          :ARG0 (h3 / human))
                                    :op8 (p6 / promote-02
                                          :ARG1 (v / value
                                                :mod h3)))))))
      :snt4 (c4 / concern-02
            :li 12
            :time n))


# ::id 15
# ::snt To facilitate rights-respecting AI governance at scale, policymakers should ensure appropriate coordination of research & development spending with AI standardization pilot programs, such as regulatory sandboxes, to ac celerate: a. the adoption of privacy-enhancing technologies, t echnical tools that enable bias and fairness detection and mitigation, or the conti nuous monitoring of AI system performance; and, b.
(r / recommend-01
      :ARG1 (e / ensure-01
            :ARG0 (p / person
                  :ARG0-of (l / legislate-01))
            :ARG1 (c / coordinate-01
                  :ARG1 (s / spend-01
                        :ARG1 (a / and
                              :op1 (r2 / research-01)
                              :op2 (d / develop-02)))
                  :ARG3 (p2 / program
                        :ARG0-of (s2 / standardize-01
                              :ARG1 (ii / intelligent-01
                                    :mod (a2 / artificial)))
                        :ARG1-of (p3 / pilot-01)
                        :example (s3 / sandbox
                              :mod (r3 / regulate-01)))
                  :ARG1-of (a3 / appropriate-02)
                  :purpose (a4 / accelerate-01
                        :ARG1 (a5 / and
                              :li "a"
                              :op1 (a6 / adopt-01
                                    :ARG1 (o / or
                                          :op1 (t / technology
                                                :ARG0-of (e2 / enhance-01
                                                      :ARG1 (p4 / privacy)))
                                          :op2 (t2 / tool
                                                :mod (t3 / technical)
                                                :ARG0-of (e3 / enable-01
                                                      :ARG1 (a7 / and
                                                            :op1 (d2 / detect-01
                                                                  :ARG1 (a8 / and
                                                                        :op1 (b / bias-01)
                                                                        :op2 (f / fair-01)))
                                                            :op2 (m / mitigate-01
                                                                  :ARG1 a8)))))
                                    :op3 (m2 / monitor-01
                                          :ARG1 (p5 / perform-02
                                                :ARG0 (s4 / system
                                                      :mod ii))
                                          :mod (c2 / conti-nuous))))))
            :purpose (f2 / facilitate-01)))


# ::id 16
# ::snt EMERGING SOLUTIONS IN 2018  2.1  Bias Busting and Formulas for Fairness: the Limits of Technological “Fixes”  Over the past year, we have seen growing consensus that AI systems perpetuate and amplify bias, and that computational methods are not inherently neutral and objective.
(s / see-01
      :ARG0 (w / we)
      :ARG1 (c / consensus
            :ARG1-of (g / grow-01)
            :topic (a / and
                  :op1 (p / perpetuate-01
                        :ARG0 (s2 / system
                              :mod (ii / intelligent-01
                                    :mod (a2 / artificial)))
                        :ARG1 (b / bias-01))
                  :op2 (a3 / amplify-01
                        :ARG0 s2
                        :ARG1 (b2 / bias-01))
                  :op3 (a4 / and
                        :op1 (n / neutral-02
                              :polarity -
                              :ARG0 (m / method
                                    :mod (c2 / computational))
                              :mod (ii2 / inherent))
                        :op2 (o / objective
                              :polarity -
                              :domain m))))
      :time (y / year
            :mod (p2 / past))
      :ARG1-of (m2 / mean-01
            :ARG2 (p3 / publication-91
                  :ARG7 2.1
                  :ARG1 (a5 / and
                        :op1 (b3 / bust-01
                              :ARG1 (b4 / bias-01))
                        :op2 (f / formula
                              :purpose (f2 / fairness))
                        :op3 (l / limit-01
                              :ARG1 (f3 / fix-02
                                    :mod (t / technology))))))
      :time (d / date-entity
            :year 2018))


# ::id 16
# ::snt Furthermore, as we have seen in the large literature on bias and fairness, classiﬁcations of this nature not only have direct impacts on human lives, but also serve as data to train and inﬂuence other AI systems.
(a / and
      :op2 (a2 / and
            :op1 (ii / impact-01
                  :ARG0 (c / class-01
                        :mod (n / nature
                              :mod (t / this)))
                  :ARG1 (l / live-01
                        :ARG0 (h / human))
                  :ARG1-of (d / direct-02))
            :op2 (s / serve-01
                  :ARG0 c
                  :ARG1 (d2 / data)
                  :ARG2 (a3 / and
                        :op1 (t2 / train-01
                              :ARG1 (s2 / system
                                    :mod (ii2 / intelligent-01
                                          :mod (a4 / artificial))
                                    :mod (o / other)))
                        :op2 (ii3 / in-advance-04
                              :ARG1 s2)))
            :ARG1-of (s3 / see-01
                  :ARG0 (w / we)
                  :location (l2 / literature
                        :mod (l3 / large)
                        :topic (a5 / and
                              :op1 (b / bias-01)
                              :op2 (f / fairness))))))


# ::id 16
# ::snt This recognition comes in the wake of a string of examples, including evidence of bias in algorithmic pretrial risk assessments and hiring algorithms, and has been aided by the work of the Fairness,  Accountability, and Transparency in Machine Learning community.
(a / and
      :op1 (c / come-01
            :ARG1 (r / recognize-01
                  :mod (t / this))
            :ARG2 (w / wake
                  :consist-of (e / example
                        :quant (s / string)
                        :ARG2-of (ii / include-01
                              :ARG1 (e2 / evidence-01
                                    :ARG1 (b / bias-01
                                          :ARG1 (a2 / and
                                                :op1 (a3 / assess-01
                                                      :ARG1 (r2 / risk-01
                                                            :mod (p / pretrial))
                                                      :mod (a4 / algorithm))
                                                :op2 (a5 / algorithm
                                                      :instrument-of (h / hire-01)
                                                      :mod (a6 / algorithm)))))))))
      :op2 (a7 / aid-01
            :ARG0 (w2 / work-01
                  :ARG0 (c2 / community
                        :name (n / name
                              :op1 "Fairness,"
                              :op2 "Accountability"
                              :op3 "and"
                              :op4 "Transparency"
                              :op5 "in"
                              :op6 "Machine"
                              :op7 "Learning")))
            :ARG1 r))


# ::id 16
# ::snt 117  The community has been at the center of an emerging body of academic research on AI-related bias and fairness, producing insights into the nature of these issues, along with methods aimed at remediating bias.
(c / center-01
      :li 117
      :ARG1 (c2 / community
            :ARG0-of (p / produce-01
                  :ARG1 (a / and
                        :op1 (ii / insight
                              :topic (n / nature
                                    :poss (ii2 / issue-02
                                          :mod (t / this))))
                        :op2 (m / method
                              :ARG1-of (a2 / aim-02
                                    :ARG2 (r / remedy-01
                                          :ARG1 (b / bias-01)))))))
      :ARG2 (b2 / body
            :ARG0-of (e / emerge-02)
            :consist-of (r2 / research-01
                  :ARG1 (a3 / and
                        :op1 (b3 / bias-01
                              :ARG1-of (r3 / relate-01
                                    :ARG2 (ii3 / intelligent-01
                                          :mod (a4 / artificial))))
                        :op2 (f / fairness))
                  :mod (a5 / academia))))


# ::id 16
# ::snt 126.Jon Kleinberg, “Inherent Trade-Offs in Algorithmic Fairness,” in  Abstracts of the 2018 ACM International Conference on Measurement and Modeling of Computer Systems , SIGMETRICS ’18 (New York: ACM, 2018), 40–40,   https://doi.org/10.1145/3219617.3219634 ; Alexandra Chouldechova, “Fair Prediction with Disparate Impact: A Study of Bias in Recidivism Prediction Instruments,”  arXiv preprint [Cs, Stat]  arXiv:1610.07524, October 24, 2016.
(p / publication-91
      :ARG0 (p2 / person
            :name (n / name
                  :op1 "Jon"
                  :op2 "Kleinberg"))
      :ARG1 (p3 / publication
            :name (n2 / name
                  :op1 "Inherent"
                  :op2 "Trade-offs"
                  :op3 "in"
                  :op4 "Algorithmic"
                  :op5 "Fairness"))
      :ARG4 (a / abstract
            :part-of (c / conference
                  :name (n3 / name
                        :op1 "ACM"
                        :op2 "International"
                        :op3 "Conference"
                        :op4 "on"
                        :op5 "Measurement"
                        :op6 "and"
                        :op7 "Modeling"
                        :op8 "of"
                        :op9 "Computer"
                        :op10 "Systems")
                  :time (d / date-entity
                        :year 2018)
                  :location (c2 / city
                        :name (n4 / name
                              :op1 "New"
                              :op2 "York"))))
      :ARG7 (v / value-interval
            :op1 40
            :op2 40)
      :ARG4 (p4 / publication
            :name (n5 / name
                  :op1 "Alexandra"
                  :op2 "Chouldechova")
            :ARG1 (p5 / publication
                  :name (n6 / name
                        :op1 "A"
                        :op2 "Study"
                        :op3 "of"
                        :op4 "Bias"
                        :op5 "in"
                        :op6 "Recidivism"
                        :op7 "Prediction"
                        :op8 "Instrument"))
            :ARG4 (j / journal
                  :name (n7 / name
                        :op1 "ARXiv"))
            :ARG6 (p6 / publication
                  :name (n8 / name
                        :op1 "C"
                        :op2 "Stat"))
            :ARG7 v
            :op1 1145
            :op2 1610.07524)
      :ARG4 (p7 / publication
            :name (n9 / name
                  :op1 "https://doi.org/10.1145/3219617.126.3219634"))
      :time (d2 / date-entity
            :day 24
            :month 10
            :year 2016))


# ::id 16
# ::snt The success of such techniques is generally measured against one or another computational deﬁnition of fairness, based on a mathematical set of results.
(m / measure-01
      :ARG1 (s / succeed-01
            :ARG0 (t / technique
                  :mod (s2 / such)))
      :ARG2 (d / determine-01
            :ARG0 (o / or
                  :op1 (o2 / one)
                  :op2 (a / another))
            :ARG1 (f / fairness)
            :manner (c / computational)
            :ARG1-of (b / base-02
                  :ARG2 (s3 / set
                        :consist-of (r / result)
                        :mod (m2 / mathematics))))
      :ARG1-of (g / general-02))


# ::id 16
# ::snt Next, we share our ﬁndings on the government use of automated decision systems, and what questions this raises for fairness, transparency, and due process when such systems are protected by trade secrecy and other laws that prevent auditing and close examination.
(s / share-01
      :ARG0 (w / we)
      :ARG1 (t / thing
            :ARG1-of (t2 / think-01
                  :ARG0 w
                  :ARG2 (u / use-01
                        :ARG0 (g / government-organization
                              :ARG0-of (g2 / govern-01))
                        :ARG1 (s2 / system
                              :purpose (d / decide-01)
                              :ARG1-of (a / automate-01))))
            :ARG0-of (r / raise-01
                  :ARG1 (q / question-01
                        :ARG1 (a2 / and
                              :op1 (f / fairness)
                              :op2 (t3 / transparency)
                              :op3 (d2 / due-process)))
                  :condition (p / protect-01
                        :ARG0 (a3 / and
                              :op1 (s3 / secrecy
                                    :mod (t4 / trade-01))
                              :op2 (l / law
                                    :mod (o / other))
                              :ARG0-of (p2 / prevent-01
                                    :ARG1 (a4 / and
                                          :op1 (a5 / audit-01)
                                          :op2 (e / examine-01
                                                :ARG1-of (c / close-10)))))
                        :ARG1 s2)))
      :time (n / next))


# ::id 16
# ::snt Below is a brief survey of some of the more prominent approaches to understanding and deﬁning issues involving algorithmic bias and fairness.
(s / survey-01
      :ARG1 (a / approach-02
            :ARG1 (a2 / and
                  :op1 (u / understand-01
                        :ARG1 (ii / issue-02
                              :ARG0-of (ii2 / involve-01
                                    :ARG1 (a3 / and
                                          :op1 (b / bias-01
                                                :mod (a4 / algorithm))
                                          :op2 (f / fairness)))))
                  :op2 (d / dismiss-01
                        :ARG1 ii))
            :ARG1-of (ii3 / include-91
                  :ARG2 (a5 / approach-02
                        :ARG1-of (h / have-degree-91
                              :ARG2 (p / prominent)
                              :ARG3 (m / more))))
            :quant (s2 / some))
      :location (b2 / below)
      :duration (b3 / brief))


# ::id 16
# ::snt 121  ●Observational fairness strategies  attempt to diagnose and mitigate bias by considering a dataset (either data used for training an AI model, or the input data processed by such a model), and applying methods to the data aimed at detecting whether it encodes bias against individuals or groups based on characteristics such as race, gender, or socioeconomic standing.
(a / attempt-01
      :li 121
      :ARG0 (s / strategy
            :mod (f / fairness
                  :mod (o / observe-01)))
      :ARG1 (a2 / and
            :op1 (d / diagnose-01
                  :ARG0 s
                  :ARG1 (b / bias-01)
                  :manner (a3 / and
                        :op1 (c / consider-02
                              :ARG0 s
                              :ARG1 (d2 / data
                                    :ARG1-of (m / mean-01
                                          :ARG2 (o2 / or
                                                :op1 (d3 / data
                                                      :ARG1-of (u / use-01
                                                            :ARG2 (t / train-01
                                                                  :ARG1 (m2 / model-01
                                                                        :ARG1 (ii / intelligent-01
                                                                              :mod (a4 / artificial))))))
                                                :op2 (d4 / data
                                                      :mod (ii2 / input)
                                                      :ARG1-of (p / process-01
                                                            :ARG0 m2))))))
                        :op2 (a5 / apply-02
                              :ARG0 s
                              :ARG1 (m3 / method)
                              :ARG2 (d5 / data
                                    :ARG1-of (a6 / aim-02
                                          :ARG2 (d6 / detect-01
                                                :ARG0 s
                                                :ARG1 (e / encode-01
                                                      :ARG0 d5
                                                      :ARG1 (b2 / bias-01
                                                            :ARG2 (o3 / or
                                                                  :op1 (ii3 / individual)
                                                                  :op2 (g / group))
                                                            :ARG1-of (b3 / base-02
                                                                  :ARG2 (t2 / thing
                                                                        :ARG2-of (c2 / characteristic-02)
                                                                        :example (o4 / or
                                                                              :op1 (r / race)
                                                                              :op2 (g2 / gender)
                                                                              :op3 (s2 / standing
                                                                                    :mod (s3 / society)))))))))))))
            :op2 (m4 / mitigate-01
                  :ARG0 s
                  :ARG1 b)))


# ::id 16
# ::snt 24 
 In the search for “algorithmic fairness”, many deﬁnitions of fairness, along with strategies to achieve it, have been proposed over the past few years, primarily by the technical community.
(p / propose-01
      :li 24
      :ARG0 (c / community
            :mod (t / technical))
      :ARG1 (a / and
            :op1 (d / define-01
                  :ARG1 (f / fairness)
                  :quant (m / many))
            :op2 (s / strategy
                  :purpose (a2 / achieve-01
                        :ARG1 (f2 / fairness
                              :mod (a3 / algorithm)))))
      :mod (p2 / primary)
      :time (b / before
            :op1 (n / now)
            :duration (f3 / few
                  :op1 (t2 / temporal-quantity
                        :quant 1
                        :unit (y / year))))
      :subevent-of (s2 / search-01
            :ARG2 f2))


# ::id 16
# ::snt 8  Zuckerberg mentioned AI technologies over 30 times in his Congressional testimony as the cure-all to the company’s problems, particularly in the complex areas of censorship, fairness, and content moderation.
(m / mention-01
      :li 8
      :ARG0 (p / person
            :name (n / name
                  :op1 "Zuckerberg"))
      :ARG1 (t / technology
            :mod (a / artificial))
      :ARG3 (c / cure-01
            :ARG1 (p2 / problem
                  :poss (c2 / company)
                  :topic (a2 / area
                        :mod (c3 / complex)
                        :example (a3 / and
                              :op1 (c4 / censor-01)
                              :op2 (f / fair-01)
                              :op3 (m2 / moderate-01
                                    :ARG1 (c5 / content)))
                        :mod (p3 / particular)))
            :ARG2 t
            :mod (a4 / all))
      :time (t2 / testify-01
            :ARG0 p
            :ARG2 (g / government-organization
                  :name (n2 / name
                        :op1 "Congress")))
      :frequency (o / over
            :op1 30))


# ::id 16
# ::snt EMERGING SOLUTIONS IN 201824 2.1  Bias Busting and Formulas for Fairness: the Limits of Technological “Fixes”24   Broader approaches27 2.2  Industry Applications: Toolkits and System Tweaks28 2.3  Why Ethics is Not Enough29 3.
(m / multi-sentence
      :snt1 (e / emerge-02
            :ARG0 (t / thing
                  :ARG1-of (s / solve-01))
            :time (d / date-entity
                  :year 2018
                  :month 2
                  :day 24))
      :snt2 (a / and
            :op1 (b / bust-01
                  :ARG1 (b2 / bias-01))
            :op2 (f / formula
                  :purpose (f2 / fairness))
            :topic (t2 / thing
                  :ARG2-of (l / limit-01
                        :ARG1 (f3 / fix-02
                              :mod (t3 / technology))))
            :time (d2 / date-entity
                  :year 2018
                  :month 2
                  :day 24))
      :snt3 (a2 / approach-02
            :ARG1-of (h / have-degree-91
                  :ARG2 (s2 / speedy)
                  :ARG3 (m2 / more)))
      :snt4 (a3 / and
            :op1 (p / publication
                  :name (n / name
                        :op1 "27/2.2"))
            :op2 (p2 / publication
                  :name (n2 / name
                        :op1 "28/2.3"))
            :op3 (p3 / publication
                  :name (n3 / name
                        :op1 "Why"
                        :op2 "Ethics"
                        :op3 "Not"
                        :op4 "Enough"))
            :snt5 (a4 / and
                  :op1 (t4 / toolkit)
                  :op2 (t5 / tweak-01
                        :ARG1 (s3 / system))
                  :topic (ii / industry))))


# ::id 16
# ::snt Broadening perspectives and expanding research into AI fairness and bias beyond the merely mathematical is critical to ensuring we are capable of addressing the core issues and moving the focus from parity to justice.
(c / critical-02
      :ARG1 (a / and
            :op1 (b / broaden-01
                  :ARG1 (p / perspective))
            :op2 (e / expand-01
                  :ARG1 (r / research-01
                        :ARG1 (a2 / and
                              :op1 (f / fairness
                                    :mod (a3 / artificial))
                              :op2 (b2 / bias-01
                                    :ARG1 a3)))
                  :ARG4 (b3 / beyond
                        :op1 (m / mathematics
                              :mod (m2 / mere)))))
      :ARG2 (e2 / ensure-01
            :ARG1 (c2 / capable-01
                  :ARG1 (w / we)
                  :ARG2 (a4 / and
                        :op1 (a5 / address-02
                              :ARG0 w
                              :ARG1 (ii / issue-02
                                    :mod (c3 / core)))
                        :op2 (m3 / move-01
                              :ARG0 w
                              :ARG1 (f2 / focus-01)
                              :ARG2 (j / justice)
                              :source (p2 / parity))))))


# ::id 16
# ::snt Yet, without a framework that accounts for social and political contexts and histories, these mathematical formulas for fairness will almost inevitably miss key factors, and can serve to paper over deeper problems in ways that ultimately increase harm or ignore justice.
(h / have-concession-91
      :ARG1 (a / and
            :op1 (m / miss-02
                  :ARG0 (f / formula
                        :mod (m2 / mathematics)
                        :mod (t / this)
                        :purpose (f2 / fair-01))
                  :ARG1 (f3 / factor
                        :ARG1-of (k / key-02))
                  :ARG1-of (a2 / avoid-01
                        :ARG1-of (p / possible-01
                              :polarity -
                              :mod (a3 / almost))))
            :op2 (p2 / possible-01
                  :ARG1 (s / serve-01
                        :ARG0 f
                        :ARG1 (p3 / paper-over-02
                              :ARG0 f
                              :ARG1 (p4 / problem
                                    :ARG1-of (h2 / have-degree-91
                                          :ARG2 (d / deep-02
                                                :ARG1 p4)
                                          :ARG3 (m3 / more)))
                              :manner (o / or
                                    :op1 (ii / increase-01
                                          :ARG0 f
                                          :ARG1 (h3 / harm-01))
                                    :op2 (ii2 / ignore-01
                                          :ARG0 f
                                          :ARG1 (j / justice))
                                    :manner (u / ultimate)))))
            :condition (f4 / framework
                  :polarity -
                  :ARG0-of (a4 / account-01
                        :ARG1 (a5 / and
                              :op1 (c / context
                                    :mod (s2 / society))
                              :op2 (p5 / politics)
                              :op3 (h4 / history))))))


# ::id 16
# ::snt The limits of technological ﬁxes to problems of fairness, bias, and discrimination:  Much new work has been done designing mathematical models for what should be considered “fair” when machines calculate outcomes, aimed at avoiding discrimination.
(m / multi-sentence
      :snt1 (l / limit-01
            :ARG1 (t / technology)
            :ARG2 (p / problem
                  :topic (a / and
                        :op1 (f / fairness)
                        :op2 (b / bias-01)
                        :op3 (d / discriminate-02))))
      :snt2 (w / work-01
            :ARG1 (d2 / design-01
                  :ARG1 (m2 / model-01
                        :ARG1 (t2 / thing
                              :ARG1-of (f2 / fair-01
                                    :ARG1-of (c / consider-01
                                          :ARG1-of (r / recommend-01)
                                          :time (c2 / calculate-01
                                                :ARG0 (m3 / machine)
                                                :ARG1 (o / outcome)
                                                :ARG1-of (a2 / aim-02
                                                      :ARG2 (a3 / avoid-01
                                                            :ARG1 (d3 / discriminate-02)))))))
                        :mod (m4 / mathematics)))
            :quant (m5 / much)
            :ARG1-of (n / new-01)))


# ::id 16
# ::snt Building on our 2016 and 2017 reports, the AI Now 2018 Report contends with this central problem and addresses the following key issues:  1.The growing accountability gap in AI, which favors those who create and deploy these technologies at the expense of those most affected 2.The use of AI to maximize and amplify surveillance, especially in conjunction with facial and affect recognition, increasing the potential for centralized control and oppression 3.Increasing government use of automated decision systems that directly impact individuals and communities without established accountability structures 4.Unregulated and unmonitored forms of AI experimentation on human populations  5.The limits of technological solutions to problems of fairness, bias, and discrimination   Within each topic, we identify emerging challenges and new research, and provide recommendations regarding AI development, deployment, and regulation.
(a / and
      :li 1
      :op1 (c / contend-02
            :ARG0 (p / publication
                  :name (n / name
                        :op1 "AI"
                        :op2 "Now"
                        :op3 "2018"
                        :op4 "Report")
                  :ARG1-of (b / build-01
                        :ARG2 (a2 / and
                              :op1 (r / report
                                    :time (d / date-entity
                                          :year 2016))
                              :op2 (r2 / report
                                    :time (d2 / date-entity
                                          :year 2017)))))
            :ARG1 (p2 / problem
                  :mod (c2 / central)
                  :mod (t / this)))
      :op2 (a3 / address-02
            :ARG0 p
            :ARG1 (a4 / and
                  :li 5
                  :op1 (g / gap
                        :ARG1-of (g2 / grow-01)
                        :topic (a5 / accountable-02)
                        :ARG0-of (f / favor-01
                              :ARG1 (p3 / person
                                    :ARG0-of (c3 / create-01
                                          :ARG1 (t2 / technology
                                                :mod (a6 / artificial)))
                                    :ARG0-of (d3 / deploy-01
                                          :ARG1 t2))
                              :ARG1-of (e / expend-01
                                    :ARG2 (p4 / person
                                          :ARG1-of (a7 / affect-01
                                                :ARG0 t2
                                                :ARG2-of (h / have-degree-91
                                                      :ARG1 p4
                                                      :ARG3 (m / most)))))))
                  :op2 (f2 / form
                        :ARG1-of (r3 / regulate-01
                              :polarity -)
                        :mod (e2 / experiment-01
                              :ARG1 (ii / intelligent-01
                                    :mod a6)))
                  :ARG1-of (m2 / monitor-01
                        :polarity -)))
      :op3 (a8 / and
            :li 2
            :op1 (m3 / maximize-01
                  :ARG1 (s / surveil-01))
            :op2 (a9 / amplify-01
                  :ARG1 s)
            :ARG1-of (c4 / combine-01
                  :ARG2 (a10 / and
                        :op1 (f3 / face)
                        :op2 a7
                        :ARG1 (r4 / recognize-01)))
            :mod (e3 / especially))
      :ARG0-of (ii2 / increase-01
            :ARG1 (p5 / potential
                  :domain (a11 / and
                        :op1 (c5 / control-01
                              :ARG1-of (c6 / centralize-01))
                        :op2 (o / oppress-01)))))


# ::id 16
# ::snt 8.Fairness, accountability, and transparency in AI require a detailed account of the “full stack supply chain.”  For meaningful accountability, we need to better understand and track the component parts of an AI system and the full supply chain on which it relies: that means accounting for the origins and use of training data, test data, models, application program interfaces (APIs), and other infrastructural components over a product life cycle.
(m / multi-sentence
      :snt1 (r / require-01
            :li 8
            :ARG0 (a / and
                  :op1 (f / fairness)
                  :op2 (a2 / accountable-02)
                  :op3 (t / transparency)
                  :topic (ii / intelligent-01
                        :mod (a3 / artificial)))
            :ARG1 (a4 / account-01
                  :ARG1 (c / chain
                        :mod (s / supply-01)
                        :mod (s2 / stack
                              :mod (f2 / full)))
                  :ARG1-of (d / detail-01)))
      :snt2 (n / need-01
            :ARG0 (w / we)
            :ARG1 (a5 / and
                  :op1 (u / understand-01
                        :ARG0 w
                        :ARG1 (a6 / and
                              :op1 (p / part
                                    :mod (c2 / component)
                                    :part-of (s3 / system
                                          :mod (ii2 / intelligent-01)))
                              :op2 (c3 / chain
                                    :mod (s4 / supply-01)
                                    :mod (f3 / full)
                                    :ARG0-of (r2 / rely-01
                                          :ARG1 s3)))
                        :ARG1-of (h / have-degree-91
                              :ARG2 (g / good-02
                                    :ARG1 a6)
                              :ARG3 (m2 / more)))
                  :op2 (t2 / track-01
                        :ARG0 w
                        :ARG1 a6)
                  :purpose (a7 / accountable-02
                        :ARG0 w
                        :ARG1-of (m3 / mean-01
                              :ARG2 (a8 / account-01
                                    :ARG1 (a9 / and
                                          :op1 (o / originate-01
                                                :ARG1 a6)
                                          :op2 (u2 / use-01
                                                :ARG1 a6)
                                          :op3 (d2 / data
                                                :mod (t3 / train-01))
                                          :op4 (m4 / model)
                                          :op5 (ii3 / interface
                                                :mod (p2 / program)
                                                :mod (a10 / application))
                                          :op6 (c4 / component
                                                :mod (ii4 / infrastructure)
                                                :mod (o2 / other))
                                          :time (c5 / cycle-02
                                                :ARG1 (p3 / product)
                                                :ARG2 (l / live-01)))))))
            :ARG0-of (m5 / meaningful-05)))


# ::id 16
# ::snt Our roundtable on  Machine Learning, Inequality and Bias , co-hosted in Berlin with the Robert Bosch Academy, gathered researchers and policymakers from across Europe to address issues of bias, discrimination, and fairness in machine learning and related technologies.
(g / gather-01
      :ARG0 (r / roundtable
            :topic (a / and
                  :op1 (l / learn-01
                        :mod (m / machine))
                  :op2 (e / equal-01
                        :polarity -)
                  :op3 (b / bias-01))
            :ARG1-of (h / host-01
                  :ARG0 (w / we)
                  :accompanier (o / organization
                        :name (n / name
                              :op1 "Robert"
                              :op2 "Bosch"
                              :op3 "Academy"))
                  :location (c / city
                        :name (n2 / name
                              :op1 "Berlin"))))
      :ARG1 (a2 / and
            :op1 (p / person
                  :ARG0-of (r2 / research-01))
            :op2 (p2 / person
                  :ARG0-of (m2 / make-01
                        :ARG1 (p3 / policy-01)))
            :source (a3 / across
                  :op1 (c2 / continent
                        :name (n3 / name
                              :op1 "Europe"))))
      :purpose (a4 / address-02
            :ARG0 a2
            :ARG1 (ii / issue-02
                  :ARG0 (a5 / and
                        :op1 (b2 / bias-01)
                        :op2 (d / discriminate-02)
                        :op3 (f / fair-01)
                        :topic (a6 / and
                              :op1 (l2 / learn-01
                                    :mod (m3 / machine))
                              :op2 (t / technology
                                    :ARG1-of (r3 / relate-01)))))))


# ::id 16
# ::snt WHAT IS NEEDED NEXT32 3.1  From Fairness to Justice32 3.2  Infrastructural Thinking33 3.3  Accounting for Hidden Labor in AI Systems34 3.4  Deeper Interdisciplinarity36 3.5  Race, Gender and Power in AI37 3.6  Strategic Litigation and Policy Interventions39 3.7  Research and Organizing: An Emergent Coalition40 CONCLUSION42 ENDNOTES44       This work is licensed under a  Creative Commons Attribution-NoDerivatives 4.0 International License  2 
ABOUT THE AI NOW INSTITUTE   The AI Now Institute at New York University is an interdisciplinary research institute dedicated to understanding the social implications of AI technologies.
(m / multi-sentence
      :snt1 (n / need-01
            :ARG1 (a / amr-unknown)
            :time (n2 / next)
            :example (a2 / and
                  :op1 (r / research-institute
                        :name (n3 / name
                              :op1 "AI"
                              :op2 "Now"
                              :op3 "Institute")
                        :ARG1-of (d / dedicate-01
                              :ARG2 (u / understand-01
                                    :ARG0 r
                                    :ARG1 (ii / implicate-01
                                          :ARG0 (t / technology
                                                :mod (a3 / artificial))
                                          :ARG1 (s / society)))))
                  :op2 (a4 / and
                        :op1 (r2 / race)
                        :op2 (g / gender)
                        :op3 (p / power)
                        :topic t)
                  :example (a5 / and
                        :op1 (r3 / research-institute)
                        :op2 (o / organize-01)
                        :op3 (c / coalition
                              :ARG1-of (e / emerge-02)))
                  :example (a6 / and
                        :op1 (r4 / research-institute
                              :name (n4 / name
                                    :op1 "Advanced"
                                    :op2 "Technology"
                                    :op3 "Infrastructure"))
                        :op2 (t2 / think-01
                              :mod (ii2 / infrastructure))
                        :example (a7 / and
                              :op1 (n5 / number
                                    :value 32)
                              :op2 (n6 / number
                                    :value 3.2))
                        :example (n7 / number
                              :value 33)
                        :example (n8 / number
                              :value 3.3)
                        :example (a8 / account-01
                              :ARG1 (l / labor-01
                                    :ARG1-of (h / hide-01)
                                    :mod (s2 / system
                                          :mod a3)))
                        :example n8
                        :value 36)
                  :example (n9 / number
                        :value 3.6)
                  :example (n10 / number
                        :value 39)
                  :example (n11 / number
                        :value 44)))
      :snt2 (l2 / license-01
            :ARG1 (w / work-01
                  :mod (t3 / this))
            :ARG3 (t4 / thing
                  :name (n12 / name
                        :op1 "CCP"
                        :op2 " Attribution-NoDerivatives")
                  :ARG1-of (m2 / mean-01
                        :ARG2 (p2 / publication
                              :name (n13 / name
                                    :op1 "Canadian"
                                    :op2 "International"
                                    :op3 "Licence"))))))


# ::id 16
# ::snt The majority of observational fairness approaches can be categorized as being a form of either anti-classiﬁcation, classiﬁcation parity, or calibration, as proposed by Sam Corbett-Davies and Sharad Goel.
(p / possible-01
      :ARG1 (c / categorize-01
            :ARG1 (a / approach-02
                  :ARG1 (f / fairness
                        :mod (o / observe-01))
                  :quant (m / majority))
            :ARG2 (f2 / form
                  :mod (o2 / or
                        :op1 (o3 / oppose-01
                              :ARG1 (c2 / class))
                        :op2 (p2 / parity
                              :mod c2)
                        :op3 (c3 / calibrate-01)
                        :ARG1-of (p3 / propose-01
                              :ARG0 (a2 / and
                                    :op1 (p4 / person
                                          :name (n / name
                                                :op1 "Sam"
                                                :op2 "Corbett-Davies"))
                                    :op2 (p5 / person
                                          :name (n2 / name
                                                :op1 "Sharad"
                                                :op2 "Goel"))))))))


# ::id 16
# ::snt 165.For a more general description of justice as fairness, see: John Rawls,  Justice as Fairness: A Restatement , ed.
(s / see-01
      :ARG0 (y / you)
      :ARG1 (p / publication-91
            :ARG0 (p2 / person
                  :name (n / name
                        :op1 "John"
                        :op2 "Rawls"))
            :ARG1 (p3 / publication
                  :name n
                  :op1 "Justice"
                  :op2 "as"
                  :op3 "Fairness"
                  :op4 ":"
                  :op5 "A"
                  :op6 "Restatement"))
      :ARG4 (e / edition
            :mod 165)
      :purpose (d / describe-01
            :ARG1 (j / justice)
            :ARG2 (f / fairness)
            :ARG1-of (h / have-degree-91
                  :ARG2 (g / general-02
                        :ARG1 d)
                  :ARG3 (m / more))))


# ::id 16
# ::snt 167.Ben Green, “‘Fair’ Risk Assessments: A Precarious Approach for Criminal Justice Reform” (5th Workshop on Fairness, Accountability, and Transparency in Machine Learning, Stockholm, 2018), https://scholar.harvard.edu/ﬁles/bgreen/ﬁles/18-fatml.pdf .
(p / publication-91
      :ARG0 (p2 / person
            :name (n / name
                  :op1 "Ben"
                  :op2 "Green"))
      :ARG1 (p3 / publication
            :name (n2 / name
                  :op1 "Fair-01 :ARG1 ( assess-01 :ARG1 ( risk-01 ) ) :ARG1-of ( mean-01 :ARG2 ( approach-02 :ARG1 ( reform-01 :ARG1 ( justice :mod ( criminal ) ) ) :mod ( precarious ) ) ) ) :ARG1-of ( cite-01 :ARG2 167 ) ) :ARG4 ( publication :name ( name_2 :op1 ")))


# ::id 16
# ::snt The following report develops these themes in detail, reﬂecting on the latest academic research, and outlines seven strategies for moving forward:   1.Expanding AI fairness research beyond a focus on mathematical parity and statistical fairness toward issues of justice 2.Studying and tracking the full stack of infrastructure needed to create AI, including accounting for material supply chains 3.Accounting for the many forms of labor required to create and maintain AI systems 4.Committing to deeper interdisciplinarity in AI  5.Analyzing race, gender, and power in AI 6.Developing new policy interventions and strategic litigation 7.Building coalitions between researchers, civil society, and organizers within the technology sector   These approaches are designed to positively recast the AI ﬁeld and address the growing power imbalance that currently favors those who develop and proﬁt from AI systems at the expense of the populations most likely to be harmed.
(m / multi-sentence
      :snt1 (a / and
            :op1 (d / develop-02
                  :ARG0 (r / report
                        :ARG1-of (f / follow-04))
                  :ARG1 (t / theme
                        :mod (t2 / this))
                  :manner (d2 / detail))
            :op2 (o / outline-01
                  :ARG0 r
                  :ARG1 (s / strategy
                        :quant 7
                        :topic (m2 / move-01
                              :ARG2 (f2 / forward)))))
      :snt2 (a2 / analyze-01
            :li 6
            :ARG1 (a3 / and
                  :op1 (r2 / race)
                  :op2 (g / gender)
                  :op3 (p / power))
            :topic (ii / issue-02
                  :ARG0 (j / justice)))
      :snt3 (d3 / design-01
            :ARG1 (a4 / approach-02
                  :mod (t3 / this))
            :ARG3 (a5 / and
                  :op1 (r3 / recast-01
                        :ARG0 a4
                        :ARG1 (ii2 / intelligent-01
                              :mod (a6 / artificial))
                        :manner (p2 / positive))
                  :op2 (a7 / address-02
                        :ARG0 a4
                        :ARG1 (b / balance-01
                              :polarity -
                              :ARG1 (p3 / power)
                              :ARG1-of (g2 / grow-01)
                              :ARG0-of (f3 / favor-01
                                    :ARG1 (p4 / person
                                          :ARG0-of (d4 / develop-02
                                                :ARG1 (s2 / system
                                                      :mod ii2))
                                          :ARG0-of f3
                                          :ARG1 (p5 / person
                                                :ARG1-of (h / harm-01
                                                      :ARG2-of (h2 / have-degree-91
                                                            :ARG1 p5
                                                            :ARG3 (m3 / most))))))
                              :time (c / current)))))
      :snt4 (a8 / and
            :li 3
            :op1 (s3 / study-01
                  :ARG1 (s4 / stack
                        :consist-of (ii3 / infrastructure)
                        :ARG1-of (n / need-01
                              :ARG0 (c2 / create-01
                                    :ARG1 s2))
                        :ARG2-of (ii4 / include-01
                              :ARG1 (a9 / account-01
                                    :ARG1 (c3 / chain
                                          :mod (s5 / supply-01
                                                :ARG1 (m4 / material))))))
                  :op2 (m5 / maintain-01
                        :ARG1 s4))))


# ::id 16
# ::snt 122  Observational fairness strategies have increasingly emerged through efforts from the community to contend with the limitations of technical fairness work and to provide entry points for other disciplines.
(e / emerge-02
      :li 122
      :ARG0 (s / strategy
            :topic (f / fairness
                  :ARG1-of (o / observe-01)))
      :manner (e2 / effort-01
            :ARG0 (c / community)
            :ARG1 (a / and
                  :op1 (c2 / contend-02
                        :ARG0 c
                        :ARG1 (l / limit-01
                              :ARG1 (w / work-01
                                    :ARG1 (f2 / fairness
                                          :mod (t / technical)))))
                  :op2 (p / provide-01
                        :ARG0 c
                        :ARG1 (p2 / point
                              :location-of (e3 / enter-01
                                    :ARG1 (d / discipline
                                          :mod (o2 / other)))))))
      :ARG1-of (ii / increase-01))


# ::id 16
# ::snt Several scholars have identiﬁed limitations with these approaches to observational fairness.
(l / limit-01
      :ARG0 (a / approach-02
            :ARG1 (f / fairness
                  :mod (o / observe-01))
            :mod (t / this))
      :ARG1 (s / scholar
            :quant (s2 / several))
      :ARG1-of (ii / identify-01))


# ::id 16
# ::snt Secondly, some have argued that different mathematical fairness criteria are mutually exclusive.
(a / argue-01
      :li 2
      :ARG0 (s / some)
      :ARG1 (e / exclusive-02
            :ARG0 (c / criteria
                  :topic (f / fair-01
                        :mod (m / mathematics))
                  :ARG1-of (d / differ-02))
            :mod (m2 / mutual)))


# ::id 16
# ::snt Combining “academically credible” technical fairness ﬁxes and certiﬁcation check boxes runs the risk of instrumenting fairness in ways that lets industry say it has ﬁxed these problems and may divert attention from examining ongoing harms.
(r / risk-01
      :ARG2 (ii / instrument-01
            :ARG1 (f / fairness)
            :manner (w / way
                  :ARG0-of (l / let-01
                        :ARG1 (s / say-01
                              :ARG0 (ii2 / industry)
                              :ARG1 (s2 / solve-01
                                    :ARG0 ii2
                                    :ARG1 (p / problem
                                          :mod (t / this)))))
                  :ARG0-of (d / divert-01
                        :ARG1 (a / attend-02)
                        :ARG2 (e / examine-01
                              :ARG1 (h / harm-01
                                    :ARG1-of (g / go-on-15)))
                        :ARG1-of (p2 / possible-01))))
      :ARG3 (c / combine-01
            :ARG1 (f2 / fairness
                  :mod (t2 / technical)
                  :mod (c2 / credible
                        :mod (a2 / academia)))
            :ARG2 (b / box
                  :ARG0-of (c3 / check-01)
                  :mod (c4 / citizenship))))


# ::id 16
# ::snt Rather than relying on quick ﬁxes, tools, and certiﬁcations, issues of bias and fairness require deeper consideration and more robust accountability frameworks, including strong disclaimers about how “automated fairness” cannot be relied on to truly eliminate bias from AI systems.
(r / require-01
      :ARG0 (ii / issue-02
            :ARG0 (a / and
                  :op1 (b / bias-01)
                  :op2 (f / fairness)))
      :ARG1 (a2 / and
            :op1 (c / consider-02
                  :ARG1 ii
                  :ARG1-of (h / have-degree-91
                        :ARG2 (d / deep-02
                              :ARG1 c)
                        :ARG3 (m / more)))
            :op2 (f2 / framework
                  :purpose (a3 / account-01)
                  :ARG2-of (ii2 / include-01
                        :ARG1 (d2 / disclaim-01
                              :ARG1 (p / possible-01
                                    :polarity -
                                    :ARG1 (r2 / rely-01
                                          :ARG1 (f3 / fairness
                                                :mod (a4 / automatic))
                                          :ARG2 (e / eliminate-01
                                                :ARG0 f3
                                                :ARG1 (b2 / bias-01)
                                                :ARG2 (s / system
                                                      :mod (ii3 / intelligent-01
                                                            :mod (a5 / artificial)))
                                                :ARG1-of (t / true-01))))
                              :ARG1-of (s2 / strong-02)))
                  :ARG1-of (h2 / have-degree-91
                        :ARG2 (r3 / robust)
                        :ARG3 (m2 / more))))
      :ARG1-of (ii4 / instead-of-91
            :ARG2 (r4 / rely-01
                  :ARG1 (a6 / and
                        :op1 (t2 / thing
                              :ARG1-of (t3 / thing-01
                                    :ARG1-of (m3 / mark-01)))
                        :op2 (t4 / tool)
                        :op3 (t5 / thing
                              :ARG1-of (c2 / certify-01))
                        :ARG1-of (q / quick-02)))))


# ::id 16
# ::snt WHAT IS NEEDED NEXT  When we released our AI Now 2016 Report, fairness formulas, debiasing toolkits, and ethical guidelines for AI were rare.
(n / need-01
      :ARG1 (a / amr-unknown)
      :time (n2 / next)
      :time (r / release-01
            :ARG0 (w / we)
            :ARG1 (r2 / report
                  :name (n3 / name
                        :op1 "AI"
                        :op2 "Now"
                        :op3 "2016")
                  :poss w))
      :ARG1-of (c / cause-01
            :ARG0 (r3 / rare-02
                  :ARG1 (a2 / and
                        :op1 (f / formula
                              :mod (f2 / fairness))
                        :op2 (t / toolkit
                              :ARG0-of (d / debilitate-01))
                        :op3 (g / guideline
                              :mod (e / ethics)
                              :topic (ii / intelligent-01
                                    :mod (a3 / artificial)))))))


# ::id 16
# ::snt 3.1  From Fairness to Justice  Any debate about bias and fairness should approach issues of power and hierarchy, looking at who is in a position to produce and proﬁt from these systems, whose values are embedded in these systems, who sets their “objective functions,” and which contexts they are intended to work within.
(r / recommend-01
      :li 3.1
      :ARG1 (a / approach-02
            :ARG0 (d / debate-01
                  :ARG1 (a2 / and
                        :op1 (b / bias-01)
                        :op2 (f / fairness))
                  :mod (a3 / any))
            :ARG1 (ii / issue-02
                  :ARG0 (a4 / and
                        :op1 (p / power)
                        :op2 (h / hierarchy)))
            :manner (l / look-01
                  :ARG0 d
                  :ARG1 (a5 / and
                        :op1 (p2 / person
                              :ARG1-of (p3 / position-01
                                    :ARG2 (a6 / and
                                          :op1 (p4 / produce-01
                                                :ARG0 p2
                                                :ARG2 (s / system
                                                      :mod (t / this)))
                                          :op2 (f2 / favor-01
                                                :ARG0 p2
                                                :ARG1 s))))
                        :op2 (p5 / person
                              :ARG0-of (v / value-01
                                    :ARG1-of (e / embed-01
                                          :ARG2 s)))
                        :op3 (p6 / person
                              :ARG0-of (s2 / set-02
                                    :ARG1 (f3 / function-01
                                          :ARG0 s
                                          :mod (o / objective))))
                        :op4 (c / context
                              :location-of (w / work-01
                                    :ARG0 (t2 / they)
                                    :ARG1-of (ii2 / intend-01)))))))


# ::id 16
# ::snt 168 To this end, our deﬁnitions of “fairness” must expand to encompass the structural, historical, and political contexts in which an algorithmic systems is deployed.
(o / obligate-01
      :li 168
      :ARG2 (e / expand-01
            :ARG1 (d / define-01
                  :ARG0 (w / we)
                  :ARG1 (f / fairness))
            :ARG4 (e2 / encompass-01
                  :ARG0 d
                  :ARG1 (a / and
                        :op1 (c / context
                              :mod (s / structure))
                        :op2 (c2 / context
                              :mod (h / history))
                        :op3 (c3 / context
                              :mod (p / politics))
                        :location-of (d2 / deploy-01
                              :ARG1 (s2 / system
                                    :mod (a2 / algorithm))))))
      :purpose (e3 / end
            :mod (t / this)))


# ::id 16
# ::snt Furthermore, fairness is a term that can be easily co-opted: important questions such as “Fair to whom?
(a / and
      :op2 (t / term
            :domain (f / fairness)
            :ARG1-of (c / coopt-01
                  :ARG1-of (p / possible-01)
                  :ARG1-of (e / easy-05))
            :ARG1-of (m / mean-01
                  :ARG2 (q / question-01
                        :ARG1-of (ii / important-01)
                        :example (f2 / fair-01
                              :ARG2 (a2 / amr-unknown))))))


# ::id 16
# ::snt For example, making a facial recognition system perform equally on people with light and dark skin may be a type of technical progress in terms of parity, but if that technology is disproportionately used on people of color and low-income communities, is it really “fair?” This is why deﬁnitions of fairness face a hard limit if they remain purely contained within the technical domain: in short, “parity is not justice.” 169 32 
 3.2  Infrastructural Thinking  In order to better understand and track the complexities of AI systems, we need to look beyond the technology and the hype to account for the broader context of how AI is shaping and shaped by social and material forces.
(m / multi-sentence
      :snt1 (e / exemplify-01
            :ARG0 (c / contrast-01
                  :ARG1 (p / possible-01
                        :ARG1 (p2 / progress-01
                              :ARG1 (m2 / make-02
                                    :ARG1 (p3 / perform-02
                                          :ARG0 (s / system
                                                :ARG0-of (r / recognize-01
                                                      :ARG1 (f / face)))
                                          :ARG3 (p4 / person
                                                :ARG0-of (h / have-03
                                                      :ARG1 (a / and
                                                            :op1 (s2 / skin
                                                                  :ARG1-of (l / light-06))
                                                            :op2 (s3 / skin
                                                                  :ARG1-of (d / dark-02))))))
                                    :mod (t / technical)
                                    :mod (t2 / type)))
                        :ARG2 (h2 / have-condition-91
                              :ARG1 (f2 / fair-01
                                    :ARG1 m2
                                    :ARG1-of (r2 / real-04)
                                    :polarity (a2 / amr-unknown))
                              :ARG2 (u / use-01
                                    :ARG1 (t3 / technology)
                                    :ARG2 (a3 / and
                                          :op1 (p5 / person
                                                :mod (c2 / color))
                                          :op2 (c3 / community
                                                :mod (ii / income
                                                      :ARG1-of (l2 / low-04))))
                                    :manner (p6 / proportionate
                                          :polarity -)))))
            :snt2 (c4 / cause-01
                  :ARG0 (t4 / this)
                  :ARG1 (f3 / face-01
                        :ARG0 (d2 / define-01
                              :ARG1 (f4 / fairness))
                        :ARG1 (l3 / limit
                              :ARG1-of (h3 / hard-02))
                        :condition (r3 / remain-01
                              :ARG1 d2
                              :ARG3 (c5 / contain-01
                                    :ARG1 d2
                                    :ARG2 (d3 / domain
                                          :mod (t5 / technical))))
                        :mod (ii2 / in-short)))
            :snt3 (c6 / cite-01
                  :ARG1 (p7 / publication
                        :ARG2 (a4 / and
                              :op1 169
                              :op2 32
                              :op3 3.2))
                  :ARG1 p7)
            :snt4 (n / need-01
                  :ARG0 (w / we)
                  :ARG1 (l4 / look-01
                        :ARG0 w
                        :ARG1 (b / beyond
                              :op1 (a5 / and
                                    :op1 (t6 / technology)
                                    :op2 (h4 / hype-01)))
                        :purpose (a6 / account-01
                              :ARG0 w
                              :ARG1 (c7 / context
                                    :ARG1-of (h5 / have-degree-91
                                          :ARG2 (b2 / broad-02
                                                :ARG1 c7)
                                          :ARG3 (m3 / more))
                                    :topic (a7 / and
                                          :op1 (s4 / shape-01
                                                :ARG0 (f5 / force
                                                      :ARG0-of (s5 / social-03))
                                                :ARG1 (ii3 / intelligent-01
                                                      :mod (a8 / artificial)))
                                          :op2 (s6 / shape-01
                                                :ARG0 (f6 / force
                                                      :mod (m4 / material)))
                                          :ARG1 ii3))
                              :op2 (j / justice
                                    :polarity -))))))


# ::id 16
# ::snt We cannot see the global environmental and labor implications of these tools of everyday convenience, nor can we meaningfully advocate for fairness, accountability, and transparency in AI systems, without an understanding of this full stack supply chain.
(a / and
      :op1 (p / possible-01
            :polarity -
            :ARG1 (s / see-01
                  :ARG0 (w / we)
                  :ARG1 (ii / implicate-01
                        :ARG1 (t / tool
                              :mod (c / convenience
                                    :mod (e / everyday))
                              :mod (t2 / this))
                        :ARG2 (a2 / and
                              :op1 (e2 / environment)
                              :op2 (l / labor-01)
                              :mod (g / globe)))))
      :op2 (p2 / possible-01
            :polarity -
            :ARG1 (a3 / advocate-01
                  :ARG0 w
                  :ARG1 (a4 / and
                        :op1 (f / fairness)
                        :op2 (a5 / accountable-02)
                        :op3 (t3 / transparency)
                        :topic (s2 / system
                              :mod (ii2 / intelligent-01
                                    :mod (a6 / artificial))))
                  :ARG0-of (m / meaningful-05)))
      :manner (u / understand-01
            :polarity -
            :ARG0 w
            :ARG1 (c2 / chain
                  :mod (s3 / supply-01)
                  :mod (s4 / stack
                        :ARG1-of (f2 / full-09))
                  :mod (t4 / this))))


# ::id 16
# ::snt But this is still an uphill battle: while there is increased attention to problems of bias in AI systems, we have yet to see much research within the fairness and bias debate focused on the state of equity and diversity in the AI ﬁeld itself.
(c / contrast-01
      :ARG2 (b / battle-01
            :mod (t / this)
            :mod (s / still)
            :mod (u / uphill)
            :ARG1-of (m / mean-01
                  :ARG2 (c2 / contrast-01
                        :ARG1 (a / attend-02
                              :ARG1 (p / problem
                                    :topic (b2 / bias-01
                                          :ARG1 (s2 / system
                                                :mod (a2 / artificial))))
                              :ARG1-of (ii / increase-01))
                        :ARG2 (h / have-11
                              :ARG0 (w / we)
                              :ARG1 (y / yet)
                              :ARG2 (s3 / see-01
                                    :ARG0 w
                                    :ARG1 (r / research-01
                                          :quant (m2 / much)
                                          :location (d / debate-01
                                                :ARG1 (a3 / and
                                                      :op1 (f / fairness)
                                                      :op2 (b3 / bias-01))
                                                :ARG1-of (f2 / focus-01
                                                      :ARG2 (s4 / state
                                                            :mod (a4 / and
                                                                  :op1 (e / equity)
                                                                  :op2 (d2 / diversity))
                                                            :poss s2))))))))))


# ::id 16
# ::snt Second, few government agencies had invested real efforts to ensure that fairness and due process protections remained in place when switching from human-driven decisions to algorithmically-driven ones.
(ii / invest-01
      :li 2
      :ARG0 (a / agency
            :mod (g / government-organization
                  :ARG0-of (g2 / govern-01))
            :quant (f / few))
      :ARG1 (e / effort-01
            :ARG0 a
            :ARG1 (e2 / ensure-01
                  :ARG0 a
                  :ARG1 (r / remain-01
                        :ARG1 (a2 / and
                              :op1 (f2 / fairness)
                              :op2 (p / protect-01
                                    :ARG1 (d / due-process)))
                        :ARG3 (ii2 / in-place)))
            :ARG1-of (r2 / real-04))
      :time (s / switch-01
            :ARG0 a
            :ARG1 (d2 / decide-01)
            :ARG2 (d3 / decide-01
                  :ARG1-of (d4 / drive-02
                        :ARG0 (h / human)))
            :ARG3 (d5 / decide-01
                  :ARG1-of (d6 / drive-02
                        :ARG0 (a3 / algorithm)))))


# ::id 16
# ::snt 53.Joy Buolamwini and Timnit Gebru, “Gender Shades: Intersectional Accuracy Disparities in Commercial Gender Classiﬁcation,” in  Conference on Fairness, Accountability and Transparency  (New York, 2018), 77–91,   http://proceedings.mlr.press/v81/buolamwini18a.html .
(p / publication-91
      :li 53
      :ARG0 (a / and
            :op1 (p2 / person
                  :name (n / name
                        :op1 "Joy"
                        :op2 "Buolamwini"))
            :op2 (p3 / person
                  :name (n2 / name
                        :op1 "Timnit"
                        :op2 "Gebru")))
      :ARG1 (p4 / publication
            :name (n3 / name
                  :op1 "Gender"
                  :op2 "Shadows"
                  :op3 "Intersectional"
                  :op4 "Accident"
                  :op5 "Disparities"
                  :op6 "in"
                  :op7 "Commercial"
                  :op8 "Gender"
                  :op9 "Classiﬁcation"))
      :ARG4 (c / conference
            :name (n4 / name
                  :op1 "Conference"
                  :op2 "on"
                  :op3 "Fairness,"
                  :op4 "Accountability"
                  :op5 "and"
                  :op6 "Transparency")
            :location (c2 / city
                  :name (n5 / name
                        :op1 "New"
                        :op2 "York"))
            :time (d / date-entity
                  :year 2018))
      :ARG7 (u / url-entity
            :value "http://proceedings.mlr.press/v81/buolamwini18a.html"))


# ::id 16
# ::snt 70.“Litigating Algorithms: Challenging Government Use of Algorithmic Decision Systems” (New York: AI Now Institute, September 2018),   https://ainowinstitute.org/litigatingalgorithms.pdf ; Dillon Reisman, Jason Schultz, Kate Crawford, and Meredith Whittaker, “Algorithmic Impact Assessments: A Practical Framework for Public Agency Accountability” (New York: AI Now Institute, April 2018), https://ainowinstitute.org/aiareport2018.pdf ; Micah Altman, Alexandra Wood, and Effy Vayena, “A Harm-Reduction Framework for Algorithmic Fairness,”  IEEE Security Privacy  16, no.
(p / publication-91
      :li 70
      :ARG0 (a / and
            :op1 (p2 / person
                  :name (n / name
                        :op1 "Dillon"
                        :op2 "Reisman"))
            :op2 (p3 / person
                  :name (n2 / name
                        :op1 "Jason"
                        :op2 "Schultz"))
            :op3 (p4 / person
                  :name (n3 / name
                        :op1 "Kate"
                        :op2 "Crawford"))
            :op4 (p5 / person
                  :name (n4 / name
                        :op1 "Meredith"
                        :op2 "Whittaker")))
      :ARG1 (p6 / publication
            :name (n5 / name
                  :op1 "Litigating"
                  :op2 "Algorithms"
                  :op3 "Challenging"
                  :op4 "Use"
                  :op5 "of"
                  :op6 "Algorithmic"
                  :op7 "Decision"
                  :op8 "Systems"))
      :ARG4 (p7 / publication
            :name (n6 / name
                  :op1 "IAI"
                  :op2 "Security"
                  :op3 "Privacy"
                  :op4 "16,"
                  :op5 "No."))
      :ARG4 (p8 / publication
            :name (n7 / name
                  :op1 "A"
                  :op2 "Harm-Reduction"
                  :op3 "Framework"
                  :op4 "for"
                  :op5 "Algorithmic"
                  :op6 "Accountability")
            :ARG1-of (p9 / practical-02))
      :ARG4 (p10 / publication
            :name (n8 / name
                  :op1 "AI"
                  :op2 "Now"
                  :op3 "Institute")
            :location (c / city
                  :name (n9 / name
                        :op1 "New"
                        :op2 "York")))
      :time (d / date-entity
            :month 9
            :year 2018)
      :medium (u / url-entity
            :value "https://ainowinstitute.org/litigatingalgorithms.pdf"))


# ::id 16
# ::snt 95.Nitin Madnani et al., “Building Better Open-Source Tools to Support Fairness in Automated Scoring,” in Proceedings of the First ACL Workshop on Ethics in Natural Language Processing  (Valencia: Association for Computational Linguistics, 2017), 41–52,   https://doi.org/10.18653/v1/W17-1605 .
(p / publication-91
      :ARG0 (a / and
            :op1 (p2 / person
                  :name (n / name
                        :op1 "Nitin"
                        :op2 "Madnani"))
            :op2 (p3 / person
                  :mod (o / other)))
      :ARG1 (p4 / publication
            :name (n2 / name
                  :op1 "Building"
                  :op2 "Better"
                  :op3 "Open-Source"
                  :op4 "Tools"
                  :op5 "to"
                  :op6 "Support"
                  :op7 "of"
                  :op8 "Fairness"
                  :op9 "in"
                  :op10 "Automated"
                  :op11 "Scoring"))
      :ARG4 (p5 / publication
            :name (n3 / name
                  :op1 "Proceedings"
                  :op2 "of"
                  :op3 "the"
                  :op4 "First"
                  :op5 "CAL"
                  :op6 "Workshop"
                  :op7 "on"
                  :op8 "Ethics"
                  :op9 "in"
                  :op10 "Natural"
                  :op11 "Language"
                  :op12 "Processing")
            :location (c / country
                  :name (n4 / name
                        :op1 "Valencia"))
            :time (d / date-entity
                  :year 2017))
      :ARG7 (v / value-interval
            :op1 41
            :op2 52)
      :ARG4 (u / url-entity
            :value "https://doi.org/10.18653/v1/W17-1605.95"))


# ::id 16
# ::snt 118.Shira Mitchell, “Mirror Mirror: Reﬂections on Quantitative Fairness,” 2018, https://shiraamitchell.github.io/fairness/ ; Arvind Narayanan,  Tutorial: 21 Fairness Deﬁnitions and Their Politics , accessed November 18, 2018,   https://www.youtube.com/watch?v=jIXIuYdnyyk .
(m / multi-sentence
      :snt1 (p / publication-91
            :ARG0 (p2 / person
                  :name (n / name
                        :op1 "Shira"
                        :op2 "Mitchell"))
            :ARG1 (p3 / publication
                  :name (n2 / name
                        :op1 "Mirror"
                        :op2 "Mirror:"
                        :op3 ":"
                        :op4 "Reﬂections"
                        :op5 "on"
                        :op6 "Quantitative"
                        :op7 "Fairness"))
            :time (d / date-entity
                  :year 2018)
            :ARG4 (u / url-entity
                  :value "https://shiraamitchell.github.io/fairness/"))
      :snt2 (p4 / publication
            :name (n3 / name
                  :op1 "Tutorial"
                  :op2 21
                  :op3 "Fairness"
                  :op4 "Deﬁnitions"
                  :op5 "and"
                  :op6 "Their"
                  :op7 "Politics")
            :ARG1-of (a / access-01
                  :time (d2 / date-entity
                        :day 18
                        :month 11
                        :year 2018))
            :ARG1-of (p5 / publication-91
                  :ARG0 (p6 / person
                        :name (n4 / name
                              :op1 "Arvind"
                              :op2 "Narayanan")))))


# ::id 16
# ::snt 122.Sam Corbett-Davies and Sharad Goel, “The Measure and Mismeasure of Fairness: A Critical Review of Fair Machine Learning,”  arXiv preprint [CS]  arXiv:1808.00023, July 31, 2018.
(p / publication-91
      :ARG0 (a / and
            :op1 (p2 / person
                  :name (n / name
                        :op1 "Sam"
                        :op2 "Corbett-Davies"))
            :op2 (p3 / person
                  :name (n2 / name
                        :op1 "Sharad"
                        :op2 "Goel")))
      :ARG1 (p4 / publication
            :name (n3 / name
                  :op1 "The"
                  :op2 "Measure"
                  :op3 "and"
                  :op4 "Mismeasure"
                  :op5 "of"
                  :op6 "Fairness"
                  :op7 "A"
                  :op8 "Critical"
                  :op9 "Review"
                  :op10 "of"
                  :op11 "Machine"
                  :op12 "Learning"))
      :ARG4 (j / journal
            :name (n4 / name
                  :op1 "ArXiv"))
      :ARG6 p4
      :name (n5 / name
            :op1 "ARXiv"
            :op2 "Preprint"
            :op3 "CS")
      :time (d / date-entity
            :day 31
            :month 7
            :year 2018)
      :ARG1-of (c / cite-01
            :ARG2 "122.00023"))


# ::id 16
# ::snt 123.Solon Barocas and Moritz Hardt, “Fairness in Machine Learning” (Conference on Neural Information Processing Systems, Long Beach, CA, 2017),   https://mrtz.org/nips17/#/ ; Narayanan,  21 Fairness Deﬁnitions and Their Politics .
(a / and
      :li 123
      :op1 (p / publication-91
            :ARG0 (a2 / and
                  :op1 (p2 / person
                        :name (n / name
                              :op1 "Solon"
                              :op2 "Barocas"))
                  :op2 (p3 / person
                        :name (n2 / name
                              :op1 "Moritz"
                              :op2 "Hardt")))
            :ARG1 (p4 / publication
                  :name (n3 / name
                        :op1 "Fairness"
                        :op2 "in"
                        :op3 "Machine"
                        :op4 "Learning"))
            :ARG4 (c / conference
                  :name (n4 / name
                        :op1 "Conference"
                        :op2 "on"
                        :op3 "Neural"
                        :op4 "Information"
                        :op5 "Processing"
                        :op6 "Systems")
                  :location (c2 / city
                        :name (n5 / name
                              :op1 "Long"
                              :op2 "Beach")
                        :location (s / state
                              :name (n6 / name
                                    :op1 "CA"))))
            :time (d / date-entity
                  :year 2017))
      :op2 (p5 / publication
            :name (n7 / name
                  :op1 "https://mrtz.org/nips17/#/"))
      :op3 (p6 / publication
            :name (n8 / name
                  :op1 "Narayanan")
            :ARG1-of (c3 / cite-01
                  :ARG2 (p7 / publication
                        :quant 21
                        :topic (a3 / and
                              :op1 (f / fairness)
                              :op2 (p8 / politics)
                              :op3 (f2 / fairness))))))


# ::id 16
# ::snt 125.Cynthia Dwork, Nicole Immorlica, Adam T. Kalai, Mark DM Leiserson, “Decoupled classiﬁers for group-fair and eﬃcient machine learning”, Conference on Fairness, Accountability and Transparency (January 21, 2018), 119-133,  http://proceedings.mlr.press/v81/dwork18a/dwork18a.pdf .
(p / publication-91
      :ARG0 (a / and
            :op1 (p2 / person
                  :name (n / name
                        :op1 "Cynthia"
                        :op2 "Dwork"))
            :op2 (p3 / person
                  :name (n2 / name
                        :op1 "Nicole"
                        :op2 "Immorlica"))
            :op3 (p4 / person
                  :name (n3 / name
                        :op1 "Adam"
                        :op2 "T."
                        :op3 "Kalai"))
            :op4 (p5 / person
                  :name (n4 / name
                        :op1 "Mark"
                        :op2 "DM"
                        :op3 "Leiserson")))
      :ARG1 (p6 / publication
            :name (n5 / name
                  :op1 "Decoupled"
                  :op2 "Classiﬁers"
                  :op3 "for"
                  :op4 "Machine"
                  :op5 "Learning"))
      :ARG4 (c / conference
            :name (n6 / name
                  :op1 "Conference"
                  :op2 "on"
                  :op3 "Fairness,"
                  :op4 "Accountability"
                  :op5 "and"
                  :op6 "Transparency"))
      :ARG7 (v / value-interval
            :op1 119
            :op2 133)
      :ARG8 (u / url-entity
            :value "http://proceedings.mlr.press/v81/dwork18a/dwork18a.pdf")
      :time (d / date-entity
            :month 1
            :day 21
            :year 2018))


# ::id 16
# ::snt 145  In both the rapid industrial adoption of academic fairness methods, and the rush to certiﬁcation, we see an eagerness to “solve” and “eliminate” problems of bias and fairness using familiar approaches and skills that avoid the need for signiﬁcant structural change, and which fail to interrogate the complex social and historical factors at play.
(s / see-01
      :li 145
      :ARG0 (w / we)
      :ARG1 (e / eager-01
            :ARG1 (a / and
                  :op1 (s2 / solve-01
                        :ARG1 (p / problem
                              :topic (a2 / and
                                    :op1 (b / bias-01)
                                    :op2 (f / fairness)))
                        :manner (u / use-01
                              :ARG1 (a3 / and
                                    :op1 (a4 / approach-02
                                          :ARG1-of (f2 / familiarize-01))
                                    :op2 (s3 / skill)
                                    :ARG0-of (a5 / avoid-01
                                          :ARG1 (n / need-01
                                                :ARG1 (c / change-01
                                                      :ARG1 (s4 / structure)
                                                      :ARG1-of (p2 / possible-01
                                                            :polarity -))))
                                    :ARG0-of (f3 / fail-01
                                          :ARG1 (ii / interrogate-01
                                                :ARG0 a3
                                                :ARG1 (f4 / factor
                                                      :mod (c2 / complex)
                                                      :mod (s5 / social)
                                                      :mod (h / history)
                                                      :ARG0-of (p3 / play-08)))))))
                  :op2 (e2 / eliminate-01
                        :ARG1 p
                        :manner u)))
      :time (a6 / and
            :op1 (a7 / adopt-01
                  :ARG0 (ii2 / industry)
                  :ARG1 (m / method
                        :topic (f5 / fairness
                              :mod (a8 / academia)))
                  :manner (r / rapid))
            :op2 (r2 / rush-01
                  :ARG2 (c3 / certify-01))))


# ::id 16
# ::snt 142 Even Accenture, a consulting ﬁrm, has developed internal software tools to help clients understand and “essentially eliminate the bias in algorithms.” 143  Industry standards bodies have also taken on fairness efforts in response to industry and public sector requests for accountability assurances.
(m / multi-sentence
      :snt1 (d / develop-02
            :li 142
            :ARG0 (c / company
                  :name (n / name
                        :op1 "Accenture")
                  :mod (e / even)
                  :ARG0-of (c2 / consult-01))
            :ARG1 (t / tool
                  :mod (s / software)
                  :ARG1-of (ii / internal-02))
            :ARG4 (h / help-01
                  :ARG0 c
                  :ARG1 (a / and
                        :op1 (u / understand-01
                              :ARG0 (c3 / client)
                              :ARG1 (b / bias-01
                                    :ARG1 (a2 / algorithm)))
                        :op2 (e2 / eliminate-01
                              :ARG0 c
                              :ARG1 b
                              :mod (e3 / essential)))
                  :ARG2 c3))
      :snt2 (t2 / take-on-09
            :li 143
            :ARG0 (b2 / body
                  :mod (s2 / standard)
                  :mod (ii2 / industry))
            :ARG1 (e4 / effort-01
                  :ARG1 (f / fair-01))
            :mod (a3 / also)
            :ARG2-of (r / respond-01
                  :ARG1 (r2 / request-01
                        :ARG0 (a4 / and
                              :op1 (ii3 / industry)
                              :op2 (s3 / sector
                                    :ARG1-of (p / public-02)))
                        :ARG1 (a5 / assure-01
                              :ARG2 (a6 / accountable-02))))))


# ::id 16
# ::snt 141  Facebook announced the creation and testing of a tool called “Fairness Flow”, an internal tool for Facebook engineers that incorporates many of the same algorithms to help identify bias in machine learning models.
(a / announce-01
      :li 141
      :ARG0 (p / publication
            :name (n / name
                  :op1 "Facebook"))
      :ARG1 (a2 / and
            :op1 (c / create-01
                  :ARG0 p
                  :ARG1 (t / tool
                        :ARG1-of (c2 / call-01
                              :ARG2 (f / flow-01
                                    :ARG1 (f2 / fairness)))
                        :ARG1-of (ii / internal-02
                              :ARG2 p)
                        :beneficiary (p2 / person
                              :ARG0-of (e / engineer-01))
                        :ARG0-of (ii2 / incorporate-02
                              :ARG1 (a3 / algorithm
                                    :quant (m / many)
                                    :ARG1-of (ii3 / include-91
                                          :ARG2 (a4 / algorithm
                                                :ARG1-of (s / same-01))))
                              :purpose (h / help-01
                                    :ARG0 (o / organization
                                          :ARG1 (ii4 / identify-01
                                                :ARG1 (b / bias-01
                                                      :ARG1 (m2 / model-01
                                                            :ARG1 (l / learn-01
                                                                  :manner (m3 / machine)))))
                                          :ARG0-of ii4))))
                  :op2 (t2 / test-01
                        :ARG0 p
                        :ARG1 t))))


# ::id 16
# ::snt 140  Microsoft released fairlearn.py, a Python package meant to help implement a binary classiﬁer subject to a developer’s intended fairness constraint.
(r / release-01
      :li 140
      :ARG0 (c / company
            :name (n / name
                  :op1 "Microsoft"))
      :ARG1 (p / package
            :name (n2 / name
                  :op1 "Fairlearn.py")
            :mod (l / language
                  :name (n3 / name
                        :op1 "Pyro"))
            :ARG1-of (m / mean-02
                  :ARG2 (h / help-01
                        :ARG0 p
                        :ARG1 (ii / implement-01
                              :ARG1 (c2 / class
                                    :mod (b / binary)
                                    :ARG1-of (s / subject-01
                                          :ARG2 (c3 / constraint
                                                :mod (f / fairness)
                                                :ARG1-of (ii2 / intend-01
                                                      :ARG0 (p2 / person
                                                            :ARG0-of (d / develop-02)))))))))))


# ::id 16
# ::snt 126  These “impossibility results” show how each fairness strategy makes implicit assumptions about what is and is not fair.
(s / show-01
      :li 126
      :ARG0 (t / thing
            :ARG2-of (r / result-01)
            :mod (t2 / this)
            :ARG1-of (p / possible-01
                  :polarity -))
      :ARG1 (a / assume-02
            :ARG0 (s2 / strategy
                  :mod (f / fairness)
                  :mod (e / each))
            :ARG1 (t3 / thing
                  :ARG1-of (f2 / fair-01)
                  :ARG1-of (f3 / fair-01
                        :polarity -))
            :mod (ii / implicit)))


# ::id 16
# ::snt 139.Animesh Singh and Michael Hind, “AI Fairness 360: Attacking Bias from All Angles!,”  IBM Developer , September 19, 2018, https://developer.ibm.com/blogs/2018/09/19/ai-fairness-360-attacking-bias-from-all-angles/ .
(p / publication-91
      :ARG0 (a / and
            :op1 (p2 / person
                  :name (n / name
                        :op1 "Animesh"
                        :op2 "Singh"))
            :op2 (p3 / person
                  :name (n2 / name
                        :op1 "Michael"
                        :op2 "Hind")))
      :ARG1 (p4 / publication
            :name (n3 / name
                  :op1 "Attacking"
                  :op2 "Bias"
                  :op3 360
                  :op4 "from"
                  :op5 "All"
                  :op6 "Angles"))
      :ARG4 (c / company
            :name (n4 / name
                  :op1 "IBM")
            :ARG0-of (d / develop-02))
      :ARG7 (u / url-entity
            :value "https://developer.ibm.com/blogs/2018/09/19/ai-fairness-360-attacking-bias-from-all-angles/")
      :time (d2 / date-entity
            :month 9
            :day 19
            :year 2018))


# ::id 16
# ::snt Ultimately, these ﬁndings serve to complicate the broader policy debate focused on solving bias issues with mathematical fairness tools.
(s / serve-01
      :ARG0 (t / thing
            :mod (t2 / this))
      :ARG1 (c / complicate-01
            :ARG0 t
            :ARG1 (d / debate-01
                  :ARG1 (p / policy-01)
                  :ARG1-of (h / have-degree-91
                        :ARG2 (c2 / clear-06
                              :ARG1 d)
                        :ARG3 (m / more))
                  :ARG1-of (f / focus-01
                        :ARG2 (s2 / solve-01
                              :ARG1 (ii / issue-02
                                    :ARG0 (b / bias-01))
                              :ARG2 (t3 / tool
                                    :purpose (f2 / fair-01
                                          :manner (m2 / mathematics)))))))
      :time (u / ultimate))


# ::id 16
# ::snt What they make clear is that solving complex policy issues related to bias and discrimination by indiscriminately applying one or more fairness metrics is unlikely to be successful.
(c / clarify-10
      :ARG0 (t / they)
      :ARG1 (l / likely-01
            :polarity -
            :ARG1 (s / succeed-01
                  :ARG0 (s2 / solve-01
                        :ARG1 (ii / issue-02
                              :ARG0 (p / policy-01
                                    :mod (c2 / complex))
                              :ARG1-of (r / relate-01
                                    :ARG2 (a / and
                                          :op1 (b / bias-01)
                                          :op2 (d / discriminate-01))))
                        :ARG2 (a2 / apply-02
                              :ARG1 (o / or
                                    :op1 (m / metric
                                          :quant 1
                                          :mod (f / fairness))
                                    :op2 (m2 / metric
                                          :quant (m3 / more)
                                          :mod (f2 / fairness)))
                              :manner d
                              :polarity -)))))


# ::id 16
# ::snt This does not mean that such metrics are not useful: observational criteria may help understanding around 26 
whether datasets and AI systems meet various notions of fairness and bias and subsequently help inform a richer discussion about the goals one hopes to achieve when deploying AI systems in complex social contexts.
(m / multi-sentence
      :snt1 (m2 / mean-01
            :polarity -
            :ARG1 (t / this)
            :ARG2 (u / useful-05
                  :polarity -
                  :ARG1 (m3 / metric
                        :mod (s / such))))
      :snt2 (p / possible-01
            :ARG1 (a / and
                  :op1 (h / help-01
                        :ARG0 (c / criteria
                              :mod (o / observe-01))
                        :ARG1 (u2 / understand-01
                              :ARG1 (t2 / truth-value
                                    :polarity-of (m4 / meet-01
                                          :ARG0 (a2 / and
                                                :op1 (d / dataset)
                                                :op2 (s2 / system
                                                      :mod (ii / intelligent-01
                                                            :mod (a3 / artificial))))
                                          :ARG1 (n / notion
                                                :mod (v / various)
                                                :topic (a4 / and
                                                      :op1 (f / fairness)
                                                      :op2 (b / bias-01)))))))
                  :op2 (h2 / help-01
                        :ARG0 c
                        :ARG1 (ii2 / inform-01
                              :ARG0 c
                              :ARG2 (d2 / discuss-01
                                    :ARG1 (g / goal
                                          :ARG1-of (a5 / achieve-01
                                                :ARG0 (o2 / one)
                                                :ARG1-of (h3 / hope-01
                                                      :ARG0 o2)
                                                :time (d3 / deploy-01
                                                      :ARG0 o2
                                                      :ARG1 s2
                                                      :ARG2 (c2 / context
                                                            :mod (s3 / society)
                                                            :mod (c3 / complex)))))
                                    :ARG1-of (h4 / have-degree-91
                                          :ARG2 (r / rich)
                                          :ARG3 (m5 / more)))
                              :time (s4 / subsequent))))))


# ::id 16
# ::snt The proliferation of observational fairness methods also raises concerns over the potential to provide a false sense of assurance.
(r / raise-01
      :ARG0 (p / proliferate-01
            :ARG0 (m / method
                  :purpose (f / fairness
                        :mod (o / observe-01))))
      :ARG1 (c / concern-01
            :ARG0 (p2 / potential
                  :domain (p3 / provide-01
                        :ARG0 m
                        :ARG1 (s / sense-01
                              :ARG1 (a / assure-01)
                              :mod (f2 / false)))))
      :mod (a2 / also))


# ::id 16
# ::snt The idea that, once “treated” with such methods, AI systems are free of bias and safe to use in sensitive domains can provide a dangerous sense of false security—one that relies heavily on mathematical deﬁnitions of fairness without looking at the deeper social and historical context.
(p / possible-01
      :ARG1 (p2 / provide-01
            :ARG0 (ii / idea
                  :topic (a / and
                        :op1 (f / free-04
                              :ARG1 (s / system
                                    :mod (a2 / artificial))
                              :ARG2 (b / bias-01
                                    :ARG1 s))
                        :op2 (s2 / safe-01
                              :ARG1 s
                              :ARG2 (u / use-01
                                    :ARG1 s
                                    :location (d / domain
                                          :ARG0-of (s3 / sensitive-03))))
                        :time (t / treat-01
                              :ARG2 (m / method
                                    :mod (s4 / such)))))
            :ARG1 (s5 / sense-01
                  :ARG1 (s6 / security
                        :mod (f2 / false))
                  :ARG0-of (r / rely-01
                        :ARG1 (n / notion
                              :mod (m2 / mathematics)
                              :topic (f3 / fairness))
                        :manner (h / heavy)
                        :manner (l / look-01
                              :polarity -
                              :ARG1 (a3 / and
                                    :op1 (c / context
                                          :mod (s7 / society))
                                    :op2 (c2 / context
                                          :mod (h2 / history))
                                    :ARG1-of (h3 / have-degree-91
                                          :ARG2 (d2 / deep-02
                                                :ARG1 c)
                                          :ARG3 (m3 / more)))))
                  :ARG0-of (e / endanger-01))))


# ::id 16
# ::snt As legal scholar Frank Pasquale observes, “algorithms alone can’t meaningfully hold other algorithms accountable.” 127  While increased attention to the problems of fairness and bias in AI is a positive development, some have expressed concern over a “mathematization of ethics.” 128  As Shira Mitchell has argued:   “As statistical thinkers in the political sphere we should be aware of the hazards of supplanting politics by an expert discourse.
(m / multi-sentence
      :snt1 (o / observe-01
            :ARG0 (p / person
                  :name (n / name
                        :op1 "Frank"
                        :op2 "Pasquale")
                  :mod (s / scholar
                        :topic (l / law)))
            :ARG1 (p2 / possible-01
                  :polarity -
                  :ARG1 (h / hold-02
                        :ARG0 (a / algorithm
                              :mod (a2 / alone))
                        :ARG1 (a3 / accountable-02
                              :ARG0 (a4 / algorithm
                                    :mod (o2 / other)))
                        :ARG1-of (m2 / meaningful-05)))
            :ARG1-of (c / cite-01
                  :ARG2 127))
      :snt2 (e / express-01
            :ARG0 (s2 / some)
            :ARG1 (c2 / concern-01
                  :ARG0 (m3 / mathematics
                        :topic (e2 / ethics))
                  :ARG1 s2)
            :concession (d / develop-01
                  :ARG1 (a5 / attend-02
                        :ARG1 (p3 / problem
                              :topic (a6 / and
                                    :op1 (f / fairness)
                                    :op2 (b / bias)
                                    :topic (ii / intelligent-01
                                          :mod (a7 / artificial))))
                        :ARG1-of (ii2 / increase-01))
                  :mod (p4 / positive)))
      :snt3 (a8 / argue-01
            :ARG0 (p5 / person
                  :name (n2 / name
                        :op1 "Shira"
                        :op2 "Mitchell"))
            :ARG1 (r / recommend-01
                  :ARG1 (r2 / realize-01
                        :ARG0 (w / we
                              :ARG0-of (t / think-01
                                    :ARG2 (s3 / statistics))
                              :location (s4 / sphere
                                    :mod (p6 / politics)))
                        :ARG1 (d2 / danger
                              :ARG1-of (c3 / cause-01
                                    :ARG0 (s5 / supply-01
                                          :ARG1 (p7 / politics)
                                          :manner (d3 / discourse-01
                                                :ARG0-of (e3 / expert-01)))))))
            :ARG1-of (c4 / cite-01
                  :ARG2 128)))


# ::id 16
# ::snt 129.Mitchell, “Mirror Mirror: Reﬂections on Quantitative Fairness.” 130.Ben Hutchinson and Margaret Mitchell, “50 Years of Test (Un)Fairness: Lessons for Machine Learning,”  arXiv preprint [CS] , arXiv:1811.10104, November 25, 2018.
(p / publication-91
      :ARG0 (a / and
            :op1 (p2 / person
                  :name (n / name
                        :op1 "Ben"
                        :op2 "Hutchinson"))
            :op2 (p3 / person
                  :name (n2 / name
                        :op1 "Margaret"
                        :op2 "Mitchell")))
      :ARG1 (p4 / publication
            :name (n3 / name
                  :op1 "Mirror"
                  :op2 "Mirror:"
                  :op3 ":"
                  :op4 "Reﬂections"
                  :op5 "on"
                  :op6 "Quantitative"
                  :op7 "Fairness"))
      :ARG4 (p5 / publication
            :name (n4 / name
                  :op1 "50"
                  :op2 "Years"
                  :op3 "of"
                  :op4 "Test"
                  :op5 "Un)"
                  :op6 "Fairness:"
                  :op7 "Lives"
                  :op8 "for"
                  :op9 "Machine"
                  :op10 "Learning"))
      :ARG4 (j / journal
            :name (n5 / name
                  :op1 "ArXiv"))
      :ARG6 (p6 / publication
            :name (n6 / name
                  :op1 "1811"
                  :op2 "CS"))
      :time (d / date-entity
            :day 25
            :month 11
            :year 2018)
      :ARG1-of (c / cite-01
            :ARG2 129))


# ::id 16
# ::snt Upcoming work by Hutchinson and Mitchell surveys over ﬁfty years of attempts to construct quantitative fairness deﬁnitions across multiple disciplines.
(w / work-01
      :ARG0 (a / and
            :op1 (p / person
                  :name (n / name
                        :op1 "Hutchinson"))
            :op2 (p2 / person
                  :name (n2 / name
                        :op1 "Mitchell")))
      :ARG1 (s / survey-01
            :ARG0 a
            :ARG2 (a2 / attempt-01
                  :ARG0 a
                  :ARG1 (c / construct-01
                        :ARG0 a
                        :ARG1 (d / dossier
                              :topic (f / fairness)
                              :mod (q / quantitative))
                        :location (a3 / across
                              :op1 (d2 / discipline
                                    :quant (m / multiple)))))
            :duration m
            :op1 (t / temporal-quantity
                  :quant 1
                  :unit (y / year)))
      :ARG1-of (c2 / come-01))


# ::id 16
# ::snt Their efforts stalled after they were unable to agree on “broad technical solutions to the issues involved in fairness.” These precedents emphasize what the Fairness, Accountability and Transparency in Machine Learning community has been discovering: without a “tight connection to real world impact,” the added value of new fairness metrics and algorithms in the machine learning community could be minimal.
(m / multi-sentence
      :snt1 (s / stall-01
            :ARG1 (e / effort-01
                  :ARG0 (t / they))
            :time (a / after
                  :op1 (p / possible-01
                        :polarity -
                        :ARG1 (a2 / agree-01
                              :ARG0 t
                              :ARG1 (t2 / thing
                                    :ARG2-of (s2 / solve-01
                                          :ARG1 (ii / issue-02
                                                :ARG0 (f / fairness))
                                          :mod (t3 / technical)
                                          :ARG1-of (b / broad-02)))))))
      :snt2 (e2 / emphasize-01
            :ARG0 (t4 / thing
                  :ARG1-of (p2 / precedent-01)
                  :mod (t5 / this))
            :ARG1 (d / discover-01
                  :ARG0 (c / community
                        :mod (m2 / machine
                              :ARG0-of (l / learn-01))
                        :consist-of (a3 / and
                              :op1 (f2 / fairness)
                              :op2 (a4 / accountable-02)
                              :op3 (t6 / transparency)))
                  :ARG1 (p3 / possible-01
                        :ARG1 (m3 / minimal-02
                              :ARG1 (t7 / thing
                                    :ARG2-of (v / value-01
                                          :ARG1 (a5 / and
                                                :op1 (m4 / metric
                                                      :mod f2)
                                                :op2 (a6 / algorithm
                                                      :mod f2)
                                                :ARG1-of (n / new-01)))
                                    :ARG1-of (a7 / add-02)))
                        :condition (c2 / connect-01
                              :polarity -
                              :ARG1 c
                              :ARG2 (ii2 / impact-01
                                    :ARG1 (w / world
                                          :ARG1-of (r / real-04)))
                              :ARG1-of (t8 / tight-05))))))


# ::id 16
# ::snt 130  In order to arrive at more meaningful research on fairness and algorithmic bias, we must continue to pair the expertise and perspectives of communities outside of technical disciplines to those within.
(o / obligate-01
      :li 130
      :ARG2 (c / continue-01
            :ARG0 (w / we)
            :ARG1 (p / pair-01
                  :ARG0 w
                  :ARG1 (a / and
                        :op1 (e / expertise)
                        :op2 (p2 / perspective)
                        :poss (c2 / community
                              :location (o2 / outside
                                    :op1 (d / discipline
                                          :mod (t / technical)))))
                  :ARG2 (p3 / person
                        :location d)))
      :purpose (a2 / arrive-01
            :ARG1 w
            :ARG4 (r / research-01
                  :ARG1 (a3 / and
                        :op1 (f / fairness)
                        :op2 (b / bias-01
                              :manner (a4 / algorithm)))
                  :ARG1-of (h / have-degree-91
                        :ARG2 (m / meaningful-05
                              :ARG0 r)
                        :ARG3 (m2 / more)))))


# ::id 16
# ::snt have drawn on the deﬁnition of bias proposed in the early value-sensitive design (VSD) literature to propose a broader view of fairness.
(d / draw-02
      :ARG1 (n / notion
            :topic (b / bias-01)
            :ARG1-of (p / propose-01
                  :location (l / literature
                        :mod (d2 / design
                              :ARG0-of (s / sensitive-03
                                    :ARG1 (v / value)))
                        :time (e / early))))
      :purpose (p2 / propose-01
            :ARG1 (v2 / view-02
                  :ARG1 (f / fairness)
                  :ARG1-of (h / have-degree-91
                        :ARG2 (b2 / broad-02
                              :ARG1 v2)
                        :ARG3 (m / more)))))


# ::id 16
# ::snt Approaches to fairness and bias must take into account both allocative and representational harms, and those that debate the deﬁnitions of fairness and bias must recognize and give voice to the individuals and communities most affected.
(a / and
      :op1 (o / obligate-01
            :ARG2 (t / take-into-account-04
                  :ARG0 (a2 / approach-02
                        :ARG1 (a3 / and
                              :op1 (f / fairness)
                              :op2 (b / bias-01)))
                  :ARG1 (h / harm-01
                        :mod (a4 / allocative)
                        :mod (r / represent-01))))
      :op2 (o2 / obligate-01
            :ARG2 (a5 / and
                  :op1 (r2 / recognize-02
                        :ARG0 (p / person
                              :ARG0-of (d / debate-01
                                    :ARG1 (d2 / define-01
                                          :ARG1 a3)))
                        :ARG1 (a6 / and
                              :op1 (ii / individual)
                              :op2 (c / community)
                              :ARG1-of (a7 / affect-01
                                    :ARG2-of (h2 / have-degree-91
                                          :ARG1 a6
                                          :ARG3 (m / most)))))
                  :op2 (g / give-01
                        :ARG0 p
                        :ARG1 (v / voice-01)
                        :ARG2 a6))))


# ::id 16
# ::snt 138  Any formulation of fairness that excludes impacted populations and the institutional context in which a system is deployed is too limited.
(h / have-degree-91
      :li 134
      :ARG1 (f / formulate-01
            :ARG1 (f2 / fairness)
            :ARG0-of (e / exclude-01
                  :ARG1 (a / and
                        :op1 (p / population
                              :ARG1-of (ii / impact-01))
                        :op2 (c / context
                              :mod (ii2 / institution)
                              :location-of (d / deploy-01
                                    :ARG1 (s / system)))))
            :mod (a2 / any))
      :ARG2 (l / limit-01
            :ARG1 f)
      :ARG3 (t / too))


# ::id 16
# ::snt 2.2  Industry Applications: Toolkits and System Tweaks  This year, we have also seen several technology companies operationalize fairness deﬁnitions, metrics, and tools.
(s / see-01
      :li 2
      :ARG0 (w / we)
      :ARG1 (o / operateize-01
            :ARG0 (c / company
                  :mod (t / technology)
                  :quant (s2 / several))
            :ARG1 (a / and
                  :op1 (f / fairness
                        :domain (d / deﬁnition))
                  :op2 (m / metric)
                  :op3 (t2 / tool)))
      :mod (a2 / also)
      :time (y / year
            :mod (t3 / this))
      :example (a3 / and
            :op1 (t4 / toolkit)
            :op2 (t5 / tweak-01
                  :ARG1 (s3 / system))
            :mod (a4 / application
                  :mod (ii / industry))))


# ::id 16
# ::snt IBM released the “AI Fairness 360” open-source tool kit, which includes nine different algorithms and many other fairness metrics developed by researchers in the Fairness, Accountability and Transparency in Machine Learning community.
(r / release-01
      :ARG0 (c / company
            :name (n / name
                  :op1 "IBM"))
      :ARG1 (k / kit
            :name (n2 / name
                  :op1 "AI"
                  :op2 "Fairness"
                  :op3 360)
            :mod (t / tool)
            :ARG1-of (o / open-04)
            :ARG2-of (ii / include-01
                  :ARG1 (a / and
                        :op1 (a2 / algorithm
                              :quant 9
                              :ARG1-of (d / differ-02))
                        :op2 (m / metric
                              :mod (f / fairness)
                              :mod (o2 / other)
                              :quant (m2 / many)
                              :ARG1-of (d2 / develop-02
                                    :ARG0 (p / person
                                          :ARG0-of (r2 / research-01)
                                          :part-of (c2 / community
                                                :name (n3 / name
                                                      :op1 "Fairness"
                                                      :op2 ","
                                                      :op3 "Accountability"
                                                      :op4 "and"
                                                      :op5 "Transparency"
                                                      :op6 "in"
                                                      :op7 "Machine"
                                                      :op8 "Learning")))))))))


# ::id 16
# ::snt 139 Google’s People + AI Research group (PAIR) released the open-source “What-If” tool, a dashboard allowing researchers to visualize the effects of different bias mitigation strategies and metrics, as well as a tool called “Facets” that supports decision-making around which fairness metric to 28 
use.
(r / release-01
      :ARG0 (o / organization
            :name (n / name
                  :op1 "People"
                  :op2 "&"
                  :op3 "AI"
                  :op4 "Research"
                  :op5 "Group")
            :poss (c / company
                  :name (n2 / name
                        :op1 "Google"))
            :quant 139)
      :ARG1 (a / and
            :op1 (t / tool
                  :name (n3 / name
                        :op1 "What-If")
                  :ARG1-of (s / source-01
                        :ARG1-of (o2 / open-04)))
            :op2 (d / dashboard
                  :ARG0-of (a2 / allow-01
                        :ARG1 (v / visualize-01
                              :ARG0 (p / person
                                    :ARG0-of (r2 / research-01))
                              :ARG1 (a3 / affect-01
                                    :ARG0 (a4 / and
                                          :op1 (s2 / strategy
                                                :ARG0-of (m / mitigate-01
                                                      :ARG1 (b / bias)))
                                          :op2 (m2 / metric)
                                          :ARG1-of (d2 / differ-02))))))
            :op3 (t2 / tool
                  :name (n4 / name
                        :op1 "Facets")
                  :ARG0-of (s3 / support-01
                        :ARG1 (m3 / make-01
                              :ARG1 (d3 / decide-01
                                    :ARG3 (m4 / metric
                                          :topic (f / fair-01))))))))


# ::id 16
# ::snt Their work recalls a period between 1964 and 1973 when researchers focused on deﬁning fairness for educational assessments in ways that echo the current AI fairness debate.
(r / recall-01
      :ARG0 (w / work-01
            :ARG0 (t / they))
      :ARG1 (p / period
            :time (d / date-interval
                  :op1 (d2 / date-entity
                        :year 1964)
                  :op2 (d3 / date-entity
                        :year 1973))
            :time-of (f / focus-01
                  :ARG0 (p2 / person
                        :ARG0-of (r2 / research-01))
                  :ARG1 (f2 / fair-01
                        :purpose (a / assess-01
                              :ARG1 (e / educate-01)))
                  :manner (w2 / way
                        :ARG1-of (e2 / echo-01
                              :ARG2 (d4 / debate-01
                                    :ARG1 (f3 / fair-01
                                          :mod (a2 / artificial))
                                    :time (c / current)))))))


# ::id 16
# ::snt They also highlight the inherent mathematical trade-offs facing those aiming to mitigate various forms of bias based on one or another fairness deﬁnition.
(h / highlight-01
      :ARG0 (t / they)
      :ARG1 (t2 / trade-off-02
            :mod (m / mathematics)
            :mod (ii / inherent)
            :ARG1-of (f / face-01
                  :ARG0 (p / person
                        :ARG0-of (a / aim-01
                              :ARG1 (m2 / mitigate-01
                                    :ARG0 p
                                    :ARG1 (b / bias-01
                                          :mod (f2 / form
                                                :mod (v / various)))
                                    :ARG1-of (b2 / base-02
                                          :ARG2 (f3 / fairness
                                                :mod (o / or
                                                      :op1 (o2 / one)
                                                      :op2 (a2 / another)))))))))
      :mod (a3 / also))


# ::id 16
# ::snt Advances in bias-busting and fairness formulas are strong signs that the ﬁeld of AI has accepted that these concerns are real.
(s / signal-07
      :ARG0 (a / advance-01
            :ARG1 (f / formula
                  :ARG0-of (b / bust-01
                        :ARG1 (b2 / bias-01))
                  :ARG0-of (f2 / fair-01)))
      :ARG1 (a2 / accept-01
            :ARG0 (p / person
                  :ARG0-of (e / engineer-01
                        :ARG1 (a3 / artificial)))
            :ARG1 (r / real-04
                  :ARG1 (c / concern-01
                        :mod (t / this))))
      :ARG1-of (s2 / strong-02))


# ::id 17
# ::snt FAccT ‘21: Proceedings of the  2021 ACM Conference on Fairness, Accountability, and Transparency, March 2021, pp.
(p / publication-91
      :ARG4 21
      :ARG1 (c / conference
            :name (n / name
                  :op1 "ACM"
                  :op2 "Conference"
                  :op3 "on"
                  :op4 "Fairness"
                  :op5 "Accountability"
                  :op6 "and"
                  :op7 "Transparency")
            :time (d / date-entity
                  :year 2021))
      :ARG7 (v / value-interval
            :op1 1
            :op2 2)
      :ARG8 (p2 / publication
            :name (n2 / name
                  :op1 "FACCT"))
      :time (d2 / date-entity
            :month 3
            :year 2021))


# ::id 18
# ::snt Ziad Obermeyer and Sendhil Mullainathan, “Dissecting Racial Bias in an Algorithm that Guides Health   Decisions for 70 Million People,” ​ Proceedings of the Conference on Fairness, Accountability, and   Transparency​ , FAT* ’19 (2019): 89–89, ​ https://dl.acm.org/citation.cfm?id=3287593​ .
(p / publication-91
      :ARG0 (a / and
            :op1 (p2 / person
                  :name (n / name
                        :op1 "Ziad"
                        :op2 "Obermeyer"))
            :op2 (p3 / person
                  :name (n2 / name
                        :op1 "Sendhil"
                        :op2 "Mullainathan")))
      :ARG1 (p4 / publication
            :name (n3 / name
                  :op1 "Dissecting"
                  :op2 "Race"
                  :op3 "Bias")
            :manner (a2 / algorithm
                  :ARG0-of (g / guide-01
                        :ARG1 (d / decide-01
                              :ARG0 (p5 / person
                                    :quant 70000000)
                              :ARG3 (h / health)))))
      :ARG4 (c / conference
            :name (n4 / name
                  :op1 "The"
                  :op2 "Conference"
                  :op3 "on"
                  :op4 "Fairness"
                  :op5 ","
                  :op6 "Accountability"
                  :op7 "and"
                  :op8 "   Transparency")
            :time (d2 / date-entity
                  :year 2019))
      :ARG7 (v / value-interval
            :op1 89
            :op2 89)
      :ARG4 (u / url-entity
            :value "https://dl.acm.org/citation.cfm?id=3287593"))


# ::id 18
# ::snt Mahmoudreza Babaei, Abhijnan Chakraborty, Juhi Kulshrestha, Elissa M. Redmiles, Meeyoung Cha,   and Krishna P. Gummadi, “Analyzing Biases in Perception of Truth in News Stories and Their Implications   for Fact Checking,” ​ Proceedings of the Conference on Fairness, Accountability, and Transparency​ , FAT* ’19   (2019): 139, ​ https://dl.acm.org/citation.cfm?id=3287581​ .
(p / publication-91
      :ARG0 (a / and
            :op1 (p2 / person
                  :name (n / name
                        :op1 "Mahmoudreza"
                        :op2 "Babaei"))
            :op2 (p3 / person
                  :name (n2 / name
                        :op1 "Abhijnan"
                        :op2 "Chakraborty"))
            :op3 (p4 / person
                  :name (n3 / name
                        :op1 "Juhi"
                        :op2 "Kulshrestha"))
            :op4 (p5 / person
                  :name (n4 / name
                        :op1 "Elissa"
                        :op2 "M."
                        :op3 "Redmiles"))
            :op5 (p6 / person
                  :name (n5 / name
                        :op1 "Meeyoung"
                        :op2 "Cha"))
            :op6 (p7 / person
                  :name (n6 / name
                        :op1 "Krishna"
                        :op2 "P."
                        :op3 "Gummadi")))
      :ARG1 (p8 / publication
            :name (n7 / name
                  :op1 "Analyzing"
                  :op2 "Biases"
                  :op3 "in"
                  :op4 "Perception"
                  :op5 "of"
                  :op6 "Truth"
                  :op7 "in"
                  :op8 "News"
                  :op9 "and"
                  :op10 "Their"
                  :op11 "Implications"
                  :op12 "for"
                  :op13 "Fact"
                  :op14 "Checking"))
      :ARG4 (c / conference
            :name (n8 / name
                  :op1 "FAT* :op2 ")))


# ::id 18
# ::snt Margaret Mitchell, Simone Wu, Andrew Zaldivar, Parker Barnes, Lucy Vasserman, Ben Hutchinson,   Elena Spitzer, Inioluwa Deborah Raji, and Timnit Gebru, “Model Cards for Model Reporting,” ​ Proceedings of   the Conference on Fairness, Accountability, and Transparency​ , FAT* ’19 (2019): 220–229,   https://doi.org/10.1145/3287560.3287596​ ; Timnit Gebru, Jamie Morgenstern, Briana Vecchione, Jennifer   Wortman Vaughan, Hanna Wallach, Hal Daumeé III, and Kate Crawford, “Datasheets for Datasets,”   arXiv:1803.09010​  (2018), ​ https://arxiv.org/abs/1803.09010?context=cs​ ; Matthew Arnold, Rachel KE   Bellamy, Michael Hind, Stephanie Houde, Sameep Mehta, Aleksandra Mojsilovic, Ravi Nair, et al.,   “FactSheets: Increasing Trust in AI Services through Supplier’s Declarations of Conformity,” ​ IBM Journal of   Research and Development​  (2019), ​ https://arxiv.org/pdf/1808.07261.pdf​ .
(p / publication-91
      :ARG0 (a / and
            :op1 (p2 / person
                  :name (n / name
                        :op1 "Margaret"
                        :op2 "Mitchell"))
            :op2 (p3 / person
                  :name (n2 / name
                        :op1 "Simone"
                        :op2 "Wu"))
            :op3 (p4 / person
                  :name (n3 / name
                        :op1 "Andrew"
                        :op2 "Zaldivar"))
            :op4 (p5 / person
                  :name (n4 / name
                        :op1 "Lucy"
                        :op2 "Vasserman"))
            :op5 (p6 / person
                  :name (n5 / name
                        :op1 "Ben"
                        :op2 "Hutchinson"))
            :op6 (p7 / person
                  :name (n6 / name
                        :op1 "Elena"
                        :op2 "Spitzer"))
            :op7 (p8 / person
                  :name (n7 / name
                        :op1 "Inioluwa"
                        :op2 "Deborah"
                        :op3 "Raji"))
            :op8 (p9 / person
                  :name (n8 / name
                        :op1 "Timmitt"
                        :op2 "Gebru"))
            :op9 (p10 / person
                  :name (n9 / name
                        :op1 "Stephanie"
                        :op2 "Houde"))
            :op10 (p11 / person
                  :name (n10 / name
                        :op1 "Sameep"
                        :op2 "Mehta"))
            :op11 (p12 / person
                  :name (n11 / name
                        :op1 "Jennifer"
                        :op2 "Wortman"
                        :op3 "Vaughan"))
            :op12 (p13 / person
                  :name (n12 / name
                        :op1 "Hal"
                        :op2 "Daumeé"
                        :op3 "III"))
            :op13 (p14 / person
                  :name (n13 / name
                        :op1 "Kate"
                        :op2 "Crawford")))
      :ARG1 (p15 / publication
            :name (n14 / name
                  :op1 "FactSheets")
            :ARG1-of (t / title-01
                  :ARG2 (j / journal
                        :name (n15 / name
                              :op1 "IBM"
                              :op2 "Journal"
                              :op3 "of"
                              :op4 "Research"
                              :op5 "and"
                              :op6 "Development")
                        :time (d / date-entity
                              :year 2019))))
      :ARG4 (u / url-entity
            :value "https://arxiv.org/10.1145/3287560.pdf"))


# ::id 18
# ::snt Roel Dobbe and Morgan G. Ames, “Translation Tutorial: Values, Engagement and Reflection in   Automated Decision Systems,” presented at the ACM Conference on Fairness, Accountability, and   Transparency, Atlanta, January 2019; see also Dobbe and Ames, “Up Next For FAT*: From Ethical Values To   Ethical Practices,” Medium, February 8, 2019,   https://medium.com/@roeldobbe/up-next-for-fat-from-ethical-values-to-ethical-practices-ebbed9f6adee​ .
(m / multi-sentence
      :snt1 (p / publication-91
            :ARG0 (a / and
                  :op1 (p2 / person
                        :name (n / name
                              :op1 "Roel"
                              :op2 "Dobbe"))
                  :op2 (p3 / person
                        :name (n2 / name
                              :op1 "Morgan"
                              :op2 "G."
                              :op3 "Ames")))
            :ARG1 (p4 / publication
                  :name (n3 / name
                        :op1 "Translation"
                        :op2 "Tutorial:"
                        :op3 ":"
                        :op4 "Value"
                        :op5 ","
                        :op6 "Engagement"
                        :op7 "and"
                        :op8 "Reflection"
                        :op9 "in"
                        :op10 "   Automated"
                        :op11 "Decision"
                        :op12 "Systems"))
            :ARG1-of (p5 / present-01
                  :ARG0 a
                  :time (c / conference
                        :name (n4 / name
                              :op1 "ACM"
                              :op2 "Conference")
                        :topic (a2 / and
                              :op1 (f / fairness)
                              :op2 (a3 / accountable-02)
                              :op3 (t / transparency))
                        :location (c2 / city
                              :name (n5 / name
                                    :op1 "Atlanta"))
                        :time (d / date-entity
                              :month 1
                              :year 2019))))
      :snt2 (s / see-01
            :ARG0 (y / you)
            :ARG1 (p6 / publication
                  :name (n6 / name
                        :op1 "Up"
                        :op2 "Next"
                        :op3 "For"
                        :op4 "Fat:"
                        :op5 "From"
                        :op6 "Ethical"
                        :op7 "Values"
                        :op8 "To"
                        :op9 "Ethical"
                        :op10 "Practices"))
            :medium (p7 / publication
                  :name (n7 / name
                        :op1 "Medium")
                  :time (d2 / date-entity
                        :month 2
                        :day 8
                        :year 2019))
            :mod (a4 / also)))


# ::id 18
# ::snt Hoffmann, “Where Fairness Fails,” ​ https://doi.org/10.1080/1369118X.2019.1573912​ .
(p / publication-91
      :ARG0 (p2 / person
            :name (n / name
                  :op1 "Hoffmann"))
      :ARG1 (p3 / publication
            :name n
            :op1 "Where"
            :op2 "Fairness"
            :op3 "Fails")
      :ARG4 (u / url-entity
            :value "https://doi.org/10.1080/1369118X.2019.1573912"))


# ::id 18
# ::snt Christopher Jung, Michael Kearns, Seth Neel, Aaron Roth, Logan Stapleton, and Zhiwei Steven Wu,   “Eliciting and Enforcing Subjective Individual Fairness,” ​ arXiv:1905.10660 [cs.LG]​ , (2019),   https://arxiv.org/abs/1905.10660​ .
(p / publication-91
      :ARG0 (a / and
            :op1 (p2 / person
                  :name (n / name
                        :op1 "Christopher"
                        :op2 "Jung"))
            :op2 (p3 / person
                  :name (n2 / name
                        :op1 "Michael"
                        :op2 "Kearns"))
            :op3 (p4 / person
                  :name (n3 / name
                        :op1 "Seth"
                        :op2 "Neel"))
            :op4 (p5 / person
                  :name (n4 / name
                        :op1 "Aaron"
                        :op2 "Roth"))
            :op5 (p6 / person
                  :name (n5 / name
                        :op1 "Logan"
                        :op2 "Stapleton"))
            :op6 (p7 / person
                  :name (n6 / name
                        :op1 "Zhiwei"
                        :op2 "Steven"
                        :op3 "Wu")))
      :ARG1 (p8 / publication
            :name (n7 / name
                  :op1 "Eliciting"
                  :op2 "and"
                  :op3 "Enforcing"
                  :op4 "Subjective"
                  :op5 "Individual"
                  :op6 "Fairness"))
      :ARG4 (j / journal
            :name (n8 / name
                  :op1 "ARXiv:1905.10660"))
      :time (d / date-entity
            :year 2019)
      :ARG1-of (h / hyperlink-91
            :ARG3 (u / url-entity
                  :value "https://arxiv.org/abs/1905.10660")))


# ::id 18
# ::snt ​ Anna Lauren Hoffman, “Where Fairness Fails: Data, Algorithms, and the Limits of Antidiscrimination   Discourse,” ​ Information, Communication & Society​  22, no.
(p / publication-91
      :ARG0 (p2 / person
            :name (n / name
                  :op1 "Anna"
                  :op2 "Lauren"
                  :op3 "Hoffman"))
      :ARG1 (p3 / publication
            :name (n2 / name
                  :op1 "Where"
                  :op2 "Fairness"
                  :op3 "Fails"
                  :op4 ":"
                  :op5 ","
                  :op6 ","
                  :op7 "Data,"
                  :op8 "Algorithms"
                  :op9 "and"
                  :op10 "the"
                  :op11 "Limitation"
                  :op12 "of"
                  :op13 "Antidiscrimination"
                  :op14 "Discourse"))
      :ARG4 (j / journal
            :name (n3 / name
                  :op1 "Information,"
                  :op2 "Communication"
                  :op3 "&"
                  :op4 "Society")
            :ord (o / ordinal-entity
                  :value 22)))


# ::id 18
# ::snt ​  Samir Passi and Solon Barocas, “Problem Formulation and Fairness,” ​ Proceedings of the Conference   on Fairness, Accountability, and Transparency​ ,​  ​ FAT* ’19 (2019)​ , 39–48,   https://doi.org/10.1145/3287560.3287567​ .
(p / publication-91
      :ARG0 (a / and
            :op1 (p2 / person
                  :name (n / name
                        :op1 "Samir"
                        :op2 "Passi"))
            :op2 (p3 / person
                  :name (n2 / name
                        :op1 "Solon"
                        :op2 "Barocas")))
      :ARG1 (p4 / publication
            :name (n3 / name
                  :op1 ""
                  :op2 "Formula"
                  :op3 "and"
                  :op4 "Fairness"))
      :ARG4 (p5 / publication
            :name (n4 / name
                  :op1 "Proceeds"
                  :op2 "of"
                  :op3 "the"
                  :op4 "Conference"
                  :op5 "on"
                  :op6 "Fairness"
                  :op7 "and"
                  :op8 "Accountability"
                  :op9 "and"
                  :op10 "Transparency")
            :time (d / date-entity
                  :year 2019))
      :ARG7 (v / value-interval
            :op1 39
            :op2 48)
      :ARG4 (u / url-entity
            :value "https://doi.org/10.1145/3287560.3287567"))


# ::id 18
# ::snt ​ Andrew D. Selbst, danah boyd, Sorelle Friedler, Suresh Venkatasubramanian, and Janet Vertesi,   “Fairness and Abstraction in Sociotechnical Systems,” November 7, 2018,   https://papers.ssrn.com/abstract=3265913​ .
(p / publication-91
      :ARG0 (a / and
            :op1 (p2 / person
                  :name (n / name
                        :op1 "Andrew"
                        :op2 "D."
                        :op3 "Selbst"))
            :op2 (p3 / person
                  :name (n2 / name
                        :op1 "Danaah"
                        :op2 "Boyd"))
            :op3 (p4 / person
                  :name (n3 / name
                        :op1 "Sorelle"
                        :op2 "Friedler"))
            :op4 (p5 / person
                  :name (n4 / name
                        :op1 "Suresh"
                        :op2 "Venkatasubramanian"))
            :op5 (p6 / person
                  :name (n5 / name
                        :op1 "Janet"
                        :op2 "Vertesi")))
      :ARG1 (p7 / publication
            :name (n6 / name
                  :op1 ""
                  :op2 "Fairness"
                  :op3 "and"
                  :op4 "Abstraction"
                  :op5 "in"
                  :op6 "Sociotechnical"
                  :op7 "Systems"))
      :ARG4 (u / url-entity
            :value "https://papers.ssrn.com/abstract=3265913")
      :time (d / date-entity
            :day 7
            :month 11
            :year 2018))


# ::id 18
# ::snt For ethical concerns, see Mason Marks, “Artificial   Intelligence Based Suicide Prediction,” ​ Yale Journal of Health Policy, Law, and Ethics​  (forthcoming, 2019),   https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3324874​ ; and Stevie Chancellor et al., “A Taxonomy   of Ethical Tensions in Inferring Mental Health States from Social Media,” ​ Proceedings of the Conference on   Fairness, Accountability, and Transparency​ , FAT* ’19 (2019): 79–88,   https://doi.org/10.1145/3287560.3287587​ .
(s / see-01
      :mode imperative
      :ARG0 (y / you)
      :ARG1 (p / publication-91
            :ARG0 (p2 / person
                  :name (n / name
                        :op1 "Mason"
                        :op2 "Marks"))
            :ARG1 (p3 / publication
                  :name (n2 / name
                        :op1 "Artificial"
                        :op2 "Intelligence"
                        :op3 "Based"
                        :op4 "Suicide"))
            :ARG4 (j / journal
                  :name (n3 / name
                        :op1 "Yale"
                        :op2 "Journal"
                        :op3 "of"
                        :op4 "Health"
                        :op5 "Policy"
                        :op6 ","
                        :op7 ","
                        :op8 "Law"
                        :op9 "and"
                        :op10 "Ethics"))
            :ARG7 (v / value-interval
                  :op1 79
                  :op2 88)
            :ARG4 (p4 / publication
                  :name (n4 / name
                        :op1 "FAT* :op2 "))))


# ::id 18
# ::snt Margaret Mitchell, Simone Wu, Andrew Zaldivar, Parker Barnes, Lucy Vasserman, Ben Hutchinson, Elena   Spitzer, Inioluwa Deborah Raji, and Timnit Gebru, “Model Cards for Model Reporting,” Proceedings of the   Conference on Fairness, Accountability, and Transparency, FAT* '19 (2019): 220–229,   https://doi.org/10.1145/3287560.3287596​ .
(p / publication-91
      :ARG0 (a / and
            :op1 (p2 / person
                  :name (n / name
                        :op1 "Margaret"
                        :op2 "Mitchell"))
            :op2 (p3 / person
                  :name (n2 / name
                        :op1 "Simone"
                        :op2 "Wu"))
            :op3 (p4 / person
                  :name (n3 / name
                        :op1 "Andrew"
                        :op2 "Zaldivar"))
            :op4 (p5 / person
                  :name (n4 / name
                        :op1 "Parker"
                        :op2 "Barnes"))
            :op5 (p6 / person
                  :name (n5 / name
                        :op1 "Lucy"
                        :op2 "Vasserman"))
            :op6 (p7 / person
                  :name (n6 / name
                        :op1 "Ben"
                        :op2 "Hutchinson"))
            :op7 (p8 / person
                  :name (n7 / name
                        :op1 "Elena"
                        :op2 "Spitzer"))
            :op8 (p9 / person
                  :name (n8 / name
                        :op1 "Inioluwa"
                        :op2 "Deborah"
                        :op3 "Raji"))
            :op9 (p10 / person
                  :name (n9 / name
                        :op1 "Timnit"
                        :op2 "Gebru")))
      :ARG1 (p11 / publication
            :name (n10 / name
                  :op1 "Model"
                  :op2 "Card"
                  :op3 "for"
                  :op4 "Model"
                  :op5 "Reporting"))
      :ARG4 (c / conference
            :name (n11 / name
                  :op1 "FAT*")
            :time (d / date-entity
                  :year 2019))
      :ARG7 (v / value-interval
            :op1 220
            :op2 229)
      :ARG4 (u / url-entity
            :value "https://doi.org/10.1145/3287560.3287596"))


# ::id 18
# ::snt Similar to our discussion of fairness and bias in the 2018 AI Now report,​ 490​  any debate   about vulnerabilities should approach issues of power and hierarchy, looking at who is in a   position to produce and profit from these systems, who determines how vulnerabilities are   accounted for and addressed, and who is most likely to be harmed.
(r / recommend-01
      :ARG1 (a / approach-02
            :ARG0 (d / debate-01
                  :ARG1 (v / vulnerable-01)
                  :mod (a2 / any))
            :ARG1 (ii / issue-02
                  :ARG0 (a3 / and
                        :op1 (p / power)
                        :op2 (h / hierarchy)))
            :manner (l / look-01
                  :ARG0 d
                  :ARG1 (a4 / and
                        :op1 (p2 / person
                              :ARG1-of (p3 / position-01
                                    :ARG2 (a5 / and
                                          :op1 (p4 / produce-01
                                                :ARG0 p2
                                                :ARG1 (s / system
                                                      :mod (t / this)))
                                          :op2 (p5 / profit-01
                                                :ARG0 p2
                                                :ARG1 s))))
                        :op2 (p6 / person
                              :ARG0-of (d2 / determine-01
                                    :ARG1 (a6 / and
                                          :op1 (a7 / account-01
                                                :ARG1 (v2 / vulnerable-01))
                                          :op2 (a8 / address-02
                                                :ARG1 v2))))
                        :op3 (p7 / person
                              :ARG1-of (h2 / harm-01
                                    :ARG1-of (h3 / have-degree-91
                                          :ARG2 (l2 / likely-01
                                                :ARG1 h2)
                                          :ARG3 (m / most)))))))
      :ARG1-of (r2 / resemble-01
            :ARG2 (d3 / discuss-01
                  :ARG0 (w / we)
                  :ARG1 (a9 / and
                        :op1 (f / fairness)
                        :op2 (b / bias-01))
                  :medium (r3 / report
                        :name (n / name
                              :op1 "AI"
                              :op2 "Now")
                        :time (d4 / date-entity
                              :year 2018)))))


# ::id 18
# ::snt AI Now 2019 Report   |   56   In our 2018 AI Now Report, we critically assessed the affordances and limitations of technical   fixes to problems of fairness.​ 453​  Since then, several convincing critiques have emerged that   further explain how these approaches fundamentally distract from more urgent issues,​ 454​  abstract   away societal context,​ 455​  are incommensurate with the political reality of how data scientists   approach “problem formulation,”​ 456​  and fail to address the hierarchical logic that produces   unlawful discrimination.​ 457       Responding to these criticisms, many technical researchers have turned to the use of so-called   “causal” or “counterfactual” fairness methods.
(m / multi-sentence
      :snt1 (p / publication
            :mod 56
            :ARG1-of (r / report-01
                  :ARG0 (p2 / publication
                        :name (n / name
                              :op1 "AI"
                              :op2 "Now"))
                  :time (d / date-entity
                        :year 2019)))
      :snt2 (a / assess-01
            :ARG0 (w / we)
            :ARG1 (a2 / and
                  :op1 (a3 / afford-01
                        :ARG1 (f / fix-02
                              :ARG1 (p3 / problem
                                    :topic (f2 / fairness))
                              :mod (t / technical)))
                  :op2 (l / limit-01
                        :ARG1 f))
            :manner (c / critical)
            :location p2)
      :snt3 (e / emerge-02
            :ARG0 (c2 / critique-01
                  :ARG0-of (c3 / convince-01)
                  :quant (s / several)
                  :ARG0-of (e2 / explain-01
                        :ARG1 (a4 / and
                              :op1 (d2 / distract-01
                                    :ARG0 (a5 / approach-02
                                          :mod (t2 / this))
                                    :ARG1 (ii / issue-02
                                          :ARG1-of (h / have-degree-91
                                                :ARG2 (u / urgent)
                                                :ARG3 (m2 / more)))
                                    :manner (f3 / fundamental))
                              :op2 (a6 / abstract-01
                                    :ARG0 (a7 / approach-02
                                          :ARG0 (s2 / scientist
                                                :mod (d3 / data))
                                          :ARG1 (f4 / formulate-01
                                                :ARG1 (p4 / problem)))
                                    :ARG1 (c4 / context
                                          :mod (s3 / society)))
                              :op3 (c5 / correct-02
                                    :polarity -
                                    :ARG1 a7)
                              :op4 (f5 / fail-01
                                    :ARG1 a7
                                    :ARG2 (a8 / address-02
                                          :ARG0 a7
                                          :ARG1 (l2 / logic
                                                :mod (h2 / hierarchy)
                                                :ARG0-of (p5 / produce-01
                                                      :ARG1 (d4 / discriminate-02
                                                            :mod (l3 / law
                                                                  :polarity -)))))))))
            :time (s4 / since
                  :op1 (t3 / then)))
      :snt4 (r2 / respond-01
            :ARG0 (p6 / person
                  :ARG0-of (r3 / research-01
                        :ARG1 (t4 / technology))
                  :quant (m3 / many))
            :ARG1 (c6 / criticize-01
                  :mod (t5 / this))))


# ::id 18
# ::snt Recently, Optum’s algorithm designed to identify “high-risk” patients in the US was based on the   number of medical services a person used, but didn’t account for the numerous socioeconomic   reasons around the nonuse of needed health services, such as being underinsured or the inability   to take time off from work.​ 451​  With long histories of addressing such social complexities, research   from fields like medical sociology and anthropology, nursing, human-computer interaction, and   public health is needed to protect against the implementation of AI systems that (even when   designed with good intentions) worsen health inequities.​ 452     2.7 Advances in the Machine Learning Community   The Tough Road Toward Sociotechnical Perspectives     As research and perspectives on the social implications of AI evolve, machine learning (ML)   research communities are realizing the limitations of narrow “fairness” definitions and are shifting   their focus to more impactful interventions and strategies, as well as fostering an increased   openness toward active inclusion and engagement with other disciplines.
(m / multi-sentence
      :snt1 (a / and
            :op1 (b / base-02
                  :ARG1 (a2 / algorithm
                        :name (n / name
                              :op1 "Optum")
                        :ARG1-of (d / design-01
                              :ARG3 (ii / identify-01
                                    :ARG0 a2
                                    :ARG1 (p / patient
                                          :ARG1-of (r / risk-01
                                                :ARG1-of (h / high-02)))
                                    :location (c / country
                                          :name (n2 / name
                                                :op1 "US")))))
                  :ARG2 (n3 / number
                        :quant-of (s / serve-01
                              :mod (m2 / medicine)
                              :ARG1-of (u / use-01
                                    :ARG0 (p2 / person))))
                  :ARG1-of (c2 / contrast-01
                        :ARG2 (a3 / account-01
                              :polarity -
                              :ARG0 a2
                              :ARG1 (r2 / reason
                                    :mod (s2 / socioeconomic)
                                    :quant (n4 / numerous)
                                    :topic (u2 / use-01
                                          :polarity -
                                          :ARG1 (s3 / serve-01
                                                :mod (h2 / health)
                                                :ARG1-of (n5 / need-01))
                                          :example (o / or
                                                :op1 (ii2 / insure-02
                                                      :polarity -
                                                      :ARG3 p2)
                                                :op2 (p3 / possible-01
                                                      :polarity -
                                                      :ARG1 (t / take-01
                                                            :ARG0 p2
                                                            :ARG1 (t2 / time
                                                                  :mod (o2 / off))
                                                            :source (w / work-01))))))))))
      :op2 (s4 / shift-01
            :ARG0 (c3 / community
                  :mod (r3 / research-01))
            :ARG1 (f / focus-01
                  :ARG0 c3
                  :ARG2 (a4 / and
                        :op1 (ii3 / intervene-01
                              :ARG0 c3
                              :ARG1-of (h3 / have-degree-91
                                    :ARG2 (ii4 / impact-01
                                          :ARG0 ii3)
                                    :ARG3 (m3 / more)))
                        :op2 (s5 / strategy
                              :ARG1-of (h4 / have-degree-91
                                    :ARG2 ii4
                                    :ARG3 (m4 / more)))))
            :ARG0-of (f2 / foster-01
                  :ARG1 (o3 / open-05
                        :ARG1 c3
                        :ARG2 (a5 / and
                              :op1 (ii5 / include-01
                                    :ARG1-of (a6 / activity-06
                                          :ARG0 c3))
                              :op2 (ii6 / interact-01
                                    :ARG0 c3
                                    :ARG1 (c4 / computer))))
                  :ARG1-of (ii7 / increase-01)))
      :time (e / evolve-01
            :ARG1 (a7 / and
                  :op1 (r4 / research-01
                        :ARG1 (ii8 / intelligent-01
                              :mod (m5 / machine)))
                  :op2 (p4 / perspective
                        :topic (ii9 / imply-01
                              :ARG0 (ii10 / intelligent-01
                                    :mod (a8 / artificial))
                              :ARG1 (s6 / society)))))
      :snt2 (a9 / and
            :op1 (p5 / publication
                  :mod 452)
            :op2 (p6 / publication
                  :name (n6 / name
                        :op1 "The"
                        :op2 "Tough"
                        :op3 "Road"
                        :op4 "Toward"
                        :op5 "Sociotechnical"
                        :op6 " Perspectives"))))


# ::id 18
# ::snt ​ Advances in understanding of bias,   fairness, and justice in machine learning research make it clear that assessments of risks   and harms are imperative.
(m / make-02
      :ARG0 (a / advance-01
            :ARG1 (u / understand-01
                  :ARG1 (a2 / and
                        :op1 (b / bias-01)
                        :op2 (f / fairness)
                        :op3 (j / justice)))
            :topic (r / research-01
                  :ARG1 (m2 / machine
                        :ARG0-of (l / learn-01))))
      :ARG1 (c / clear-06
            :ARG0 a
            :ARG1 imperative
            :domain (a3 / assess-01
                  :ARG1 (a4 / and
                        :op1 (r2 / risk-01)
                        :op2 (h / harm-01)))))


# ::id 18
# ::snt ​ Research on AI bias and fairness has begun to expand   beyond technical solutions that target statistical parity, but there needs to be a much    
  AI Now 2019 Report   |   7   more rigorous examination of AI’s politics and consequences, including close attention to   AI’s classification practices and harms.
(h / have-concession-91
      :ARG1 (n / need-01
            :ARG1 (e / examine-01
                  :ARG1 (a / and
                        :op1 (p / politics
                              :poss (ii / intelligent-01
                                    :mod (a2 / artificial)))
                        :op2 (c / consequence-03
                              :ARG1 ii))
                  :ARG0-of (ii2 / include-01
                        :ARG1 (a3 / attend-02
                              :ARG1 (a4 / and
                                    :op1 (p2 / practice-01
                                          :ARG0 ii
                                          :ARG1 (c2 / classify-01))
                                    :op2 (h2 / harm-01
                                          :ARG0 ii))
                              :ARG1-of (c3 / close-10)))
                  :ARG1-of (h3 / have-degree-91
                        :ARG2 (r / rigorous)
                        :ARG3 (m / more
                              :quant (m2 / much)))))
      :ARG2 (b / begin-01
            :ARG0 (r2 / research-01
                  :ARG1 (a5 / and
                        :op1 (b2 / bias-01
                              :ARG0 ii)
                        :op2 (f / fair-01
                              :ARG1 ii)))
            :ARG1 (e2 / expand-01
                  :ARG1 r2
                  :ARG4 (b3 / beyond
                        :op1 (s / solution
                              :mod (t / technical)
                              :ARG0-of (t2 / target-01
                                    :ARG1 (p3 / parity
                                          :mod (s2 / statistics)))))))
      :ARG1-of (d / describe-01
            :ARG0 (p4 / publication
                  :name (n2 / name
                        :op1 "AI"
                        :op2 "Now"
                        :op3 "2019"
                        :op4 7))))


# ::id 18
# ::snt Joy Buolamwini and Timnit Gebru, “Gender Shades: Intersectional Accuracy Disparities in Commercial   Gender Classification,” Conference on Fairness, Accountability and Transparency (2018): 77–91,   http://gendershades.org/​ .
(p / publication-91
      :ARG0 (a / and
            :op1 (p2 / person
                  :name (n / name
                        :op1 "Joy"
                        :op2 "Buolamwini"))
            :op2 (p3 / person
                  :name (n2 / name
                        :op1 "Timnit"
                        :op2 "Gebru")))
      :ARG1 (p4 / publication
            :name (n3 / name
                  :op1 "Gender"
                  :op2 "Shadows"
                  :op3 "Intersectional"
                  :op4 "Accident"
                  :op5 "Disparities"
                  :op6 "in"
                  :op7 "Commercial"
                  :op8 "Gender"
                  :op9 "Classification"))
      :ARG4 (c / conference
            :name (n4 / name
                  :op1 "Conference"
                  :op2 "on"
                  :op3 "Fairness,"
                  :op4 "Accountability"
                  :op5 "and"
                  :op6 "Transparency"))
      :ARG7 (v / value-interval
            :op1 77
            :op2 91)
      :time (d / date-entity
            :year 2018)
      :ARG7 (u / url-entity
            :value "http://gendershades.org/​.html"))


# ::id 18
# ::snt Ben Green and Salomé Viljoen, “Algorithmic Realism: Expanding the Boundaries of Algorithmic   Thought,” ​ Proceedings of the ACM Conference on Fairness, Accountability, and Transparency (FAT*)​ , 2020.
(p / publication-91
      :ARG0 (a / and
            :op1 (p2 / person
                  :name (n / name
                        :op1 "Ben"
                        :op2 "Green"))
            :op2 (p3 / person
                  :name (n2 / name
                        :op1 "Salomé"
                        :op2 "Viljoen")))
      :ARG1 (p4 / publication
            :name (n3 / name
                  :op1 "Algorithmic"
                  :op2 "Realism"
                  :op3 ":"
                  :op4 ":"
                  :op5 ":"
                  :op6 "Algorithmic"
                  :op7 "Expanding"
                  :op8 "the"
                  :op9 "Boundaries"
                  :op10 "of"
                  :op11 "Algorithmic"
                  :op12 "Think"))
      :ARG4 (c / conference
            :name (n4 / name
                  :op1 "ACM"
                  :op2 "Conference"
                  :op3 "on"
                  :op4 "Fairness,"
                  :op5 "Accountability"
                  :op6 "and"
                  :op7 "Transparency"))
      :time (d / date-entity
            :year 2020)
      :ARG2-of (p5 / proceed-01))


# ::id 24
# ::snt FAIRNESS AND JUST ICE    2.
(a / and
      :li 2
      :op1 (f / fairness)
      :op2 (ii / ice
            :mod (j / just)))


# ::id 24
# ::snt Designers , developers and users of AI systems ( AI stakeholders ) must respect:   • Applicable laws in  New Zealand and other relevant jurisdictions   • Human rights  recognised under domestic and international law   • Rights of Māori  articulated in Te Tiriti o Waitangi    • Democratic values including the electoral process and informed public debate   • Principles of equality a nd fairness so that AI systems do not unjustly harm, exclude, disempower or  discriminate against individuals or particular groups.
(o / obligate-01
      :ARG2 (r / respect-01
            :ARG0 (a / and
                  :op1 (p / person
                        :ARG0-of (d / design-01
                              :ARG1 (s / system
                                    :mod (a2 / artificial))))
                  :op2 (p2 / person
                        :ARG0-of (d2 / develop-02
                              :ARG1 s))
                  :op3 (p3 / person
                        :ARG0-of (u / use-01
                              :ARG1 s))
                  :domain (s2 / stake-01
                        :ARG2 s))
            :ARG1 (a3 / and
                  :op1 (l / law
                        :ARG1-of (a4 / apply-02
                              :location (a5 / and
                                    :op1 (c / country
                                          :name (n / name
                                                :op1 "New"
                                                :op2 "Zealand"))
                                    :op2 (j / jurisdiction
                                          :ARG1-of (r2 / relevant-01)
                                          :mod (o2 / other)))))
                  :op2 (r3 / right-05
                        :mod (h / human)
                        :ARG1-of (r4 / recognize-02
                              :ARG0 (a6 / and
                                    :op1 (l2 / law
                                          :mod (d3 / domestic))
                                    :op2 (l3 / law
                                          :mod (ii / international)))))
                  :op3 (r5 / right-05
                        :ARG1 (p4 / person
                              :mod (e / ethnic-group
                                    :name (n2 / name
                                          :op1 "Māori")))
                        :ARG1-of (a7 / articulate-01
                              :medium (w / work-of-art
                                    :name (n3 / name
                                          :op1 "Te"
                                          :op2 "Tiriti"
                                          :op3 "O"
                                          :op4 "Waitangi"))))
                  :op4 (v / value
                        :mod (d4 / democracy)
                        :ARG2-of (ii2 / include-01
                              :ARG1 (a8 / and
                                    :op1 (p5 / process-02
                                          :ARG1 (e2 / elect-01))
                                    :op2 (d5 / debate-01
                                          :ARG1-of (ii3 / inform-01)
                                          :ARG1-of (p6 / public-02)))))
                  :op5 (p7 / principle
                        :topic (a9 / and
                              :op1 (e3 / equal-01)
                              :op2 (f / fair-01)))
                  :purpose (o3 / or
                        :op1 (h2 / harm-01
                              :polarity -
                              :ARG0 s
                              :ARG1 o3
                              :op1 (ii4 / individual)
                              :op2 (g / group
                                    :mod (p8 / particular))))
                  :op2 (e4 / exclude-01
                        :ARG0 s
                        :ARG1 o3)
                  :op3 (d6 / disempower-01
                        :ARG0 s
                        :ARG1 o3)
                  :op4 (d7 / discriminate-02
                        :polarity -
                        :ARG0 s
                        :ARG1 o3))
            :ARG1-of (j2 / just-02
                  :polarity -)))


# ::id 25
# ::snt ​  ​ Social ​  ​ and ​  ​ Ethical    Implications ​  ​ of ​  ​ Autonomous ​  ​ Experimentation ​  ​ in ​  ​ AI," ​  ​​ Workshop ​  ​ on ​  ​ Fairness, ​  ​ Accountability, ​  ​ and ​  ​ Transparency ​  ​ in ​  ​ Machine    Learning ​ , ​  ​​ https://papers.ssrn.com/sol3/papers.cfm?abstract_id=2846909 ​ ​  ​ (2016).
(p / publication-91
      :ARG1 (t / thing
            :ARG0-of (ii / imply-01
                  :ARG1 (e / experiment-01
                        :ARG1 (a / autonomy)
                        :topic (ii2 / intelligent-01
                              :mod (a2 / artificial))))
            :topic (a3 / and
                  :op1 (f / fairness)
                  :op2 (a4 / accountable-02)
                  :op3 (t2 / transparency)
                  :op4 (l / learn-01))
            :topic (e2 / ethics))
      :ARG4 (u / url-entity
            :value "https://papers.ssrn.com/sol3/papers.cfm?abstract_id=2846909")
      :time (d / date-entity
            :year 2016))


# ::id 25
# ::snt ​  ​ Technical ​  ​ approaches ​  ​ that ​  ​ look ​  ​ for ​  ​ a    one-time ​  ​ “fix” ​  ​ for ​  ​ fairness ​  ​ risk ​  ​ oversimplifying ​  ​ the ​  ​ complexity ​  ​ of ​  ​ social ​  ​ systems.
(a / approach-02
      :ARG1 (t / technical)
      :ARG0-of (l / look-01
            :ARG1 (f / fix-02
                  :frequency 1
                  :purpose (f2 / fairness)
                  :ARG0-of (r / risk-01
                        :ARG2 (o / oversimplify-01
                              :ARG1 (c / complexity
                                    :poss (s / system
                                          :mod (s2 / society))))))))


# ::id 25
# ::snt ​  ​ Addressing ​  ​ fairness    meaningfully ​  ​ will ​  ​ require ​  ​ interdisciplinary ​  ​ collaboration ​  ​ and ​  ​ methods ​  ​ of ​  ​ listening ​  ​ across    different ​  ​ disciplines.
(r / require-01
      :ARG0 (a / address-02
            :ARG1 (f / fairness)
            :ARG0-of (m / meaningful-05))
      :ARG1 (a2 / and
            :op1 (c / collaborate-01
                  :mod (ii / interdisciplinary))
            :op2 (m2 / method
                  :mod (l / listen-01
                        :manner (a3 / across
                              :op1 (d / discipline
                                    :ARG1-of (d2 / differ-02)))))))


# ::id 25
# ::snt ​  ​ Most ​  ​ promisingly, ​  ​ the ​  ​ approaches ​  ​ described ​  ​ in ​  ​ this ​  ​ report    demonstrate ​  ​ that ​  ​ there ​  ​ is ​  ​ growing ​  ​ interest ​  ​ in ​  ​ developing ​  ​ AI ​  ​ that ​  ​ is ​  ​ attuned ​  ​ to ​  ​ underlying    issues ​  ​ of ​  ​ fairness ​  ​ and ​  ​ equality.
(d / demonstrate-01
      :ARG0 (a / approach-02
            :ARG1-of (d2 / describe-01
                  :ARG0 (r / report-01
                        :mod (t / this))))
      :ARG1 (g / grow-01
            :ARG1 (ii / interest-01
                  :ARG2 (d3 / develop-02
                        :ARG1 (ii2 / intelligent-01
                              :mod (a2 / artificial)
                              :ARG1-of (a3 / attune-01
                                    :ARG2 (ii3 / issue-02
                                          :ARG0 (a4 / and
                                                :op1 (f / fairness)
                                                :op2 (e / equal-01))
                                          :ARG0-of (u / underlie-01)))))))
      :ARG1-of (h / have-degree-91
            :ARG2 (p / promise-01
                  :ARG0 a)
            :ARG3 (m / most)))


# ::id 25
# ::snt ​  ​ This ​  ​ sense ​  ​ of    the ​  ​ word ​  ​ bias ​  ​ is ​  ​ closely ​  ​ linked ​  ​ to ​  ​ normative ​  ​ and ​  ​ ethical ​  ​ perspectives ​  ​ on ​  ​ fairness, ​  ​ and ​  ​ the    idea ​  ​ that ​  ​ different ​  ​ groups ​  ​ should ​  ​ be ​  ​ treated ​  ​ equally.
(l / link-01
      :ARG1 (s / sense-01
            :ARG1 (b / bias-01
                  :ARG1 (w / word
                        :mod (t / this))))
      :ARG2 (a / and
            :op1 (p / perspective
                  :mod (n / norm))
            :op2 (p2 / perspective
                  :mod (e / ethics))
            :op3 (ii / idea
                  :topic (r / recommend-01
                        :ARG1 (t2 / treat-01
                              :ARG1 (g / group
                                    :ARG1-of (d / differ-02))
                              :ARG2 (e2 / equal-01)))))
      :ARG1-of (c / close-10))


# ::id 25
# ::snt Ethics ​  ​ and ​  ​ Governance    So ​  ​ far, ​  ​ this ​  ​ report ​  ​ has ​  ​ addressed ​  ​ issues ​  ​ of ​  ​ power, ​  ​ markets, ​  ​ bias, ​  ​ fairness ​  ​ and ​  ​ rights ​  ​ and    liberties ​  ​ – ​  ​ all ​  ​ subjects ​  ​ closely ​  ​ tied ​  ​ to ​  ​ ethics.
(a / and
      :op1 (e / ethics)
      :op2 (g / govern-01)
      :op3 (a2 / address-01
            :ARG0 (r / report-01
                  :mod (t / this))
            :ARG1 (ii / issue-02
                  :ARG0 (a3 / and
                        :op1 (p / power)
                        :op2 (m / market)
                        :op3 (b / bias-01)
                        :op4 (f / fairness)
                        :op5 (r2 / right-05)
                        :op6 (l / liberty)
                        :mod (a4 / all)
                        :ARG1-of (t2 / tie-01
                              :ARG2 (e2 / ethics))))
            :ARG1-of (c / close-10))
      :mod (s / so))


# ::id 25
# ::snt ​  ​ 3 ​  ​ (2006):    709-738; ​  ​ David ​  ​ Weisburd, ​  ​ "Does ​  ​ Hot ​  ​ Spots ​  ​ Policing ​  ​ Inevitably ​  ​ Lead ​  ​ to ​  ​ Unfair ​  ​ and ​  ​ Abusive ​  ​ Police ​  ​ Practices, ​  ​ or ​  ​ Can ​  ​ We    Maximize ​  ​ Both ​  ​ Fairness ​  ​ and ​  ​ Effectiveness ​  ​ in ​  ​ the ​  ​ New ​  ​ Proactive ​  ​ Policing," ​  ​​ University ​  ​ of ​  ​ Chicago ​  ​ Legal ​  ​ Forum ​ ​  ​ (2016):    661-689.
(p / publication-91
      :ARG0 (p2 / person
            :name (n / name
                  :op1 "David"
                  :op2 "Weisburd"))
      :ARG1 (p3 / publication
            :name (n2 / name
                  :op1 "Does"
                  :op2 "polarity"
                  :op3 "lead-03 :ARG0 ( spot :ARG1-of ( hot-05 ) :mod ( police_0 ) ) :ARG2 ( or :op1 ( practice-01_0 :ARG0 police_0 :ARG1-of ( fair-01_0 :polarity - ) ) :op2 ( practice-01_1 :ARG0 ( police_1 ) :ARG1-of ( abuse-01 ) ) :op3 ( possible-01 :ARG1 ( maximize-01 :ARG0 ( we ) :ARG1 ( effective-04 :ARG0 we :ARG1 ( and :op1 ( fairness_1 ) :op2 effective-04 ) ) ) ) ) :ARG1-of ( avoid-01 :ARG1-of ( possible-01 :polarity - ) ) ) ) :ARG4 ( conference :name ( name_2 :op1 ")))


# ::id 25
# ::snt ​  ​ Among ​  ​ conferences, ​  ​ the  88  Fairness, ​  ​ Accountability, ​  ​ and ​  ​ Transparency ​  ​ in ​  ​ Machine ​  ​ Learning ​  ​ (FAT/ML ​  ​ and ​  ​ now ​  ​ FAT* ​  ​ )    Conferences ​  ​ are ​  ​ notable ​  ​ for ​  ​ a ​  ​ focus ​  ​ on ​  ​ technical ​  ​ research ​  ​ and ​  ​ experimentation ​  ​ dedicated    to ​  ​ making ​  ​ AI ​  ​ more ​  ​ inclusive, ​  ​ legible ​  ​ and ​  ​ representative.
(ii / include-91
      :ARG1 (c / conference
            :ARG1-of (d / dedicate-01
                  :ARG2 (m / make-02
                        :ARG1 (a / and
                              :op1 (h / have-degree-91
                                    :ARG1 (ii2 / intelligent-01
                                          :mod (a2 / artificial))
                                    :ARG2 (ii3 / inclusive)
                                    :ARG3 (m2 / more))
                              :op2 (p / possible-01
                                    :ARG1 (l / legible))
                              :op3 (r / represent-01
                                    :ARG0 ii2)))
                  :ARG2-of (f / focus-01
                        :ARG1 (a3 / and
                              :op1 (r2 / research-01
                                    :ARG1 (t / technical))
                              :op2 (e / experiment-01)))))
      :ARG2 (c2 / conference
            :time (d2 / date-entity
                  :year 1988))
      :ARG3 (n / notable-04
            :ARG1 c))


# ::id 25
# ::snt 89  3rd ​ ​  ​​ Workshop ​  ​ on ​  ​ Fairness, ​  ​ Accountability, ​  ​ and ​  ​ Transparency ​  ​ in ​  ​ Machine ​  ​ Learning, ​  ​ New ​  ​ York, ​  ​ November ​  ​ 18, ​  ​ 2016,    http://www.fatml.org/ ​ .
(w / workshop
      :li 89
      :topic (a / and
            :op1 (f / fairness)
            :op2 (a2 / accountable-02)
            :op3 (t / transparency)
            :location (m / machine)
            :location (l / learn-01))
      :location (c / city
            :name (n / name
                  :op1 "New"
                  :op2 "York"))
      :time (d / date-entity
            :month 11
            :day 18
            :year 2016)
      :medium (u / url-entity
            :value "http://www.fatml.org/"))


# ::id 25
# ::snt ​  ​ Due ​  ​ to    misaligned ​  ​ interests ​  ​ and ​  ​ the ​  ​ information ​  ​ asymmetry ​  ​ that ​  ​ AI ​  ​ exacerbates ​  ​ in ​  ​ these    industries, ​  ​ new ​  ​ incentives ​  ​ for ​  ​ fairness ​  ​ and ​  ​ new ​  ​ methods ​  ​ for ​  ​ validating ​  ​ fair ​  ​ practices ​  ​ need    83  Andreas ​  ​ Holzinger, ​  ​ "Interactive ​  ​ machine ​  ​ learning ​  ​ for ​  ​ health ​  ​ informatics: ​  ​ when ​  ​ do ​  ​ we ​  ​ need ​  ​ the ​  ​ human-in-the-loop?," ​  ​​ Brain    Informatics ​ ​  ​ 3, ​  ​ No.
(a / and
      :op1 (c / cause-01
            :ARG0 (a2 / and
                  :op1 (a3 / align-01
                        :polarity -
                        :ARG1 (ii / interest-01))
                  :op2 (a4 / asymmetry))
            :ARG1 (e / exacerbate-01
                  :ARG0 (ii2 / intelligent-01
                        :mod (a5 / artificial))
                  :ARG1 (ii3 / industry
                        :mod (t / this))))
      :op2 (a6 / and
            :op1 (t2 / thing
                  :ARG0-of (ii4 / incentivize-01
                        :ARG2 (f / fairness))
                  :ARG1-of (n / new-01))
            :op2 (m / method
                  :ARG1-of (n2 / new-01)
                  :purpose (v / validate-01
                        :ARG1 (p / practice-01
                              :ARG1-of (f2 / fair-01)))))
      :ARG1-of (d / describe-01
            :ARG0 (p2 / publication-91
                  :ARG0 (p3 / person
                        :name (n3 / name
                              :op1 "Andreas"
                              :op2 "Holzinger"))
                  :ARG1 (p4 / publication
                        :name (n4 / name
                              :op1 "Interactive"
                              :op2 "Machine"
                              :op3 "Learning"
                              :op4 "for"
                              :op5 "Health"
                              :op6 ","
                              :op7 "Informatics"))
                  :ARG7 (c2 / chapter
                        :mod 3))
            :time (n5 / need-01
                  :ARG0 (w / we)
                  :ARG1 (h / human
                        :location (l / loop)))))


# ::id 25
# ::snt 91  Part ​  ​ of ​  ​ the ​  ​ fundamental ​  ​ difficulty ​  ​ in ​  ​ defining, ​  ​ understanding ​  ​ and ​  ​ measuring ​  ​ bias ​  ​ stems    from ​  ​ the ​  ​ contentious ​  ​ and ​  ​ conceptually ​  ​ difficult ​  ​ task ​  ​ of ​  ​ defining ​  ​ fairness.
(s / stem-01
      :li 91
      :ARG1 (p / part
            :part-of (d / difficult
                  :domain (a / and
                        :op1 (d2 / define-01
                              :ARG1 (b / bias-01))
                        :op2 (u / understand-01
                              :ARG1 b)
                        :op3 (m / measure-01
                              :ARG1 b))
                  :mod (f / fundamental)))
      :ARG2 (a2 / and
            :op1 (c / contentious)
            :op2 (t / task-01
                  :ARG1 (d3 / define-01
                        :ARG1 (f2 / fairness))
                  :mod (d4 / difficult)
                  :mod (c2 / conceptual))))


# ::id 25
# ::snt ​  ​ Tradeoffs ​  ​ are    inherent ​  ​ in ​  ​ the ​  ​ adoption ​  ​ of ​  ​ particular ​  ​ fairness ​  ​ definitions, ​  ​ possibly ​  ​ perpetuating ​  ​ particular    biases ​  ​ in ​  ​ the ​  ​ service ​  ​ of ​  ​ addressing ​  ​ others.
(t / tradeoff-01
      :ARG1-of (m / mean-01
            :ARG2 (ii / inherent
                  :domain (a / adopt-01
                        :ARG1 (d / define-01
                              :ARG1 (f / fairness)
                              :mod (p / particular))
                        :ARG0-of (p2 / perpetuate-01
                              :ARG1 (b / bias-01
                                    :mod (p3 / particular))
                              :ARG1-of (p4 / possible-01)
                              :location (s / serve-01
                                    :ARG1 (a2 / address-02
                                          :ARG1 (o / other))))))))


# ::id 25
# ::snt ​  ​ Recent ​  ​ efforts ​  ​ have ​  ​ sought ​  ​ to ​  ​ implement  92  fairness ​  ​ by ​  ​ mathematically ​  ​ specifying ​  ​ social ​  ​ norms ​  ​ and ​  ​ values, ​  ​ then ​  ​ using ​  ​ those    specifications ​  ​ as ​  ​ constraints ​  ​ when ​  ​ training ​  ​ AI ​  ​ systems.
(s / seek-01
      :ARG0 (e / effort-01
            :time (r / recent))
      :ARG1 (ii / implement-01
            :ARG0 e
            :ARG1 (f / fairness
                  :li 92)
            :manner (a / and
                  :op1 (s2 / specify-01
                        :ARG0 e
                        :ARG1 (a2 / and
                              :op1 (n / norm
                                    :mod (s3 / society))
                              :op2 (v / value
                                    :mod s3))
                        :manner (m / mathematics))
                  :op2 (u / use-01
                        :ARG0 e
                        :ARG1 (s4 / specification
                              :mod (t / that))
                        :ARG2 (c / constrain-01)
                        :time (t2 / train-01
                              :ARG1 (s5 / system
                                    :mod (ii2 / intelligent-01
                                          :mod (a3 / artificial))))))))


# ::id 25
# ::snt ​  ​ Steven ​  ​ Wu, ​  ​ "Fairness    Incentives ​  ​ for ​  ​ Myopic ​  ​ Agents," ​  ​ arXiv ​  ​ preprint ​  ​ arXiv:1705.02321 ​  ​ (2017); ​  ​ Julia ​  ​ Lane, ​  ​ "Perspective: ​  ​ Fix ​  ​ the ​  ​ incentives," ​  ​​ Nature    537, ​  ​ No.
(a / and
      :op1 (p / publication-91
            :ARG0 (p2 / person
                  :name (n / name
                        :op1 "Steven"
                        :op2 "Wu"))
            :ARG1 (p3 / publication
                  :name (n2 / name
                        :op1 "Fairness"
                        :op2 " :op3 "))))


# ::id 25
# ::snt ​  ​ 12 ​  ​ (2016): ​  ​ 102-108; ​  ​ Shahin ​  ​ Jabbari, ​  ​ Matthew ​  ​ Joseph, ​  ​ Michael    Kearns, ​  ​ Jamie ​  ​ Morgenstern ​  ​ and ​  ​ Aaron ​  ​ Roth, ​  ​ "Fair ​  ​ Learning ​  ​ in ​  ​ Markovian ​  ​ Environments," ​  ​ arXiv ​  ​ preprint ​  ​ arXiv:1611.03071    (2016); ​  ​ Matthew ​  ​ Joseph, ​  ​ Michael ​  ​ Kearns, ​  ​ Jamie ​  ​ Morgenstern, ​  ​ Seth ​  ​ Neel ​  ​ and ​  ​ Aaron ​  ​ Roth, ​  ​ "Rawlsian ​  ​ fairness ​  ​ for ​  ​ machine    learning," ​  ​ arXiv ​  ​ preprint ​  ​ arXiv:1610.09559 ​  ​ (2016).
(p / publication-91
      :ARG0 (a / and
            :op1 (p2 / publication
                  :name (n / name
                        :op1 "Shahin"
                        :op2 "Jabbari"))
            :op2 (p3 / publication
                  :name (n2 / name
                        :op1 "Matthew"
                        :op2 "  :op3 "
                        :op3 "Morgenstern"))
            :op5 (p4 / publication
                  :name (n3 / name
                        :op1 "Seth"
                        :op2 "Neel"))
            :op6 (p5 / publication
                  :name (n4 / name
                        :op1 "Aaron"
                        :op2 "  :op3 "))))


# ::id 25
# ::snt Separately, ​  ​ criminologist ​  ​ Richard ​  ​ Berk ​  ​ and ​  ​ his ​  ​ colleagues ​  ​ argue ​  ​ that ​  ​ there ​  ​ are ​  ​ intractable    tradeoffs ​  ​ between ​  ​ accuracy ​  ​ and ​  ​ fairness—the ​  ​ occurrence ​  ​ of ​  ​ false ​  ​ positives ​  ​ and    negatives—in ​  ​ populations ​  ​ where ​  ​ base ​  ​ rates ​  ​ (the ​  ​ percentage ​  ​ of ​  ​ a ​  ​ given ​  ​ population ​  ​ that ​  ​ fall    into ​  ​ a ​  ​ specific ​  ​ category) ​  ​ vary ​  ​ between ​  ​ different ​  ​ social ​  ​ groups.
(a / argue-01
      :ARG0 (a2 / and
            :op1 (p / person
                  :name (n / name
                        :op1 "Richard"
                        :op2 "Berk")
                  :ARG0-of (h / have-org-role-91
                        :ARG2 (c / criminologist)))
            :op2 (p2 / person
                  :ARG0-of (h2 / have-rel-role-91
                        :ARG1 p
                        :ARG2 (c2 / colleague))))
      :ARG1 (t / tradeoff-02
            :ARG0 (b / between
                  :op1 (a3 / accurate)
                  :op2 (f / fairness))
            :ARG1 (a4 / and
                  :op1 (p3 / positive
                        :mod (f2 / false))
                  :op2 (n2 / negative-02))
            :location (p4 / population
                  :location-of (b2 / base-02
                        :ARG1 (r / rate)))
            :ARG1-of (m / mean-01
                  :ARG2 (p5 / percentage
                        :quant-of (p6 / population
                              :ARG1-of (g / give-01)
                              :ARG1-of (f3 / fall-04
                                    :ARG2 (c3 / category
                                          :ARG1-of (s / specific-02)
                                          :mod (s2 / social)))))))
      :ARG1-of (p7 / possible-01
            :polarity -
            :ARG1-of (s3 / separate-02)))


# ::id 25
# ::snt ​  ​ Difficult ​  ​ decisions ​  ​ need ​  ​ to  137  be ​  ​ made ​  ​ about ​  ​ how ​  ​ we ​  ​ value ​  ​ fairness ​  ​ and ​  ​ accuracy ​  ​ in ​  ​ risk ​  ​ assessment.
(n / need-01
      :ARG0 (d / decide-01
            :ARG1-of (d2 / difficult-02))
      :ARG1 (m / make-01
            :ARG1 137
            :ARG1 (t / thing
                  :manner-of (v / value-02
                        :ARG0 (w / we)
                        :ARG1 (a / and
                              :op1 (f / fairness)
                              :op2 (a2 / accurate))
                        :prep-in (a3 / assess-01
                              :ARG1 (r / risk-01))))))


# ::id 25
# ::snt 137  Richard ​  ​ Berk, ​  ​ Hoda ​  ​ Heidari, ​  ​ Shahin ​  ​ Jabbari, ​  ​ Michael ​  ​ Kearns ​  ​ and ​  ​ Aaron ​  ​ Roth, ​  ​ “Fairness ​  ​ in ​  ​ Criminal ​  ​ Justice ​  ​ Risk ​  ​ Assessments:    The ​  ​ State ​  ​ of ​  ​ the ​  ​ Art,” ​  ​ arXiv:1703.09207, ​  ​ March ​  ​ 27, ​  ​ 2017.
(p / publication-91
      :ARG7 137
      :ARG0 (a / and
            :op1 (p2 / person
                  :name (n / name
                        :op1 "Richard"
                        :op2 "Berk"))
            :op2 (p3 / person
                  :name (n2 / name
                        :op1 "Hoda"
                        :op2 "Hoda"))
            :op3 (p4 / person
                  :name (n3 / name
                        :op1 "Heidari"))
            :op4 (p5 / person
                  :name (n4 / name
                        :op1 "Shahin"
                        :op2 "Jabbari"))
            :op5 (p6 / person
                  :name (n5 / name
                        :op1 "Michael"
                        :op2 "Kearns"))
            :op6 (p7 / person
                  :name (n6 / name
                        :op1 "Aaron"
                        :op2 "Roth")))
      :ARG1 (p8 / publication
            :name (n7 / name
                  :op1 "Fairness"
                  :op2 "in"
                  :op3 "Criminal"
                  :op4 "Justice"
                  :op5 ""
                  :op6 ""
                  :op7 "Risk"))
      :ARG4 (j / journal
            :name (n8 / name
                  :op1 "ArXiv"))
      :time (d / date-entity
            :day 27
            :month 3
            :year 2017))


# ::id 25
# ::snt 90  JM ​  ​ Schumacher, ​  ​ “Linear ​  ​ Versus ​  ​ Nonlinear ​  ​ Allocation ​  ​ Rules ​  ​ in ​  ​ Risk ​  ​ Sharing ​  ​ Under ​  ​ Financial ​  ​ Fairness,” ​  ​ (March ​  ​ 2, ​  ​ 2017),    http://dx.doi.org/10.2139/ssrn.2892760 ​ .
(p / publication-91
      :li 90
      :ARG0 (p2 / person
            :name (n / name
                  :op1 "JM"
                  :op2 "Schumacher"))
      :ARG1 (p3 / publication
            :name n
            :op1 "Linear"
            :op2 "Versus"
            :op3 "Nonlinear"
            :op4 "Allocation"
            :op5 "Rules"
            :op6 "in"
            :op7 "the"
            :op8 "Risk"
            :op9 "Under"
            :op10 "Financial"
            :op11 "Fairness")
      :ARG4 (u / url-entity
            :value "http://dx.doi.org/10.2139/ssrn.2892760")
      :time (d / date-entity
            :day 2
            :month 3
            :year 2017))


# ::id 26
# ::snt For example, making a facial recognition system perform equally on people with light and dark skin may be a type of technical progress in terms of parity, but if that technology is disproportionately used on people of color and low-income communities, is it really “fair?” This is why deﬁnitions of fairness face a hard limit if they remain purely contained within the technical domain: in short, “parity is not justice.” 169 32 
 3.2  Infrastructural Thinking  In order to better understand and track the complexities of AI systems, we need to look beyond the technology and the hype to account for the broader context of how AI is shaping and shaped by social and material forces.
(m / multi-sentence
      :snt1 (e / exemplify-01
            :ARG0 (c / contrast-01
                  :ARG1 (p / possible-01
                        :ARG1 (p2 / progress-01
                              :ARG1 (m2 / make-02
                                    :ARG1 (p3 / perform-02
                                          :ARG0 (s / system
                                                :ARG0-of (r / recognize-01
                                                      :ARG1 (f / face)))
                                          :ARG3 (p4 / person
                                                :ARG0-of (h / have-03
                                                      :ARG1 (a / and
                                                            :op1 (s2 / skin
                                                                  :ARG1-of (l / light-06))
                                                            :op2 (s3 / skin
                                                                  :ARG1-of (d / dark-02))))))
                                    :mod (t / technical)
                                    :mod (t2 / type)))
                        :ARG2 (h2 / have-condition-91
                              :ARG1 (f2 / fair-01
                                    :ARG1 m2
                                    :ARG1-of (r2 / real-04)
                                    :polarity (a2 / amr-unknown))
                              :ARG2 (u / use-01
                                    :ARG1 (t3 / technology)
                                    :ARG2 (a3 / and
                                          :op1 (p5 / person
                                                :mod (c2 / color))
                                          :op2 (c3 / community
                                                :mod (ii / income
                                                      :ARG1-of (l2 / low-04))))
                                    :manner (p6 / proportionate
                                          :polarity -)))))
            :snt2 (c4 / cause-01
                  :ARG0 (t4 / this)
                  :ARG1 (f3 / face-01
                        :ARG0 (d2 / define-01
                              :ARG1 (f4 / fairness))
                        :ARG1 (l3 / limit
                              :ARG1-of (h3 / hard-02))
                        :condition (r3 / remain-01
                              :ARG1 d2
                              :ARG3 (c5 / contain-01
                                    :ARG1 d2
                                    :ARG2 (d3 / domain
                                          :mod (t5 / technical))))
                        :mod (ii2 / in-short)))
            :snt3 (c6 / cite-01
                  :ARG1 (p7 / publication
                        :ARG2 (a4 / and
                              :op1 169
                              :op2 32
                              :op3 3.2))
                  :ARG1 p7)
            :snt4 (n / need-01
                  :ARG0 (w / we)
                  :ARG1 (l4 / look-01
                        :ARG0 w
                        :ARG1 (b / beyond
                              :op1 (a5 / and
                                    :op1 (t6 / technology)
                                    :op2 (h4 / hype-01)))
                        :purpose (a6 / account-01
                              :ARG0 w
                              :ARG1 (c7 / context
                                    :ARG1-of (h5 / have-degree-91
                                          :ARG2 (b2 / broad-02
                                                :ARG1 c7)
                                          :ARG3 (m3 / more))
                                    :topic (a7 / and
                                          :op1 (s4 / shape-01
                                                :ARG0 (f5 / force
                                                      :ARG0-of (s5 / social-03))
                                                :ARG1 (ii3 / intelligent-01
                                                      :mod (a8 / artificial)))
                                          :op2 (s6 / shape-01
                                                :ARG0 (f6 / force
                                                      :mod (m4 / material)))
                                          :ARG1 ii3))
                              :op2 (j / justice
                                    :polarity -))))))


# ::id 26
# ::snt But this is still an uphill battle: while there is increased attention to problems of bias in AI systems, we have yet to see much research within the fairness and bias debate focused on the state of equity and diversity in the AI ﬁeld itself.
(c / contrast-01
      :ARG2 (b / battle-01
            :mod (t / this)
            :mod (s / still)
            :mod (u / uphill)
            :ARG1-of (m / mean-01
                  :ARG2 (c2 / contrast-01
                        :ARG1 (a / attend-02
                              :ARG1 (p / problem
                                    :topic (b2 / bias-01
                                          :ARG1 (s2 / system
                                                :mod (a2 / artificial))))
                              :ARG1-of (ii / increase-01))
                        :ARG2 (h / have-11
                              :ARG0 (w / we)
                              :ARG1 (y / yet)
                              :ARG2 (s3 / see-01
                                    :ARG0 w
                                    :ARG1 (r / research-01
                                          :quant (m2 / much)
                                          :location (d / debate-01
                                                :ARG1 (a3 / and
                                                      :op1 (f / fairness)
                                                      :op2 (b3 / bias-01))
                                                :ARG1-of (f2 / focus-01
                                                      :ARG2 (s4 / state
                                                            :mod (a4 / and
                                                                  :op1 (e / equity)
                                                                  :op2 (d2 / diversity))
                                                            :poss s2))))))))))


# ::id 26
# ::snt We cannot see the global environmental and labor implications of these tools of everyday convenience, nor can we meaningfully advocate for fairness, accountability, and transparency in AI systems, without an understanding of this full stack supply chain.
(a / and
      :op1 (p / possible-01
            :polarity -
            :ARG1 (s / see-01
                  :ARG0 (w / we)
                  :ARG1 (ii / implicate-01
                        :ARG1 (t / tool
                              :mod (c / convenience
                                    :mod (e / everyday))
                              :mod (t2 / this))
                        :ARG2 (a2 / and
                              :op1 (e2 / environment)
                              :op2 (l / labor-01)
                              :mod (g / globe)))))
      :op2 (p2 / possible-01
            :polarity -
            :ARG1 (a3 / advocate-01
                  :ARG0 w
                  :ARG1 (a4 / and
                        :op1 (f / fairness)
                        :op2 (a5 / accountable-02)
                        :op3 (t3 / transparency)
                        :topic (s2 / system
                              :mod (ii2 / intelligent-01
                                    :mod (a6 / artificial))))
                  :ARG0-of (m / meaningful-05)))
      :manner (u / understand-01
            :polarity -
            :ARG0 w
            :ARG1 (c2 / chain
                  :mod (s3 / supply-01)
                  :mod (s4 / stack
                        :ARG1-of (f2 / full-09))
                  :mod (t4 / this))))


# ::id 26
# ::snt Furthermore, fairness is a term that can be easily co-opted: important questions such as “Fair to whom?
(a / and
      :op2 (t / term
            :domain (f / fairness)
            :ARG1-of (c / coopt-01
                  :ARG1-of (p / possible-01)
                  :ARG1-of (e / easy-05))
            :ARG1-of (m / mean-01
                  :ARG2 (q / question-01
                        :ARG1-of (ii / important-01)
                        :example (f2 / fair-01
                              :ARG2 (a2 / amr-unknown))))))


# ::id 26
# ::snt WHAT IS NEEDED NEXT32 3.1  From Fairness to Justice32 3.2  Infrastructural Thinking33 3.3  Accounting for Hidden Labor in AI Systems34 3.4  Deeper Interdisciplinarity36 3.5  Race, Gender and Power in AI37 3.6  Strategic Litigation and Policy Interventions39 3.7  Research and Organizing: An Emergent Coalition40 CONCLUSION42 ENDNOTES44       This work is licensed under a  Creative Commons Attribution-NoDerivatives 4.0 International License  2 
ABOUT THE AI NOW INSTITUTE   The AI Now Institute at New York University is an interdisciplinary research institute dedicated to understanding the social implications of AI technologies.
(m / multi-sentence
      :snt1 (n / need-01
            :ARG1 (a / amr-unknown)
            :time (n2 / next)
            :example (a2 / and
                  :op1 (r / research-institute
                        :name (n3 / name
                              :op1 "AI"
                              :op2 "Now"
                              :op3 "Institute")
                        :ARG1-of (d / dedicate-01
                              :ARG2 (u / understand-01
                                    :ARG0 r
                                    :ARG1 (ii / implicate-01
                                          :ARG0 (t / technology
                                                :mod (a3 / artificial))
                                          :ARG1 (s / society)))))
                  :op2 (a4 / and
                        :op1 (r2 / race)
                        :op2 (g / gender)
                        :op3 (p / power)
                        :topic t)
                  :example (a5 / and
                        :op1 (r3 / research-institute)
                        :op2 (o / organize-01)
                        :op3 (c / coalition
                              :ARG1-of (e / emerge-02)))
                  :example (a6 / and
                        :op1 (r4 / research-institute
                              :name (n4 / name
                                    :op1 "Advanced"
                                    :op2 "Technology"
                                    :op3 "Infrastructure"))
                        :op2 (t2 / think-01
                              :mod (ii2 / infrastructure))
                        :example (a7 / and
                              :op1 (n5 / number
                                    :value 32)
                              :op2 (n6 / number
                                    :value 3.2))
                        :example (n7 / number
                              :value 33)
                        :example (n8 / number
                              :value 3.3)
                        :example (a8 / account-01
                              :ARG1 (l / labor-01
                                    :ARG1-of (h / hide-01)
                                    :mod (s2 / system
                                          :mod a3)))
                        :example n8
                        :value 36)
                  :example (n9 / number
                        :value 3.6)
                  :example (n10 / number
                        :value 39)
                  :example (n11 / number
                        :value 44)))
      :snt2 (l2 / license-01
            :ARG1 (w / work-01
                  :mod (t3 / this))
            :ARG3 (t4 / thing
                  :name (n12 / name
                        :op1 "CCP"
                        :op2 " Attribution-NoDerivatives")
                  :ARG1-of (m2 / mean-01
                        :ARG2 (p2 / publication
                              :name (n13 / name
                                    :op1 "Canadian"
                                    :op2 "International"
                                    :op3 "Licence"))))))


# ::id 26
# ::snt 3.1  From Fairness to Justice  Any debate about bias and fairness should approach issues of power and hierarchy, looking at who is in a position to produce and proﬁt from these systems, whose values are embedded in these systems, who sets their “objective functions,” and which contexts they are intended to work within.
(r / recommend-01
      :li 3.1
      :ARG1 (a / approach-02
            :ARG0 (d / debate-01
                  :ARG1 (a2 / and
                        :op1 (b / bias-01)
                        :op2 (f / fairness))
                  :mod (a3 / any))
            :ARG1 (ii / issue-02
                  :ARG0 (a4 / and
                        :op1 (p / power)
                        :op2 (h / hierarchy)))
            :manner (l / look-01
                  :ARG0 d
                  :ARG1 (a5 / and
                        :op1 (p2 / person
                              :ARG1-of (p3 / position-01
                                    :ARG2 (a6 / and
                                          :op1 (p4 / produce-01
                                                :ARG0 p2
                                                :ARG2 (s / system
                                                      :mod (t / this)))
                                          :op2 (f2 / favor-01
                                                :ARG0 p2
                                                :ARG1 s))))
                        :op2 (p5 / person
                              :ARG0-of (v / value-01
                                    :ARG1-of (e / embed-01
                                          :ARG2 s)))
                        :op3 (p6 / person
                              :ARG0-of (s2 / set-02
                                    :ARG1 (f3 / function-01
                                          :ARG0 s
                                          :mod (o / objective))))
                        :op4 (c / context
                              :location-of (w / work-01
                                    :ARG0 (t2 / they)
                                    :ARG1-of (ii2 / intend-01)))))))


# ::id 26
# ::snt WHAT IS NEEDED NEXT  When we released our AI Now 2016 Report, fairness formulas, debiasing toolkits, and ethical guidelines for AI were rare.
(n / need-01
      :ARG1 (a / amr-unknown)
      :time (n2 / next)
      :time (r / release-01
            :ARG0 (w / we)
            :ARG1 (r2 / report
                  :name (n3 / name
                        :op1 "AI"
                        :op2 "Now"
                        :op3 "2016")
                  :poss w))
      :ARG1-of (c / cause-01
            :ARG0 (r3 / rare-02
                  :ARG1 (a2 / and
                        :op1 (f / formula
                              :mod (f2 / fairness))
                        :op2 (t / toolkit
                              :ARG0-of (d / debilitate-01))
                        :op3 (g / guideline
                              :mod (e / ethics)
                              :topic (ii / intelligent-01
                                    :mod (a3 / artificial)))))))


# ::id 26
# ::snt Rather than relying on quick ﬁxes, tools, and certiﬁcations, issues of bias and fairness require deeper consideration and more robust accountability frameworks, including strong disclaimers about how “automated fairness” cannot be relied on to truly eliminate bias from AI systems.
(r / require-01
      :ARG0 (ii / issue-02
            :ARG0 (a / and
                  :op1 (b / bias-01)
                  :op2 (f / fairness)))
      :ARG1 (a2 / and
            :op1 (c / consider-02
                  :ARG1 ii
                  :ARG1-of (h / have-degree-91
                        :ARG2 (d / deep-02
                              :ARG1 c)
                        :ARG3 (m / more)))
            :op2 (f2 / framework
                  :purpose (a3 / account-01)
                  :ARG2-of (ii2 / include-01
                        :ARG1 (d2 / disclaim-01
                              :ARG1 (p / possible-01
                                    :polarity -
                                    :ARG1 (r2 / rely-01
                                          :ARG1 (f3 / fairness
                                                :mod (a4 / automatic))
                                          :ARG2 (e / eliminate-01
                                                :ARG0 f3
                                                :ARG1 (b2 / bias-01)
                                                :ARG2 (s / system
                                                      :mod (ii3 / intelligent-01
                                                            :mod (a5 / artificial)))
                                                :ARG1-of (t / true-01))))
                              :ARG1-of (s2 / strong-02)))
                  :ARG1-of (h2 / have-degree-91
                        :ARG2 (r3 / robust)
                        :ARG3 (m2 / more))))
      :ARG1-of (ii4 / instead-of-91
            :ARG2 (r4 / rely-01
                  :ARG1 (a6 / and
                        :op1 (t2 / thing
                              :ARG1-of (t3 / thing-01
                                    :ARG1-of (m3 / mark-01)))
                        :op2 (t4 / tool)
                        :op3 (t5 / thing
                              :ARG1-of (c2 / certify-01))
                        :ARG1-of (q / quick-02)))))


# ::id 26
# ::snt Combining “academically credible” technical fairness ﬁxes and certiﬁcation check boxes runs the risk of instrumenting fairness in ways that lets industry say it has ﬁxed these problems and may divert attention from examining ongoing harms.
(r / risk-01
      :ARG2 (ii / instrument-01
            :ARG1 (f / fairness)
            :manner (w / way
                  :ARG0-of (l / let-01
                        :ARG1 (s / say-01
                              :ARG0 (ii2 / industry)
                              :ARG1 (s2 / solve-01
                                    :ARG0 ii2
                                    :ARG1 (p / problem
                                          :mod (t / this)))))
                  :ARG0-of (d / divert-01
                        :ARG1 (a / attend-02)
                        :ARG2 (e / examine-01
                              :ARG1 (h / harm-01
                                    :ARG1-of (g / go-on-15)))
                        :ARG1-of (p2 / possible-01))))
      :ARG3 (c / combine-01
            :ARG1 (f2 / fairness
                  :mod (t2 / technical)
                  :mod (c2 / credible
                        :mod (a2 / academia)))
            :ARG2 (b / box
                  :ARG0-of (c3 / check-01)
                  :mod (c4 / citizenship))))


# ::id 26
# ::snt 168 To this end, our deﬁnitions of “fairness” must expand to encompass the structural, historical, and political contexts in which an algorithmic systems is deployed.
(o / obligate-01
      :li 168
      :ARG2 (e / expand-01
            :ARG1 (d / define-01
                  :ARG0 (w / we)
                  :ARG1 (f / fairness))
            :ARG4 (e2 / encompass-01
                  :ARG0 d
                  :ARG1 (a / and
                        :op1 (c / context
                              :mod (s / structure))
                        :op2 (c2 / context
                              :mod (h / history))
                        :op3 (c3 / context
                              :mod (p / politics))
                        :location-of (d2 / deploy-01
                              :ARG1 (s2 / system
                                    :mod (a2 / algorithm))))))
      :purpose (e3 / end
            :mod (t / this)))


# ::id 26
# ::snt Second, few government agencies had invested real efforts to ensure that fairness and due process protections remained in place when switching from human-driven decisions to algorithmically-driven ones.
(ii / invest-01
      :li 2
      :ARG0 (a / agency
            :mod (g / government-organization
                  :ARG0-of (g2 / govern-01))
            :quant (f / few))
      :ARG1 (e / effort-01
            :ARG0 a
            :ARG1 (e2 / ensure-01
                  :ARG0 a
                  :ARG1 (r / remain-01
                        :ARG1 (a2 / and
                              :op1 (f2 / fairness)
                              :op2 (p / protect-01
                                    :ARG1 (d / due-process)))
                        :ARG3 (ii2 / in-place)))
            :ARG1-of (r2 / real-04))
      :time (s / switch-01
            :ARG0 a
            :ARG1 (d2 / decide-01)
            :ARG2 (d3 / decide-01
                  :ARG1-of (d4 / drive-02
                        :ARG0 (h / human)))
            :ARG3 (d5 / decide-01
                  :ARG1-of (d6 / drive-02
                        :ARG0 (a3 / algorithm)))))


# ::id 26
# ::snt 139.Animesh Singh and Michael Hind, “AI Fairness 360: Attacking Bias from All Angles!,”  IBM Developer , September 19, 2018, https://developer.ibm.com/blogs/2018/09/19/ai-fairness-360-attacking-bias-from-all-angles/ .
(p / publication-91
      :ARG0 (a / and
            :op1 (p2 / person
                  :name (n / name
                        :op1 "Animesh"
                        :op2 "Singh"))
            :op2 (p3 / person
                  :name (n2 / name
                        :op1 "Michael"
                        :op2 "Hind")))
      :ARG1 (p4 / publication
            :name (n3 / name
                  :op1 "Attacking"
                  :op2 "Bias"
                  :op3 360
                  :op4 "from"
                  :op5 "All"
                  :op6 "Angles"))
      :ARG4 (c / company
            :name (n4 / name
                  :op1 "IBM")
            :ARG0-of (d / develop-02))
      :ARG7 (u / url-entity
            :value "https://developer.ibm.com/blogs/2018/09/19/ai-fairness-360-attacking-bias-from-all-angles/")
      :time (d2 / date-entity
            :month 9
            :day 19
            :year 2018))


# ::id 26
# ::snt 70.“Litigating Algorithms: Challenging Government Use of Algorithmic Decision Systems” (New York: AI Now Institute, September 2018),   https://ainowinstitute.org/litigatingalgorithms.pdf ; Dillon Reisman, Jason Schultz, Kate Crawford, and Meredith Whittaker, “Algorithmic Impact Assessments: A Practical Framework for Public Agency Accountability” (New York: AI Now Institute, April 2018), https://ainowinstitute.org/aiareport2018.pdf ; Micah Altman, Alexandra Wood, and Effy Vayena, “A Harm-Reduction Framework for Algorithmic Fairness,”  IEEE Security Privacy  16, no.
(p / publication-91
      :li 70
      :ARG0 (a / and
            :op1 (p2 / person
                  :name (n / name
                        :op1 "Dillon"
                        :op2 "Reisman"))
            :op2 (p3 / person
                  :name (n2 / name
                        :op1 "Jason"
                        :op2 "Schultz"))
            :op3 (p4 / person
                  :name (n3 / name
                        :op1 "Kate"
                        :op2 "Crawford"))
            :op4 (p5 / person
                  :name (n4 / name
                        :op1 "Meredith"
                        :op2 "Whittaker")))
      :ARG1 (p6 / publication
            :name (n5 / name
                  :op1 "Litigating"
                  :op2 "Algorithms"
                  :op3 "Challenging"
                  :op4 "Use"
                  :op5 "of"
                  :op6 "Algorithmic"
                  :op7 "Decision"
                  :op8 "Systems"))
      :ARG4 (p7 / publication
            :name (n6 / name
                  :op1 "IAI"
                  :op2 "Security"
                  :op3 "Privacy"
                  :op4 "16,"
                  :op5 "No."))
      :ARG4 (p8 / publication
            :name (n7 / name
                  :op1 "A"
                  :op2 "Harm-Reduction"
                  :op3 "Framework"
                  :op4 "for"
                  :op5 "Algorithmic"
                  :op6 "Accountability")
            :ARG1-of (p9 / practical-02))
      :ARG4 (p10 / publication
            :name (n8 / name
                  :op1 "AI"
                  :op2 "Now"
                  :op3 "Institute")
            :location (c / city
                  :name (n9 / name
                        :op1 "New"
                        :op2 "York")))
      :time (d / date-entity
            :month 9
            :year 2018)
      :medium (u / url-entity
            :value "https://ainowinstitute.org/litigatingalgorithms.pdf"))


# ::id 26
# ::snt 118.Shira Mitchell, “Mirror Mirror: Reﬂections on Quantitative Fairness,” 2018, https://shiraamitchell.github.io/fairness/ ; Arvind Narayanan,  Tutorial: 21 Fairness Deﬁnitions and Their Politics , accessed November 18, 2018,   https://www.youtube.com/watch?v=jIXIuYdnyyk .
(m / multi-sentence
      :snt1 (p / publication-91
            :ARG0 (p2 / person
                  :name (n / name
                        :op1 "Shira"
                        :op2 "Mitchell"))
            :ARG1 (p3 / publication
                  :name (n2 / name
                        :op1 "Mirror"
                        :op2 "Mirror:"
                        :op3 ":"
                        :op4 "Reﬂections"
                        :op5 "on"
                        :op6 "Quantitative"
                        :op7 "Fairness"))
            :time (d / date-entity
                  :year 2018)
            :ARG4 (u / url-entity
                  :value "https://shiraamitchell.github.io/fairness/"))
      :snt2 (p4 / publication
            :name (n3 / name
                  :op1 "Tutorial"
                  :op2 21
                  :op3 "Fairness"
                  :op4 "Deﬁnitions"
                  :op5 "and"
                  :op6 "Their"
                  :op7 "Politics")
            :ARG1-of (a / access-01
                  :time (d2 / date-entity
                        :day 18
                        :month 11
                        :year 2018))
            :ARG1-of (p5 / publication-91
                  :ARG0 (p6 / person
                        :name (n4 / name
                              :op1 "Arvind"
                              :op2 "Narayanan")))))


# ::id 26
# ::snt 122.Sam Corbett-Davies and Sharad Goel, “The Measure and Mismeasure of Fairness: A Critical Review of Fair Machine Learning,”  arXiv preprint [CS]  arXiv:1808.00023, July 31, 2018.
(p / publication-91
      :ARG0 (a / and
            :op1 (p2 / person
                  :name (n / name
                        :op1 "Sam"
                        :op2 "Corbett-Davies"))
            :op2 (p3 / person
                  :name (n2 / name
                        :op1 "Sharad"
                        :op2 "Goel")))
      :ARG1 (p4 / publication
            :name (n3 / name
                  :op1 "The"
                  :op2 "Measure"
                  :op3 "and"
                  :op4 "Mismeasure"
                  :op5 "of"
                  :op6 "Fairness"
                  :op7 "A"
                  :op8 "Critical"
                  :op9 "Review"
                  :op10 "of"
                  :op11 "Machine"
                  :op12 "Learning"))
      :ARG4 (j / journal
            :name (n4 / name
                  :op1 "ArXiv"))
      :ARG6 p4
      :name (n5 / name
            :op1 "ARXiv"
            :op2 "Preprint"
            :op3 "CS")
      :time (d / date-entity
            :day 31
            :month 7
            :year 2018)
      :ARG1-of (c / cite-01
            :ARG2 "122.00023"))


# ::id 26
# ::snt 123.Solon Barocas and Moritz Hardt, “Fairness in Machine Learning” (Conference on Neural Information Processing Systems, Long Beach, CA, 2017),   https://mrtz.org/nips17/#/ ; Narayanan,  21 Fairness Deﬁnitions and Their Politics .
(a / and
      :li 123
      :op1 (p / publication-91
            :ARG0 (a2 / and
                  :op1 (p2 / person
                        :name (n / name
                              :op1 "Solon"
                              :op2 "Barocas"))
                  :op2 (p3 / person
                        :name (n2 / name
                              :op1 "Moritz"
                              :op2 "Hardt")))
            :ARG1 (p4 / publication
                  :name (n3 / name
                        :op1 "Fairness"
                        :op2 "in"
                        :op3 "Machine"
                        :op4 "Learning"))
            :ARG4 (c / conference
                  :name (n4 / name
                        :op1 "Conference"
                        :op2 "on"
                        :op3 "Neural"
                        :op4 "Information"
                        :op5 "Processing"
                        :op6 "Systems")
                  :location (c2 / city
                        :name (n5 / name
                              :op1 "Long"
                              :op2 "Beach")
                        :location (s / state
                              :name (n6 / name
                                    :op1 "CA"))))
            :time (d / date-entity
                  :year 2017))
      :op2 (p5 / publication
            :name (n7 / name
                  :op1 "https://mrtz.org/nips17/#/"))
      :op3 (p6 / publication
            :name (n8 / name
                  :op1 "Narayanan")
            :ARG1-of (c3 / cite-01
                  :ARG2 (p7 / publication
                        :quant 21
                        :topic (a3 / and
                              :op1 (f / fairness)
                              :op2 (p8 / politics)
                              :op3 (f2 / fairness))))))


# ::id 26
# ::snt 125.Cynthia Dwork, Nicole Immorlica, Adam T. Kalai, Mark DM Leiserson, “Decoupled classiﬁers for group-fair and eﬃcient machine learning”, Conference on Fairness, Accountability and Transparency (January 21, 2018), 119-133,  http://proceedings.mlr.press/v81/dwork18a/dwork18a.pdf .
(p / publication-91
      :ARG0 (a / and
            :op1 (p2 / person
                  :name (n / name
                        :op1 "Cynthia"
                        :op2 "Dwork"))
            :op2 (p3 / person
                  :name (n2 / name
                        :op1 "Nicole"
                        :op2 "Immorlica"))
            :op3 (p4 / person
                  :name (n3 / name
                        :op1 "Adam"
                        :op2 "T."
                        :op3 "Kalai"))
            :op4 (p5 / person
                  :name (n4 / name
                        :op1 "Mark"
                        :op2 "DM"
                        :op3 "Leiserson")))
      :ARG1 (p6 / publication
            :name (n5 / name
                  :op1 "Decoupled"
                  :op2 "Classiﬁers"
                  :op3 "for"
                  :op4 "Machine"
                  :op5 "Learning"))
      :ARG4 (c / conference
            :name (n6 / name
                  :op1 "Conference"
                  :op2 "on"
                  :op3 "Fairness,"
                  :op4 "Accountability"
                  :op5 "and"
                  :op6 "Transparency"))
      :ARG7 (v / value-interval
            :op1 119
            :op2 133)
      :ARG8 (u / url-entity
            :value "http://proceedings.mlr.press/v81/dwork18a/dwork18a.pdf")
      :time (d / date-entity
            :month 1
            :day 21
            :year 2018))


# ::id 26
# ::snt 126.Jon Kleinberg, “Inherent Trade-Offs in Algorithmic Fairness,” in  Abstracts of the 2018 ACM International Conference on Measurement and Modeling of Computer Systems , SIGMETRICS ’18 (New York: ACM, 2018), 40–40,   https://doi.org/10.1145/3219617.3219634 ; Alexandra Chouldechova, “Fair Prediction with Disparate Impact: A Study of Bias in Recidivism Prediction Instruments,”  arXiv preprint [Cs, Stat]  arXiv:1610.07524, October 24, 2016.
(p / publication-91
      :ARG0 (p2 / person
            :name (n / name
                  :op1 "Jon"
                  :op2 "Kleinberg"))
      :ARG1 (p3 / publication
            :name (n2 / name
                  :op1 "Inherent"
                  :op2 "Trade-offs"
                  :op3 "in"
                  :op4 "Algorithmic"
                  :op5 "Fairness"))
      :ARG4 (a / abstract
            :part-of (c / conference
                  :name (n3 / name
                        :op1 "ACM"
                        :op2 "International"
                        :op3 "Conference"
                        :op4 "on"
                        :op5 "Measurement"
                        :op6 "and"
                        :op7 "Modeling"
                        :op8 "of"
                        :op9 "Computer"
                        :op10 "Systems")
                  :time (d / date-entity
                        :year 2018)
                  :location (c2 / city
                        :name (n4 / name
                              :op1 "New"
                              :op2 "York"))))
      :ARG7 (v / value-interval
            :op1 40
            :op2 40)
      :ARG4 (p4 / publication
            :name (n5 / name
                  :op1 "Alexandra"
                  :op2 "Chouldechova")
            :ARG1 (p5 / publication
                  :name (n6 / name
                        :op1 "A"
                        :op2 "Study"
                        :op3 "of"
                        :op4 "Bias"
                        :op5 "in"
                        :op6 "Recidivism"
                        :op7 "Prediction"
                        :op8 "Instrument"))
            :ARG4 (j / journal
                  :name (n7 / name
                        :op1 "ARXiv"))
            :ARG6 (p6 / publication
                  :name (n8 / name
                        :op1 "C"
                        :op2 "Stat"))
            :ARG7 v
            :op1 1145
            :op2 1610.07524)
      :ARG4 (p7 / publication
            :name (n9 / name
                  :op1 "https://doi.org/10.1145/3219617.126.3219634"))
      :time (d2 / date-entity
            :day 24
            :month 10
            :year 2016))


# ::id 26
# ::snt 129.Mitchell, “Mirror Mirror: Reﬂections on Quantitative Fairness.” 130.Ben Hutchinson and Margaret Mitchell, “50 Years of Test (Un)Fairness: Lessons for Machine Learning,”  arXiv preprint [CS] , arXiv:1811.10104, November 25, 2018.
(p / publication-91
      :ARG0 (a / and
            :op1 (p2 / person
                  :name (n / name
                        :op1 "Ben"
                        :op2 "Hutchinson"))
            :op2 (p3 / person
                  :name (n2 / name
                        :op1 "Margaret"
                        :op2 "Mitchell")))
      :ARG1 (p4 / publication
            :name (n3 / name
                  :op1 "Mirror"
                  :op2 "Mirror:"
                  :op3 ":"
                  :op4 "Reﬂections"
                  :op5 "on"
                  :op6 "Quantitative"
                  :op7 "Fairness"))
      :ARG4 (p5 / publication
            :name (n4 / name
                  :op1 "50"
                  :op2 "Years"
                  :op3 "of"
                  :op4 "Test"
                  :op5 "Un)"
                  :op6 "Fairness:"
                  :op7 "Lives"
                  :op8 "for"
                  :op9 "Machine"
                  :op10 "Learning"))
      :ARG4 (j / journal
            :name (n5 / name
                  :op1 "ArXiv"))
      :ARG6 (p6 / publication
            :name (n6 / name
                  :op1 "1811"
                  :op2 "CS"))
      :time (d / date-entity
            :day 25
            :month 11
            :year 2018)
      :ARG1-of (c / cite-01
            :ARG2 129))


# ::id 26
# ::snt 145  In both the rapid industrial adoption of academic fairness methods, and the rush to certiﬁcation, we see an eagerness to “solve” and “eliminate” problems of bias and fairness using familiar approaches and skills that avoid the need for signiﬁcant structural change, and which fail to interrogate the complex social and historical factors at play.
(s / see-01
      :li 145
      :ARG0 (w / we)
      :ARG1 (e / eager-01
            :ARG1 (a / and
                  :op1 (s2 / solve-01
                        :ARG1 (p / problem
                              :topic (a2 / and
                                    :op1 (b / bias-01)
                                    :op2 (f / fairness)))
                        :manner (u / use-01
                              :ARG1 (a3 / and
                                    :op1 (a4 / approach-02
                                          :ARG1-of (f2 / familiarize-01))
                                    :op2 (s3 / skill)
                                    :ARG0-of (a5 / avoid-01
                                          :ARG1 (n / need-01
                                                :ARG1 (c / change-01
                                                      :ARG1 (s4 / structure)
                                                      :ARG1-of (p2 / possible-01
                                                            :polarity -))))
                                    :ARG0-of (f3 / fail-01
                                          :ARG1 (ii / interrogate-01
                                                :ARG0 a3
                                                :ARG1 (f4 / factor
                                                      :mod (c2 / complex)
                                                      :mod (s5 / social)
                                                      :mod (h / history)
                                                      :ARG0-of (p3 / play-08)))))))
                  :op2 (e2 / eliminate-01
                        :ARG1 p
                        :manner u)))
      :time (a6 / and
            :op1 (a7 / adopt-01
                  :ARG0 (ii2 / industry)
                  :ARG1 (m / method
                        :topic (f5 / fairness
                              :mod (a8 / academia)))
                  :manner (r / rapid))
            :op2 (r2 / rush-01
                  :ARG2 (c3 / certify-01))))


# ::id 26
# ::snt 165.For a more general description of justice as fairness, see: John Rawls,  Justice as Fairness: A Restatement , ed.
(s / see-01
      :ARG0 (y / you)
      :ARG1 (p / publication-91
            :ARG0 (p2 / person
                  :name (n / name
                        :op1 "John"
                        :op2 "Rawls"))
            :ARG1 (p3 / publication
                  :name n
                  :op1 "Justice"
                  :op2 "as"
                  :op3 "Fairness"
                  :op4 ":"
                  :op5 "A"
                  :op6 "Restatement"))
      :ARG4 (e / edition
            :mod 165)
      :purpose (d / describe-01
            :ARG1 (j / justice)
            :ARG2 (f / fairness)
            :ARG1-of (h / have-degree-91
                  :ARG2 (g / general-02
                        :ARG1 d)
                  :ARG3 (m / more))))


# ::id 26
# ::snt 167.Ben Green, “‘Fair’ Risk Assessments: A Precarious Approach for Criminal Justice Reform” (5th Workshop on Fairness, Accountability, and Transparency in Machine Learning, Stockholm, 2018), https://scholar.harvard.edu/ﬁles/bgreen/ﬁles/18-fatml.pdf .
(p / publication-91
      :ARG0 (p2 / person
            :name (n / name
                  :op1 "Ben"
                  :op2 "Green"))
      :ARG1 (p3 / publication
            :name (n2 / name
                  :op1 "Fair-01 :ARG1 ( assess-01 :ARG1 ( risk-01 ) ) :ARG1-of ( mean-01 :ARG2 ( approach-02 :ARG1 ( reform-01 :ARG1 ( justice :mod ( criminal ) ) ) :mod ( precarious ) ) ) ) :ARG1-of ( cite-01 :ARG2 167 ) ) :ARG4 ( publication :name ( name_2 :op1 ")))


# ::id 26
# ::snt 53.Joy Buolamwini and Timnit Gebru, “Gender Shades: Intersectional Accuracy Disparities in Commercial Gender Classiﬁcation,” in  Conference on Fairness, Accountability and Transparency  (New York, 2018), 77–91,   http://proceedings.mlr.press/v81/buolamwini18a.html .
(p / publication-91
      :li 53
      :ARG0 (a / and
            :op1 (p2 / person
                  :name (n / name
                        :op1 "Joy"
                        :op2 "Buolamwini"))
            :op2 (p3 / person
                  :name (n2 / name
                        :op1 "Timnit"
                        :op2 "Gebru")))
      :ARG1 (p4 / publication
            :name (n3 / name
                  :op1 "Gender"
                  :op2 "Shadows"
                  :op3 "Intersectional"
                  :op4 "Accident"
                  :op5 "Disparities"
                  :op6 "in"
                  :op7 "Commercial"
                  :op8 "Gender"
                  :op9 "Classiﬁcation"))
      :ARG4 (c / conference
            :name (n4 / name
                  :op1 "Conference"
                  :op2 "on"
                  :op3 "Fairness,"
                  :op4 "Accountability"
                  :op5 "and"
                  :op6 "Transparency")
            :location (c2 / city
                  :name (n5 / name
                        :op1 "New"
                        :op2 "York"))
            :time (d / date-entity
                  :year 2018))
      :ARG7 (u / url-entity
            :value "http://proceedings.mlr.press/v81/buolamwini18a.html"))


# ::id 26
# ::snt 142 Even Accenture, a consulting ﬁrm, has developed internal software tools to help clients understand and “essentially eliminate the bias in algorithms.” 143  Industry standards bodies have also taken on fairness efforts in response to industry and public sector requests for accountability assurances.
(m / multi-sentence
      :snt1 (d / develop-02
            :li 142
            :ARG0 (c / company
                  :name (n / name
                        :op1 "Accenture")
                  :mod (e / even)
                  :ARG0-of (c2 / consult-01))
            :ARG1 (t / tool
                  :mod (s / software)
                  :ARG1-of (ii / internal-02))
            :ARG4 (h / help-01
                  :ARG0 c
                  :ARG1 (a / and
                        :op1 (u / understand-01
                              :ARG0 (c3 / client)
                              :ARG1 (b / bias-01
                                    :ARG1 (a2 / algorithm)))
                        :op2 (e2 / eliminate-01
                              :ARG0 c
                              :ARG1 b
                              :mod (e3 / essential)))
                  :ARG2 c3))
      :snt2 (t2 / take-on-09
            :li 143
            :ARG0 (b2 / body
                  :mod (s2 / standard)
                  :mod (ii2 / industry))
            :ARG1 (e4 / effort-01
                  :ARG1 (f / fair-01))
            :mod (a3 / also)
            :ARG2-of (r / respond-01
                  :ARG1 (r2 / request-01
                        :ARG0 (a4 / and
                              :op1 (ii3 / industry)
                              :op2 (s3 / sector
                                    :ARG1-of (p / public-02)))
                        :ARG1 (a5 / assure-01
                              :ARG2 (a6 / accountable-02))))))


# ::id 26
# ::snt 95.Nitin Madnani et al., “Building Better Open-Source Tools to Support Fairness in Automated Scoring,” in Proceedings of the First ACL Workshop on Ethics in Natural Language Processing  (Valencia: Association for Computational Linguistics, 2017), 41–52,   https://doi.org/10.18653/v1/W17-1605 .
(p / publication-91
      :ARG0 (a / and
            :op1 (p2 / person
                  :name (n / name
                        :op1 "Nitin"
                        :op2 "Madnani"))
            :op2 (p3 / person
                  :mod (o / other)))
      :ARG1 (p4 / publication
            :name (n2 / name
                  :op1 "Building"
                  :op2 "Better"
                  :op3 "Open-Source"
                  :op4 "Tools"
                  :op5 "to"
                  :op6 "Support"
                  :op7 "of"
                  :op8 "Fairness"
                  :op9 "in"
                  :op10 "Automated"
                  :op11 "Scoring"))
      :ARG4 (p5 / publication
            :name (n3 / name
                  :op1 "Proceedings"
                  :op2 "of"
                  :op3 "the"
                  :op4 "First"
                  :op5 "CAL"
                  :op6 "Workshop"
                  :op7 "on"
                  :op8 "Ethics"
                  :op9 "in"
                  :op10 "Natural"
                  :op11 "Language"
                  :op12 "Processing")
            :location (c / country
                  :name (n4 / name
                        :op1 "Valencia"))
            :time (d / date-entity
                  :year 2017))
      :ARG7 (v / value-interval
            :op1 41
            :op2 52)
      :ARG4 (u / url-entity
            :value "https://doi.org/10.18653/v1/W17-1605.95"))


# ::id 26
# ::snt 140  Microsoft released fairlearn.py, a Python package meant to help implement a binary classiﬁer subject to a developer’s intended fairness constraint.
(r / release-01
      :li 140
      :ARG0 (c / company
            :name (n / name
                  :op1 "Microsoft"))
      :ARG1 (p / package
            :name (n2 / name
                  :op1 "Fairlearn.py")
            :mod (l / language
                  :name (n3 / name
                        :op1 "Pyro"))
            :ARG1-of (m / mean-02
                  :ARG2 (h / help-01
                        :ARG0 p
                        :ARG1 (ii / implement-01
                              :ARG1 (c2 / class
                                    :mod (b / binary)
                                    :ARG1-of (s / subject-01
                                          :ARG2 (c3 / constraint
                                                :mod (f / fairness)
                                                :ARG1-of (ii2 / intend-01
                                                      :ARG0 (p2 / person
                                                            :ARG0-of (d / develop-02)))))))))))


# ::id 26
# ::snt The majority of observational fairness approaches can be categorized as being a form of either anti-classiﬁcation, classiﬁcation parity, or calibration, as proposed by Sam Corbett-Davies and Sharad Goel.
(p / possible-01
      :ARG1 (c / categorize-01
            :ARG1 (a / approach-02
                  :ARG1 (f / fairness
                        :mod (o / observe-01))
                  :quant (m / majority))
            :ARG2 (f2 / form
                  :mod (o2 / or
                        :op1 (o3 / oppose-01
                              :ARG1 (c2 / class))
                        :op2 (p2 / parity
                              :mod c2)
                        :op3 (c3 / calibrate-01)
                        :ARG1-of (p3 / propose-01
                              :ARG0 (a2 / and
                                    :op1 (p4 / person
                                          :name (n / name
                                                :op1 "Sam"
                                                :op2 "Corbett-Davies"))
                                    :op2 (p5 / person
                                          :name (n2 / name
                                                :op1 "Sharad"
                                                :op2 "Goel"))))))))


# ::id 26
# ::snt 121  ●Observational fairness strategies  attempt to diagnose and mitigate bias by considering a dataset (either data used for training an AI model, or the input data processed by such a model), and applying methods to the data aimed at detecting whether it encodes bias against individuals or groups based on characteristics such as race, gender, or socioeconomic standing.
(a / attempt-01
      :li 121
      :ARG0 (s / strategy
            :mod (f / fairness
                  :mod (o / observe-01)))
      :ARG1 (a2 / and
            :op1 (d / diagnose-01
                  :ARG0 s
                  :ARG1 (b / bias-01)
                  :manner (a3 / and
                        :op1 (c / consider-02
                              :ARG0 s
                              :ARG1 (d2 / data
                                    :ARG1-of (m / mean-01
                                          :ARG2 (o2 / or
                                                :op1 (d3 / data
                                                      :ARG1-of (u / use-01
                                                            :ARG2 (t / train-01
                                                                  :ARG1 (m2 / model-01
                                                                        :ARG1 (ii / intelligent-01
                                                                              :mod (a4 / artificial))))))
                                                :op2 (d4 / data
                                                      :mod (ii2 / input)
                                                      :ARG1-of (p / process-01
                                                            :ARG0 m2))))))
                        :op2 (a5 / apply-02
                              :ARG0 s
                              :ARG1 (m3 / method)
                              :ARG2 (d5 / data
                                    :ARG1-of (a6 / aim-02
                                          :ARG2 (d6 / detect-01
                                                :ARG0 s
                                                :ARG1 (e / encode-01
                                                      :ARG0 d5
                                                      :ARG1 (b2 / bias-01
                                                            :ARG2 (o3 / or
                                                                  :op1 (ii3 / individual)
                                                                  :op2 (g / group))
                                                            :ARG1-of (b3 / base-02
                                                                  :ARG2 (t2 / thing
                                                                        :ARG2-of (c2 / characteristic-02)
                                                                        :example (o4 / or
                                                                              :op1 (r / race)
                                                                              :op2 (g2 / gender)
                                                                              :op3 (s2 / standing
                                                                                    :mod (s3 / society)))))))))))))
            :op2 (m4 / mitigate-01
                  :ARG0 s
                  :ARG1 b)))


# ::id 26
# ::snt Below is a brief survey of some of the more prominent approaches to understanding and deﬁning issues involving algorithmic bias and fairness.
(s / survey-01
      :ARG1 (a / approach-02
            :ARG1 (a2 / and
                  :op1 (u / understand-01
                        :ARG1 (ii / issue-02
                              :ARG0-of (ii2 / involve-01
                                    :ARG1 (a3 / and
                                          :op1 (b / bias-01
                                                :mod (a4 / algorithm))
                                          :op2 (f / fairness)))))
                  :op2 (d / dismiss-01
                        :ARG1 ii))
            :ARG1-of (ii3 / include-91
                  :ARG2 (a5 / approach-02
                        :ARG1-of (h / have-degree-91
                              :ARG2 (p / prominent)
                              :ARG3 (m / more))))
            :quant (s2 / some))
      :location (b2 / below)
      :duration (b3 / brief))


# ::id 26
# ::snt The success of such techniques is generally measured against one or another computational deﬁnition of fairness, based on a mathematical set of results.
(m / measure-01
      :ARG1 (s / succeed-01
            :ARG0 (t / technique
                  :mod (s2 / such)))
      :ARG2 (d / determine-01
            :ARG0 (o / or
                  :op1 (o2 / one)
                  :op2 (a / another))
            :ARG1 (f / fairness)
            :manner (c / computational)
            :ARG1-of (b / base-02
                  :ARG2 (s3 / set
                        :consist-of (r / result)
                        :mod (m2 / mathematics))))
      :ARG1-of (g / general-02))


# ::id 26
# ::snt 24 
 In the search for “algorithmic fairness”, many deﬁnitions of fairness, along with strategies to achieve it, have been proposed over the past few years, primarily by the technical community.
(p / propose-01
      :li 24
      :ARG0 (c / community
            :mod (t / technical))
      :ARG1 (a / and
            :op1 (d / define-01
                  :ARG1 (f / fairness)
                  :quant (m / many))
            :op2 (s / strategy
                  :purpose (a2 / achieve-01
                        :ARG1 (f2 / fairness
                              :mod (a3 / algorithm)))))
      :mod (p2 / primary)
      :time (b / before
            :op1 (n / now)
            :duration (f3 / few
                  :op1 (t2 / temporal-quantity
                        :quant 1
                        :unit (y / year))))
      :subevent-of (s2 / search-01
            :ARG2 f2))


# ::id 26
# ::snt 117  The community has been at the center of an emerging body of academic research on AI-related bias and fairness, producing insights into the nature of these issues, along with methods aimed at remediating bias.
(c / center-01
      :li 117
      :ARG1 (c2 / community
            :ARG0-of (p / produce-01
                  :ARG1 (a / and
                        :op1 (ii / insight
                              :topic (n / nature
                                    :poss (ii2 / issue-02
                                          :mod (t / this))))
                        :op2 (m / method
                              :ARG1-of (a2 / aim-02
                                    :ARG2 (r / remedy-01
                                          :ARG1 (b / bias-01)))))))
      :ARG2 (b2 / body
            :ARG0-of (e / emerge-02)
            :consist-of (r2 / research-01
                  :ARG1 (a3 / and
                        :op1 (b3 / bias-01
                              :ARG1-of (r3 / relate-01
                                    :ARG2 (ii3 / intelligent-01
                                          :mod (a4 / artificial))))
                        :op2 (f / fairness))
                  :mod (a5 / academia))))


# ::id 26
# ::snt This recognition comes in the wake of a string of examples, including evidence of bias in algorithmic pretrial risk assessments and hiring algorithms, and has been aided by the work of the Fairness,  Accountability, and Transparency in Machine Learning community.
(a / and
      :op1 (c / come-01
            :ARG1 (r / recognize-01
                  :mod (t / this))
            :ARG2 (w / wake
                  :consist-of (e / example
                        :quant (s / string)
                        :ARG2-of (ii / include-01
                              :ARG1 (e2 / evidence-01
                                    :ARG1 (b / bias-01
                                          :ARG1 (a2 / and
                                                :op1 (a3 / assess-01
                                                      :ARG1 (r2 / risk-01
                                                            :mod (p / pretrial))
                                                      :mod (a4 / algorithm))
                                                :op2 (a5 / algorithm
                                                      :instrument-of (h / hire-01)
                                                      :mod (a6 / algorithm)))))))))
      :op2 (a7 / aid-01
            :ARG0 (w2 / work-01
                  :ARG0 (c2 / community
                        :name (n / name
                              :op1 "Fairness,"
                              :op2 "Accountability"
                              :op3 "and"
                              :op4 "Transparency"
                              :op5 "in"
                              :op6 "Machine"
                              :op7 "Learning")))
            :ARG1 r))


# ::id 26
# ::snt EMERGING SOLUTIONS IN 2018  2.1  Bias Busting and Formulas for Fairness: the Limits of Technological “Fixes”  Over the past year, we have seen growing consensus that AI systems perpetuate and amplify bias, and that computational methods are not inherently neutral and objective.
(s / see-01
      :ARG0 (w / we)
      :ARG1 (c / consensus
            :ARG1-of (g / grow-01)
            :topic (a / and
                  :op1 (p / perpetuate-01
                        :ARG0 (s2 / system
                              :mod (ii / intelligent-01
                                    :mod (a2 / artificial)))
                        :ARG1 (b / bias-01))
                  :op2 (a3 / amplify-01
                        :ARG0 s2
                        :ARG1 (b2 / bias-01))
                  :op3 (a4 / and
                        :op1 (n / neutral-02
                              :polarity -
                              :ARG0 (m / method
                                    :mod (c2 / computational))
                              :mod (ii2 / inherent))
                        :op2 (o / objective
                              :polarity -
                              :domain m))))
      :time (y / year
            :mod (p2 / past))
      :ARG1-of (m2 / mean-01
            :ARG2 (p3 / publication-91
                  :ARG7 2.1
                  :ARG1 (a5 / and
                        :op1 (b3 / bust-01
                              :ARG1 (b4 / bias-01))
                        :op2 (f / formula
                              :purpose (f2 / fairness))
                        :op3 (l / limit-01
                              :ARG1 (f3 / fix-02
                                    :mod (t / technology))))))
      :time (d / date-entity
            :year 2018))


# ::id 26
# ::snt 141  Facebook announced the creation and testing of a tool called “Fairness Flow”, an internal tool for Facebook engineers that incorporates many of the same algorithms to help identify bias in machine learning models.
(a / announce-01
      :li 141
      :ARG0 (p / publication
            :name (n / name
                  :op1 "Facebook"))
      :ARG1 (a2 / and
            :op1 (c / create-01
                  :ARG0 p
                  :ARG1 (t / tool
                        :ARG1-of (c2 / call-01
                              :ARG2 (f / flow-01
                                    :ARG1 (f2 / fairness)))
                        :ARG1-of (ii / internal-02
                              :ARG2 p)
                        :beneficiary (p2 / person
                              :ARG0-of (e / engineer-01))
                        :ARG0-of (ii2 / incorporate-02
                              :ARG1 (a3 / algorithm
                                    :quant (m / many)
                                    :ARG1-of (ii3 / include-91
                                          :ARG2 (a4 / algorithm
                                                :ARG1-of (s / same-01))))
                              :purpose (h / help-01
                                    :ARG0 (o / organization
                                          :ARG1 (ii4 / identify-01
                                                :ARG1 (b / bias-01
                                                      :ARG1 (m2 / model-01
                                                            :ARG1 (l / learn-01
                                                                  :manner (m3 / machine)))))
                                          :ARG0-of ii4))))
                  :op2 (t2 / test-01
                        :ARG0 p
                        :ARG1 t))))


# ::id 26
# ::snt Furthermore, as we have seen in the large literature on bias and fairness, classiﬁcations of this nature not only have direct impacts on human lives, but also serve as data to train and inﬂuence other AI systems.
(a / and
      :op2 (a2 / and
            :op1 (ii / impact-01
                  :ARG0 (c / class-01
                        :mod (n / nature
                              :mod (t / this)))
                  :ARG1 (l / live-01
                        :ARG0 (h / human))
                  :ARG1-of (d / direct-02))
            :op2 (s / serve-01
                  :ARG0 c
                  :ARG1 (d2 / data)
                  :ARG2 (a3 / and
                        :op1 (t2 / train-01
                              :ARG1 (s2 / system
                                    :mod (ii2 / intelligent-01
                                          :mod (a4 / artificial))
                                    :mod (o / other)))
                        :op2 (ii3 / in-advance-04
                              :ARG1 s2)))
            :ARG1-of (s3 / see-01
                  :ARG0 (w / we)
                  :location (l2 / literature
                        :mod (l3 / large)
                        :topic (a5 / and
                              :op1 (b / bias-01)
                              :op2 (f / fairness))))))


# ::id 26
# ::snt 8  Zuckerberg mentioned AI technologies over 30 times in his Congressional testimony as the cure-all to the company’s problems, particularly in the complex areas of censorship, fairness, and content moderation.
(m / mention-01
      :li 8
      :ARG0 (p / person
            :name (n / name
                  :op1 "Zuckerberg"))
      :ARG1 (t / technology
            :mod (a / artificial))
      :ARG3 (c / cure-01
            :ARG1 (p2 / problem
                  :poss (c2 / company)
                  :topic (a2 / area
                        :mod (c3 / complex)
                        :example (a3 / and
                              :op1 (c4 / censor-01)
                              :op2 (f / fair-01)
                              :op3 (m2 / moderate-01
                                    :ARG1 (c5 / content)))
                        :mod (p3 / particular)))
            :ARG2 t
            :mod (a4 / all))
      :time (t2 / testify-01
            :ARG0 p
            :ARG2 (g / government-organization
                  :name (n2 / name
                        :op1 "Congress")))
      :frequency (o / over
            :op1 30))


# ::id 26
# ::snt The following report develops these themes in detail, reﬂecting on the latest academic research, and outlines seven strategies for moving forward:   1.Expanding AI fairness research beyond a focus on mathematical parity and statistical fairness toward issues of justice 2.Studying and tracking the full stack of infrastructure needed to create AI, including accounting for material supply chains 3.Accounting for the many forms of labor required to create and maintain AI systems 4.Committing to deeper interdisciplinarity in AI  5.Analyzing race, gender, and power in AI 6.Developing new policy interventions and strategic litigation 7.Building coalitions between researchers, civil society, and organizers within the technology sector   These approaches are designed to positively recast the AI ﬁeld and address the growing power imbalance that currently favors those who develop and proﬁt from AI systems at the expense of the populations most likely to be harmed.
(m / multi-sentence
      :snt1 (a / and
            :op1 (d / develop-02
                  :ARG0 (r / report
                        :ARG1-of (f / follow-04))
                  :ARG1 (t / theme
                        :mod (t2 / this))
                  :manner (d2 / detail))
            :op2 (o / outline-01
                  :ARG0 r
                  :ARG1 (s / strategy
                        :quant 7
                        :topic (m2 / move-01
                              :ARG2 (f2 / forward)))))
      :snt2 (a2 / analyze-01
            :li 6
            :ARG1 (a3 / and
                  :op1 (r2 / race)
                  :op2 (g / gender)
                  :op3 (p / power))
            :topic (ii / issue-02
                  :ARG0 (j / justice)))
      :snt3 (d3 / design-01
            :ARG1 (a4 / approach-02
                  :mod (t3 / this))
            :ARG3 (a5 / and
                  :op1 (r3 / recast-01
                        :ARG0 a4
                        :ARG1 (ii2 / intelligent-01
                              :mod (a6 / artificial))
                        :manner (p2 / positive))
                  :op2 (a7 / address-02
                        :ARG0 a4
                        :ARG1 (b / balance-01
                              :polarity -
                              :ARG1 (p3 / power)
                              :ARG1-of (g2 / grow-01)
                              :ARG0-of (f3 / favor-01
                                    :ARG1 (p4 / person
                                          :ARG0-of (d4 / develop-02
                                                :ARG1 (s2 / system
                                                      :mod ii2))
                                          :ARG0-of f3
                                          :ARG1 (p5 / person
                                                :ARG1-of (h / harm-01
                                                      :ARG2-of (h2 / have-degree-91
                                                            :ARG1 p5
                                                            :ARG3 (m3 / most))))))
                              :time (c / current)))))
      :snt4 (a8 / and
            :li 3
            :op1 (s3 / study-01
                  :ARG1 (s4 / stack
                        :consist-of (ii3 / infrastructure)
                        :ARG1-of (n / need-01
                              :ARG0 (c2 / create-01
                                    :ARG1 s2))
                        :ARG2-of (ii4 / include-01
                              :ARG1 (a9 / account-01
                                    :ARG1 (c3 / chain
                                          :mod (s5 / supply-01
                                                :ARG1 (m4 / material))))))
                  :op2 (m5 / maintain-01
                        :ARG1 s4))))


# ::id 26
# ::snt Broadening perspectives and expanding research into AI fairness and bias beyond the merely mathematical is critical to ensuring we are capable of addressing the core issues and moving the focus from parity to justice.
(c / critical-02
      :ARG1 (a / and
            :op1 (b / broaden-01
                  :ARG1 (p / perspective))
            :op2 (e / expand-01
                  :ARG1 (r / research-01
                        :ARG1 (a2 / and
                              :op1 (f / fairness
                                    :mod (a3 / artificial))
                              :op2 (b2 / bias-01
                                    :ARG1 a3)))
                  :ARG4 (b3 / beyond
                        :op1 (m / mathematics
                              :mod (m2 / mere)))))
      :ARG2 (e2 / ensure-01
            :ARG1 (c2 / capable-01
                  :ARG1 (w / we)
                  :ARG2 (a4 / and
                        :op1 (a5 / address-02
                              :ARG0 w
                              :ARG1 (ii / issue-02
                                    :mod (c3 / core)))
                        :op2 (m3 / move-01
                              :ARG0 w
                              :ARG1 (f2 / focus-01)
                              :ARG2 (j / justice)
                              :source (p2 / parity))))))


# ::id 26
# ::snt Yet, without a framework that accounts for social and political contexts and histories, these mathematical formulas for fairness will almost inevitably miss key factors, and can serve to paper over deeper problems in ways that ultimately increase harm or ignore justice.
(h / have-concession-91
      :ARG1 (a / and
            :op1 (m / miss-02
                  :ARG0 (f / formula
                        :mod (m2 / mathematics)
                        :mod (t / this)
                        :purpose (f2 / fair-01))
                  :ARG1 (f3 / factor
                        :ARG1-of (k / key-02))
                  :ARG1-of (a2 / avoid-01
                        :ARG1-of (p / possible-01
                              :polarity -
                              :mod (a3 / almost))))
            :op2 (p2 / possible-01
                  :ARG1 (s / serve-01
                        :ARG0 f
                        :ARG1 (p3 / paper-over-02
                              :ARG0 f
                              :ARG1 (p4 / problem
                                    :ARG1-of (h2 / have-degree-91
                                          :ARG2 (d / deep-02
                                                :ARG1 p4)
                                          :ARG3 (m3 / more)))
                              :manner (o / or
                                    :op1 (ii / increase-01
                                          :ARG0 f
                                          :ARG1 (h3 / harm-01))
                                    :op2 (ii2 / ignore-01
                                          :ARG0 f
                                          :ARG1 (j / justice))
                                    :manner (u / ultimate)))))
            :condition (f4 / framework
                  :polarity -
                  :ARG0-of (a4 / account-01
                        :ARG1 (a5 / and
                              :op1 (c / context
                                    :mod (s2 / society))
                              :op2 (p5 / politics)
                              :op3 (h4 / history))))))


# ::id 26
# ::snt The limits of technological ﬁxes to problems of fairness, bias, and discrimination:  Much new work has been done designing mathematical models for what should be considered “fair” when machines calculate outcomes, aimed at avoiding discrimination.
(m / multi-sentence
      :snt1 (l / limit-01
            :ARG1 (t / technology)
            :ARG2 (p / problem
                  :topic (a / and
                        :op1 (f / fairness)
                        :op2 (b / bias-01)
                        :op3 (d / discriminate-02))))
      :snt2 (w / work-01
            :ARG1 (d2 / design-01
                  :ARG1 (m2 / model-01
                        :ARG1 (t2 / thing
                              :ARG1-of (f2 / fair-01
                                    :ARG1-of (c / consider-01
                                          :ARG1-of (r / recommend-01)
                                          :time (c2 / calculate-01
                                                :ARG0 (m3 / machine)
                                                :ARG1 (o / outcome)
                                                :ARG1-of (a2 / aim-02
                                                      :ARG2 (a3 / avoid-01
                                                            :ARG1 (d3 / discriminate-02)))))))
                        :mod (m4 / mathematics)))
            :quant (m5 / much)
            :ARG1-of (n / new-01)))


# ::id 26
# ::snt Building on our 2016 and 2017 reports, the AI Now 2018 Report contends with this central problem and addresses the following key issues:  1.The growing accountability gap in AI, which favors those who create and deploy these technologies at the expense of those most affected 2.The use of AI to maximize and amplify surveillance, especially in conjunction with facial and affect recognition, increasing the potential for centralized control and oppression 3.Increasing government use of automated decision systems that directly impact individuals and communities without established accountability structures 4.Unregulated and unmonitored forms of AI experimentation on human populations  5.The limits of technological solutions to problems of fairness, bias, and discrimination   Within each topic, we identify emerging challenges and new research, and provide recommendations regarding AI development, deployment, and regulation.
(a / and
      :li 1
      :op1 (c / contend-02
            :ARG0 (p / publication
                  :name (n / name
                        :op1 "AI"
                        :op2 "Now"
                        :op3 "2018"
                        :op4 "Report")
                  :ARG1-of (b / build-01
                        :ARG2 (a2 / and
                              :op1 (r / report
                                    :time (d / date-entity
                                          :year 2016))
                              :op2 (r2 / report
                                    :time (d2 / date-entity
                                          :year 2017)))))
            :ARG1 (p2 / problem
                  :mod (c2 / central)
                  :mod (t / this)))
      :op2 (a3 / address-02
            :ARG0 p
            :ARG1 (a4 / and
                  :li 5
                  :op1 (g / gap
                        :ARG1-of (g2 / grow-01)
                        :topic (a5 / accountable-02)
                        :ARG0-of (f / favor-01
                              :ARG1 (p3 / person
                                    :ARG0-of (c3 / create-01
                                          :ARG1 (t2 / technology
                                                :mod (a6 / artificial)))
                                    :ARG0-of (d3 / deploy-01
                                          :ARG1 t2))
                              :ARG1-of (e / expend-01
                                    :ARG2 (p4 / person
                                          :ARG1-of (a7 / affect-01
                                                :ARG0 t2
                                                :ARG2-of (h / have-degree-91
                                                      :ARG1 p4
                                                      :ARG3 (m / most)))))))
                  :op2 (f2 / form
                        :ARG1-of (r3 / regulate-01
                              :polarity -)
                        :mod (e2 / experiment-01
                              :ARG1 (ii / intelligent-01
                                    :mod a6)))
                  :ARG1-of (m2 / monitor-01
                        :polarity -)))
      :op3 (a8 / and
            :li 2
            :op1 (m3 / maximize-01
                  :ARG1 (s / surveil-01))
            :op2 (a9 / amplify-01
                  :ARG1 s)
            :ARG1-of (c4 / combine-01
                  :ARG2 (a10 / and
                        :op1 (f3 / face)
                        :op2 a7
                        :ARG1 (r4 / recognize-01)))
            :mod (e3 / especially))
      :ARG0-of (ii2 / increase-01
            :ARG1 (p5 / potential
                  :domain (a11 / and
                        :op1 (c5 / control-01
                              :ARG1-of (c6 / centralize-01))
                        :op2 (o / oppress-01)))))


# ::id 26
# ::snt 8.Fairness, accountability, and transparency in AI require a detailed account of the “full stack supply chain.”  For meaningful accountability, we need to better understand and track the component parts of an AI system and the full supply chain on which it relies: that means accounting for the origins and use of training data, test data, models, application program interfaces (APIs), and other infrastructural components over a product life cycle.
(m / multi-sentence
      :snt1 (r / require-01
            :li 8
            :ARG0 (a / and
                  :op1 (f / fairness)
                  :op2 (a2 / accountable-02)
                  :op3 (t / transparency)
                  :topic (ii / intelligent-01
                        :mod (a3 / artificial)))
            :ARG1 (a4 / account-01
                  :ARG1 (c / chain
                        :mod (s / supply-01)
                        :mod (s2 / stack
                              :mod (f2 / full)))
                  :ARG1-of (d / detail-01)))
      :snt2 (n / need-01
            :ARG0 (w / we)
            :ARG1 (a5 / and
                  :op1 (u / understand-01
                        :ARG0 w
                        :ARG1 (a6 / and
                              :op1 (p / part
                                    :mod (c2 / component)
                                    :part-of (s3 / system
                                          :mod (ii2 / intelligent-01)))
                              :op2 (c3 / chain
                                    :mod (s4 / supply-01)
                                    :mod (f3 / full)
                                    :ARG0-of (r2 / rely-01
                                          :ARG1 s3)))
                        :ARG1-of (h / have-degree-91
                              :ARG2 (g / good-02
                                    :ARG1 a6)
                              :ARG3 (m2 / more)))
                  :op2 (t2 / track-01
                        :ARG0 w
                        :ARG1 a6)
                  :purpose (a7 / accountable-02
                        :ARG0 w
                        :ARG1-of (m3 / mean-01
                              :ARG2 (a8 / account-01
                                    :ARG1 (a9 / and
                                          :op1 (o / originate-01
                                                :ARG1 a6)
                                          :op2 (u2 / use-01
                                                :ARG1 a6)
                                          :op3 (d2 / data
                                                :mod (t3 / train-01))
                                          :op4 (m4 / model)
                                          :op5 (ii3 / interface
                                                :mod (p2 / program)
                                                :mod (a10 / application))
                                          :op6 (c4 / component
                                                :mod (ii4 / infrastructure)
                                                :mod (o2 / other))
                                          :time (c5 / cycle-02
                                                :ARG1 (p3 / product)
                                                :ARG2 (l / live-01)))))))
            :ARG0-of (m5 / meaningful-05)))


# ::id 26
# ::snt Our roundtable on  Machine Learning, Inequality and Bias , co-hosted in Berlin with the Robert Bosch Academy, gathered researchers and policymakers from across Europe to address issues of bias, discrimination, and fairness in machine learning and related technologies.
(g / gather-01
      :ARG0 (r / roundtable
            :topic (a / and
                  :op1 (l / learn-01
                        :mod (m / machine))
                  :op2 (e / equal-01
                        :polarity -)
                  :op3 (b / bias-01))
            :ARG1-of (h / host-01
                  :ARG0 (w / we)
                  :accompanier (o / organization
                        :name (n / name
                              :op1 "Robert"
                              :op2 "Bosch"
                              :op3 "Academy"))
                  :location (c / city
                        :name (n2 / name
                              :op1 "Berlin"))))
      :ARG1 (a2 / and
            :op1 (p / person
                  :ARG0-of (r2 / research-01))
            :op2 (p2 / person
                  :ARG0-of (m2 / make-01
                        :ARG1 (p3 / policy-01)))
            :source (a3 / across
                  :op1 (c2 / continent
                        :name (n3 / name
                              :op1 "Europe"))))
      :purpose (a4 / address-02
            :ARG0 a2
            :ARG1 (ii / issue-02
                  :ARG0 (a5 / and
                        :op1 (b2 / bias-01)
                        :op2 (d / discriminate-02)
                        :op3 (f / fair-01)
                        :topic (a6 / and
                              :op1 (l2 / learn-01
                                    :mod (m3 / machine))
                              :op2 (t / technology
                                    :ARG1-of (r3 / relate-01)))))))


# ::id 26
# ::snt Next, we share our ﬁndings on the government use of automated decision systems, and what questions this raises for fairness, transparency, and due process when such systems are protected by trade secrecy and other laws that prevent auditing and close examination.
(s / share-01
      :ARG0 (w / we)
      :ARG1 (t / thing
            :ARG1-of (t2 / think-01
                  :ARG0 w
                  :ARG2 (u / use-01
                        :ARG0 (g / government-organization
                              :ARG0-of (g2 / govern-01))
                        :ARG1 (s2 / system
                              :purpose (d / decide-01)
                              :ARG1-of (a / automate-01))))
            :ARG0-of (r / raise-01
                  :ARG1 (q / question-01
                        :ARG1 (a2 / and
                              :op1 (f / fairness)
                              :op2 (t3 / transparency)
                              :op3 (d2 / due-process)))
                  :condition (p / protect-01
                        :ARG0 (a3 / and
                              :op1 (s3 / secrecy
                                    :mod (t4 / trade-01))
                              :op2 (l / law
                                    :mod (o / other))
                              :ARG0-of (p2 / prevent-01
                                    :ARG1 (a4 / and
                                          :op1 (a5 / audit-01)
                                          :op2 (e / examine-01
                                                :ARG1-of (c / close-10)))))
                        :ARG1 s2)))
      :time (n / next))


# ::id 26
# ::snt 122  Observational fairness strategies have increasingly emerged through efforts from the community to contend with the limitations of technical fairness work and to provide entry points for other disciplines.
(e / emerge-02
      :li 122
      :ARG0 (s / strategy
            :topic (f / fairness
                  :ARG1-of (o / observe-01)))
      :manner (e2 / effort-01
            :ARG0 (c / community)
            :ARG1 (a / and
                  :op1 (c2 / contend-02
                        :ARG0 c
                        :ARG1 (l / limit-01
                              :ARG1 (w / work-01
                                    :ARG1 (f2 / fairness
                                          :mod (t / technical)))))
                  :op2 (p / provide-01
                        :ARG0 c
                        :ARG1 (p2 / point
                              :location-of (e3 / enter-01
                                    :ARG1 (d / discipline
                                          :mod (o2 / other)))))))
      :ARG1-of (ii / increase-01))


# ::id 26
# ::snt EMERGING SOLUTIONS IN 201824 2.1  Bias Busting and Formulas for Fairness: the Limits of Technological “Fixes”24   Broader approaches27 2.2  Industry Applications: Toolkits and System Tweaks28 2.3  Why Ethics is Not Enough29 3.
(m / multi-sentence
      :snt1 (e / emerge-02
            :ARG0 (t / thing
                  :ARG1-of (s / solve-01))
            :time (d / date-entity
                  :year 2018
                  :month 2
                  :day 24))
      :snt2 (a / and
            :op1 (b / bust-01
                  :ARG1 (b2 / bias-01))
            :op2 (f / formula
                  :purpose (f2 / fairness))
            :topic (t2 / thing
                  :ARG2-of (l / limit-01
                        :ARG1 (f3 / fix-02
                              :mod (t3 / technology))))
            :time (d2 / date-entity
                  :year 2018
                  :month 2
                  :day 24))
      :snt3 (a2 / approach-02
            :ARG1-of (h / have-degree-91
                  :ARG2 (s2 / speedy)
                  :ARG3 (m2 / more)))
      :snt4 (a3 / and
            :op1 (p / publication
                  :name (n / name
                        :op1 "27/2.2"))
            :op2 (p2 / publication
                  :name (n2 / name
                        :op1 "28/2.3"))
            :op3 (p3 / publication
                  :name (n3 / name
                        :op1 "Why"
                        :op2 "Ethics"
                        :op3 "Not"
                        :op4 "Enough"))
            :snt5 (a4 / and
                  :op1 (t4 / toolkit)
                  :op2 (t5 / tweak-01
                        :ARG1 (s3 / system))
                  :topic (ii / industry))))


# ::id 26
# ::snt Secondly, some have argued that different mathematical fairness criteria are mutually exclusive.
(a / argue-01
      :li 2
      :ARG0 (s / some)
      :ARG1 (e / exclusive-02
            :ARG0 (c / criteria
                  :topic (f / fair-01
                        :mod (m / mathematics))
                  :ARG1-of (d / differ-02))
            :mod (m2 / mutual)))


# ::id 26
# ::snt What they make clear is that solving complex policy issues related to bias and discrimination by indiscriminately applying one or more fairness metrics is unlikely to be successful.
(c / clarify-10
      :ARG0 (t / they)
      :ARG1 (l / likely-01
            :polarity -
            :ARG1 (s / succeed-01
                  :ARG0 (s2 / solve-01
                        :ARG1 (ii / issue-02
                              :ARG0 (p / policy-01
                                    :mod (c2 / complex))
                              :ARG1-of (r / relate-01
                                    :ARG2 (a / and
                                          :op1 (b / bias-01)
                                          :op2 (d / discriminate-01))))
                        :ARG2 (a2 / apply-02
                              :ARG1 (o / or
                                    :op1 (m / metric
                                          :quant 1
                                          :mod (f / fairness))
                                    :op2 (m2 / metric
                                          :quant (m3 / more)
                                          :mod (f2 / fairness)))
                              :manner d
                              :polarity -)))))


# ::id 26
# ::snt Several scholars have identiﬁed limitations with these approaches to observational fairness.
(l / limit-01
      :ARG0 (a / approach-02
            :ARG1 (f / fairness
                  :mod (o / observe-01))
            :mod (t / this))
      :ARG1 (s / scholar
            :quant (s2 / several))
      :ARG1-of (ii / identify-01))


# ::id 26
# ::snt This does not mean that such metrics are not useful: observational criteria may help understanding around 26 
whether datasets and AI systems meet various notions of fairness and bias and subsequently help inform a richer discussion about the goals one hopes to achieve when deploying AI systems in complex social contexts.
(m / multi-sentence
      :snt1 (m2 / mean-01
            :polarity -
            :ARG1 (t / this)
            :ARG2 (u / useful-05
                  :polarity -
                  :ARG1 (m3 / metric
                        :mod (s / such))))
      :snt2 (p / possible-01
            :ARG1 (a / and
                  :op1 (h / help-01
                        :ARG0 (c / criteria
                              :mod (o / observe-01))
                        :ARG1 (u2 / understand-01
                              :ARG1 (t2 / truth-value
                                    :polarity-of (m4 / meet-01
                                          :ARG0 (a2 / and
                                                :op1 (d / dataset)
                                                :op2 (s2 / system
                                                      :mod (ii / intelligent-01
                                                            :mod (a3 / artificial))))
                                          :ARG1 (n / notion
                                                :mod (v / various)
                                                :topic (a4 / and
                                                      :op1 (f / fairness)
                                                      :op2 (b / bias-01)))))))
                  :op2 (h2 / help-01
                        :ARG0 c
                        :ARG1 (ii2 / inform-01
                              :ARG0 c
                              :ARG2 (d2 / discuss-01
                                    :ARG1 (g / goal
                                          :ARG1-of (a5 / achieve-01
                                                :ARG0 (o2 / one)
                                                :ARG1-of (h3 / hope-01
                                                      :ARG0 o2)
                                                :time (d3 / deploy-01
                                                      :ARG0 o2
                                                      :ARG1 s2
                                                      :ARG2 (c2 / context
                                                            :mod (s3 / society)
                                                            :mod (c3 / complex)))))
                                    :ARG1-of (h4 / have-degree-91
                                          :ARG2 (r / rich)
                                          :ARG3 (m5 / more)))
                              :time (s4 / subsequent))))))


# ::id 26
# ::snt The proliferation of observational fairness methods also raises concerns over the potential to provide a false sense of assurance.
(r / raise-01
      :ARG0 (p / proliferate-01
            :ARG0 (m / method
                  :purpose (f / fairness
                        :mod (o / observe-01))))
      :ARG1 (c / concern-01
            :ARG0 (p2 / potential
                  :domain (p3 / provide-01
                        :ARG0 m
                        :ARG1 (s / sense-01
                              :ARG1 (a / assure-01)
                              :mod (f2 / false)))))
      :mod (a2 / also))


# ::id 26
# ::snt The idea that, once “treated” with such methods, AI systems are free of bias and safe to use in sensitive domains can provide a dangerous sense of false security—one that relies heavily on mathematical deﬁnitions of fairness without looking at the deeper social and historical context.
(p / possible-01
      :ARG1 (p2 / provide-01
            :ARG0 (ii / idea
                  :topic (a / and
                        :op1 (f / free-04
                              :ARG1 (s / system
                                    :mod (a2 / artificial))
                              :ARG2 (b / bias-01
                                    :ARG1 s))
                        :op2 (s2 / safe-01
                              :ARG1 s
                              :ARG2 (u / use-01
                                    :ARG1 s
                                    :location (d / domain
                                          :ARG0-of (s3 / sensitive-03))))
                        :time (t / treat-01
                              :ARG2 (m / method
                                    :mod (s4 / such)))))
            :ARG1 (s5 / sense-01
                  :ARG1 (s6 / security
                        :mod (f2 / false))
                  :ARG0-of (r / rely-01
                        :ARG1 (n / notion
                              :mod (m2 / mathematics)
                              :topic (f3 / fairness))
                        :manner (h / heavy)
                        :manner (l / look-01
                              :polarity -
                              :ARG1 (a3 / and
                                    :op1 (c / context
                                          :mod (s7 / society))
                                    :op2 (c2 / context
                                          :mod (h2 / history))
                                    :ARG1-of (h3 / have-degree-91
                                          :ARG2 (d2 / deep-02
                                                :ARG1 c)
                                          :ARG3 (m3 / more)))))
                  :ARG0-of (e / endanger-01))))


# ::id 26
# ::snt As legal scholar Frank Pasquale observes, “algorithms alone can’t meaningfully hold other algorithms accountable.” 127  While increased attention to the problems of fairness and bias in AI is a positive development, some have expressed concern over a “mathematization of ethics.” 128  As Shira Mitchell has argued:   “As statistical thinkers in the political sphere we should be aware of the hazards of supplanting politics by an expert discourse.
(m / multi-sentence
      :snt1 (o / observe-01
            :ARG0 (p / person
                  :name (n / name
                        :op1 "Frank"
                        :op2 "Pasquale")
                  :mod (s / scholar
                        :topic (l / law)))
            :ARG1 (p2 / possible-01
                  :polarity -
                  :ARG1 (h / hold-02
                        :ARG0 (a / algorithm
                              :mod (a2 / alone))
                        :ARG1 (a3 / accountable-02
                              :ARG0 (a4 / algorithm
                                    :mod (o2 / other)))
                        :ARG1-of (m2 / meaningful-05)))
            :ARG1-of (c / cite-01
                  :ARG2 127))
      :snt2 (e / express-01
            :ARG0 (s2 / some)
            :ARG1 (c2 / concern-01
                  :ARG0 (m3 / mathematics
                        :topic (e2 / ethics))
                  :ARG1 s2)
            :concession (d / develop-01
                  :ARG1 (a5 / attend-02
                        :ARG1 (p3 / problem
                              :topic (a6 / and
                                    :op1 (f / fairness)
                                    :op2 (b / bias)
                                    :topic (ii / intelligent-01
                                          :mod (a7 / artificial))))
                        :ARG1-of (ii2 / increase-01))
                  :mod (p4 / positive)))
      :snt3 (a8 / argue-01
            :ARG0 (p5 / person
                  :name (n2 / name
                        :op1 "Shira"
                        :op2 "Mitchell"))
            :ARG1 (r / recommend-01
                  :ARG1 (r2 / realize-01
                        :ARG0 (w / we
                              :ARG0-of (t / think-01
                                    :ARG2 (s3 / statistics))
                              :location (s4 / sphere
                                    :mod (p6 / politics)))
                        :ARG1 (d2 / danger
                              :ARG1-of (c3 / cause-01
                                    :ARG0 (s5 / supply-01
                                          :ARG1 (p7 / politics)
                                          :manner (d3 / discourse-01
                                                :ARG0-of (e3 / expert-01)))))))
            :ARG1-of (c4 / cite-01
                  :ARG2 128)))


# ::id 26
# ::snt Upcoming work by Hutchinson and Mitchell surveys over ﬁfty years of attempts to construct quantitative fairness deﬁnitions across multiple disciplines.
(w / work-01
      :ARG0 (a / and
            :op1 (p / person
                  :name (n / name
                        :op1 "Hutchinson"))
            :op2 (p2 / person
                  :name (n2 / name
                        :op1 "Mitchell")))
      :ARG1 (s / survey-01
            :ARG0 a
            :ARG2 (a2 / attempt-01
                  :ARG0 a
                  :ARG1 (c / construct-01
                        :ARG0 a
                        :ARG1 (d / dossier
                              :topic (f / fairness)
                              :mod (q / quantitative))
                        :location (a3 / across
                              :op1 (d2 / discipline
                                    :quant (m / multiple)))))
            :duration m
            :op1 (t / temporal-quantity
                  :quant 1
                  :unit (y / year)))
      :ARG1-of (c2 / come-01))


# ::id 26
# ::snt Their work recalls a period between 1964 and 1973 when researchers focused on deﬁning fairness for educational assessments in ways that echo the current AI fairness debate.
(r / recall-01
      :ARG0 (w / work-01
            :ARG0 (t / they))
      :ARG1 (p / period
            :time (d / date-interval
                  :op1 (d2 / date-entity
                        :year 1964)
                  :op2 (d3 / date-entity
                        :year 1973))
            :time-of (f / focus-01
                  :ARG0 (p2 / person
                        :ARG0-of (r2 / research-01))
                  :ARG1 (f2 / fair-01
                        :purpose (a / assess-01
                              :ARG1 (e / educate-01)))
                  :manner (w2 / way
                        :ARG1-of (e2 / echo-01
                              :ARG2 (d4 / debate-01
                                    :ARG1 (f3 / fair-01
                                          :mod (a2 / artificial))
                                    :time (c / current)))))))


# ::id 26
# ::snt Their efforts stalled after they were unable to agree on “broad technical solutions to the issues involved in fairness.” These precedents emphasize what the Fairness, Accountability and Transparency in Machine Learning community has been discovering: without a “tight connection to real world impact,” the added value of new fairness metrics and algorithms in the machine learning community could be minimal.
(m / multi-sentence
      :snt1 (s / stall-01
            :ARG1 (e / effort-01
                  :ARG0 (t / they))
            :time (a / after
                  :op1 (p / possible-01
                        :polarity -
                        :ARG1 (a2 / agree-01
                              :ARG0 t
                              :ARG1 (t2 / thing
                                    :ARG2-of (s2 / solve-01
                                          :ARG1 (ii / issue-02
                                                :ARG0 (f / fairness))
                                          :mod (t3 / technical)
                                          :ARG1-of (b / broad-02)))))))
      :snt2 (e2 / emphasize-01
            :ARG0 (t4 / thing
                  :ARG1-of (p2 / precedent-01)
                  :mod (t5 / this))
            :ARG1 (d / discover-01
                  :ARG0 (c / community
                        :mod (m2 / machine
                              :ARG0-of (l / learn-01))
                        :consist-of (a3 / and
                              :op1 (f2 / fairness)
                              :op2 (a4 / accountable-02)
                              :op3 (t6 / transparency)))
                  :ARG1 (p3 / possible-01
                        :ARG1 (m3 / minimal-02
                              :ARG1 (t7 / thing
                                    :ARG2-of (v / value-01
                                          :ARG1 (a5 / and
                                                :op1 (m4 / metric
                                                      :mod f2)
                                                :op2 (a6 / algorithm
                                                      :mod f2)
                                                :ARG1-of (n / new-01)))
                                    :ARG1-of (a7 / add-02)))
                        :condition (c2 / connect-01
                              :polarity -
                              :ARG1 c
                              :ARG2 (ii2 / impact-01
                                    :ARG1 (w / world
                                          :ARG1-of (r / real-04)))
                              :ARG1-of (t8 / tight-05))))))


# ::id 26
# ::snt 130  In order to arrive at more meaningful research on fairness and algorithmic bias, we must continue to pair the expertise and perspectives of communities outside of technical disciplines to those within.
(o / obligate-01
      :li 130
      :ARG2 (c / continue-01
            :ARG0 (w / we)
            :ARG1 (p / pair-01
                  :ARG0 w
                  :ARG1 (a / and
                        :op1 (e / expertise)
                        :op2 (p2 / perspective)
                        :poss (c2 / community
                              :location (o2 / outside
                                    :op1 (d / discipline
                                          :mod (t / technical)))))
                  :ARG2 (p3 / person
                        :location d)))
      :purpose (a2 / arrive-01
            :ARG1 w
            :ARG4 (r / research-01
                  :ARG1 (a3 / and
                        :op1 (f / fairness)
                        :op2 (b / bias-01
                              :manner (a4 / algorithm)))
                  :ARG1-of (h / have-degree-91
                        :ARG2 (m / meaningful-05
                              :ARG0 r)
                        :ARG3 (m2 / more)))))


# ::id 26
# ::snt have drawn on the deﬁnition of bias proposed in the early value-sensitive design (VSD) literature to propose a broader view of fairness.
(d / draw-02
      :ARG1 (n / notion
            :topic (b / bias-01)
            :ARG1-of (p / propose-01
                  :location (l / literature
                        :mod (d2 / design
                              :ARG0-of (s / sensitive-03
                                    :ARG1 (v / value)))
                        :time (e / early))))
      :purpose (p2 / propose-01
            :ARG1 (v2 / view-02
                  :ARG1 (f / fairness)
                  :ARG1-of (h / have-degree-91
                        :ARG2 (b2 / broad-02
                              :ARG1 v2)
                        :ARG3 (m / more)))))


# ::id 26
# ::snt Advances in bias-busting and fairness formulas are strong signs that the ﬁeld of AI has accepted that these concerns are real.
(s / signal-07
      :ARG0 (a / advance-01
            :ARG1 (f / formula
                  :ARG0-of (b / bust-01
                        :ARG1 (b2 / bias-01))
                  :ARG0-of (f2 / fair-01)))
      :ARG1 (a2 / accept-01
            :ARG0 (p / person
                  :ARG0-of (e / engineer-01
                        :ARG1 (a3 / artificial)))
            :ARG1 (r / real-04
                  :ARG1 (c / concern-01
                        :mod (t / this))))
      :ARG1-of (s2 / strong-02))


# ::id 26
# ::snt 126  These “impossibility results” show how each fairness strategy makes implicit assumptions about what is and is not fair.
(s / show-01
      :li 126
      :ARG0 (t / thing
            :ARG2-of (r / result-01)
            :mod (t2 / this)
            :ARG1-of (p / possible-01
                  :polarity -))
      :ARG1 (a / assume-02
            :ARG0 (s2 / strategy
                  :mod (f / fairness)
                  :mod (e / each))
            :ARG1 (t3 / thing
                  :ARG1-of (f2 / fair-01)
                  :ARG1-of (f3 / fair-01
                        :polarity -))
            :mod (ii / implicit)))


# ::id 26
# ::snt Approaches to fairness and bias must take into account both allocative and representational harms, and those that debate the deﬁnitions of fairness and bias must recognize and give voice to the individuals and communities most affected.
(a / and
      :op1 (o / obligate-01
            :ARG2 (t / take-into-account-04
                  :ARG0 (a2 / approach-02
                        :ARG1 (a3 / and
                              :op1 (f / fairness)
                              :op2 (b / bias-01)))
                  :ARG1 (h / harm-01
                        :mod (a4 / allocative)
                        :mod (r / represent-01))))
      :op2 (o2 / obligate-01
            :ARG2 (a5 / and
                  :op1 (r2 / recognize-02
                        :ARG0 (p / person
                              :ARG0-of (d / debate-01
                                    :ARG1 (d2 / define-01
                                          :ARG1 a3)))
                        :ARG1 (a6 / and
                              :op1 (ii / individual)
                              :op2 (c / community)
                              :ARG1-of (a7 / affect-01
                                    :ARG2-of (h2 / have-degree-91
                                          :ARG1 a6
                                          :ARG3 (m / most)))))
                  :op2 (g / give-01
                        :ARG0 p
                        :ARG1 (v / voice-01)
                        :ARG2 a6))))


# ::id 26
# ::snt 138  Any formulation of fairness that excludes impacted populations and the institutional context in which a system is deployed is too limited.
(h / have-degree-91
      :li 134
      :ARG1 (f / formulate-01
            :ARG1 (f2 / fairness)
            :ARG0-of (e / exclude-01
                  :ARG1 (a / and
                        :op1 (p / population
                              :ARG1-of (ii / impact-01))
                        :op2 (c / context
                              :mod (ii2 / institution)
                              :location-of (d / deploy-01
                                    :ARG1 (s / system)))))
            :mod (a2 / any))
      :ARG2 (l / limit-01
            :ARG1 f)
      :ARG3 (t / too))


# ::id 26
# ::snt 2.2  Industry Applications: Toolkits and System Tweaks  This year, we have also seen several technology companies operationalize fairness deﬁnitions, metrics, and tools.
(s / see-01
      :li 2
      :ARG0 (w / we)
      :ARG1 (o / operateize-01
            :ARG0 (c / company
                  :mod (t / technology)
                  :quant (s2 / several))
            :ARG1 (a / and
                  :op1 (f / fairness
                        :domain (d / deﬁnition))
                  :op2 (m / metric)
                  :op3 (t2 / tool)))
      :mod (a2 / also)
      :time (y / year
            :mod (t3 / this))
      :example (a3 / and
            :op1 (t4 / toolkit)
            :op2 (t5 / tweak-01
                  :ARG1 (s3 / system))
            :mod (a4 / application
                  :mod (ii / industry))))


# ::id 26
# ::snt IBM released the “AI Fairness 360” open-source tool kit, which includes nine different algorithms and many other fairness metrics developed by researchers in the Fairness, Accountability and Transparency in Machine Learning community.
(r / release-01
      :ARG0 (c / company
            :name (n / name
                  :op1 "IBM"))
      :ARG1 (k / kit
            :name (n2 / name
                  :op1 "AI"
                  :op2 "Fairness"
                  :op3 360)
            :mod (t / tool)
            :ARG1-of (o / open-04)
            :ARG2-of (ii / include-01
                  :ARG1 (a / and
                        :op1 (a2 / algorithm
                              :quant 9
                              :ARG1-of (d / differ-02))
                        :op2 (m / metric
                              :mod (f / fairness)
                              :mod (o2 / other)
                              :quant (m2 / many)
                              :ARG1-of (d2 / develop-02
                                    :ARG0 (p / person
                                          :ARG0-of (r2 / research-01)
                                          :part-of (c2 / community
                                                :name (n3 / name
                                                      :op1 "Fairness"
                                                      :op2 ","
                                                      :op3 "Accountability"
                                                      :op4 "and"
                                                      :op5 "Transparency"
                                                      :op6 "in"
                                                      :op7 "Machine"
                                                      :op8 "Learning")))))))))


# ::id 26
# ::snt 139 Google’s People + AI Research group (PAIR) released the open-source “What-If” tool, a dashboard allowing researchers to visualize the effects of different bias mitigation strategies and metrics, as well as a tool called “Facets” that supports decision-making around which fairness metric to 28 
use.
(r / release-01
      :ARG0 (o / organization
            :name (n / name
                  :op1 "People"
                  :op2 "&"
                  :op3 "AI"
                  :op4 "Research"
                  :op5 "Group")
            :poss (c / company
                  :name (n2 / name
                        :op1 "Google"))
            :quant 139)
      :ARG1 (a / and
            :op1 (t / tool
                  :name (n3 / name
                        :op1 "What-If")
                  :ARG1-of (s / source-01
                        :ARG1-of (o2 / open-04)))
            :op2 (d / dashboard
                  :ARG0-of (a2 / allow-01
                        :ARG1 (v / visualize-01
                              :ARG0 (p / person
                                    :ARG0-of (r2 / research-01))
                              :ARG1 (a3 / affect-01
                                    :ARG0 (a4 / and
                                          :op1 (s2 / strategy
                                                :ARG0-of (m / mitigate-01
                                                      :ARG1 (b / bias)))
                                          :op2 (m2 / metric)
                                          :ARG1-of (d2 / differ-02))))))
            :op3 (t2 / tool
                  :name (n4 / name
                        :op1 "Facets")
                  :ARG0-of (s3 / support-01
                        :ARG1 (m3 / make-01
                              :ARG1 (d3 / decide-01
                                    :ARG3 (m4 / metric
                                          :topic (f / fair-01))))))))


# ::id 26
# ::snt Ultimately, these ﬁndings serve to complicate the broader policy debate focused on solving bias issues with mathematical fairness tools.
(s / serve-01
      :ARG0 (t / thing
            :mod (t2 / this))
      :ARG1 (c / complicate-01
            :ARG0 t
            :ARG1 (d / debate-01
                  :ARG1 (p / policy-01)
                  :ARG1-of (h / have-degree-91
                        :ARG2 (c2 / clear-06
                              :ARG1 d)
                        :ARG3 (m / more))
                  :ARG1-of (f / focus-01
                        :ARG2 (s2 / solve-01
                              :ARG1 (ii / issue-02
                                    :ARG0 (b / bias-01))
                              :ARG2 (t3 / tool
                                    :purpose (f2 / fair-01
                                          :manner (m2 / mathematics)))))))
      :time (u / ultimate))


# ::id 26
# ::snt They also highlight the inherent mathematical trade-offs facing those aiming to mitigate various forms of bias based on one or another fairness deﬁnition.
(h / highlight-01
      :ARG0 (t / they)
      :ARG1 (t2 / trade-off-02
            :mod (m / mathematics)
            :mod (ii / inherent)
            :ARG1-of (f / face-01
                  :ARG0 (p / person
                        :ARG0-of (a / aim-01
                              :ARG1 (m2 / mitigate-01
                                    :ARG0 p
                                    :ARG1 (b / bias-01
                                          :mod (f2 / form
                                                :mod (v / various)))
                                    :ARG1-of (b2 / base-02
                                          :ARG2 (f3 / fairness
                                                :mod (o / or
                                                      :op1 (o2 / one)
                                                      :op2 (a2 / another)))))))))
      :mod (a3 / also))


# ::id 30
# ::snt The four  cardinal virtues are prudence, fairness, moderation, and courage.. 8 If it is to be expected that you will process personal data when using an  AI, it is advisable to combine this step with the consideration of whether a  DPIA is necessary.
(m / multi-sentence
      :snt1 (v / virtue
            :quant 4
            :mod (c / cardinal)
            :domain (a / and
                  :op1 (p / prudence)
                  :op2 (f / fairness)
                  :op3 (m2 / moderate-01)
                  :op4 (c2 / courage)))
      :snt2 (a2 / advise-01
            :li 8
            :ARG2 (c3 / combine-01
                  :ARG1 (s / step-01
                        :mod (t / this))
                  :ARG2 (c4 / consider-02
                        :ARG1 (t2 / truth-value
                              :polarity-of (n / need-01
                                    :ARG1 (p2 / product
                                          :name (n2 / name
                                                :op1 "DPIA"))))))
            :condition (e / expect-01
                  :ARG1 (p3 / process-01
                        :ARG0 (y / you)
                        :ARG1 (d / data
                              :ARG1-of (p4 / personal-02))
                        :time (u / use-01
                              :ARG0 y
                              :ARG1 (ii / intelligent-01
                                    :mod (a3 / artificial)))))))


# ::id 30
# ::snt Fairness 6.
(f / fairness
      :li 6)


# ::id 30
# ::snt It is important to examine what effects the AI application has, in addition to  the fairness of individual decisions, on more abstract norms such as legal  certainty, equal opportunities and equal access.4.
(ii / important-01
      :li 4
      :ARG1 (e / examine-01
            :ARG1 (a / and
                  :op1 (a2 / affect-01
                        :ARG0 (a3 / apply-02
                              :ARG1 (ii2 / intelligent-01
                                    :mod (a4 / artificial)))
                        :ARG1 (n / norm
                              :example (a5 / and
                                    :op1 (c / certainty
                                          :ARG1-of (l / legal-02))
                                    :op2 (o / opportunity
                                          :ARG1-of (e2 / equal-01))
                                    :op3 (a6 / access-01
                                          :ARG1-of (e3 / equal-01)))
                              :mod (a7 / abstract
                                    :ARG2-of (h / have-degree-91
                                          :ARG1 n
                                          :ARG3 (m / more)))))
                  :op2 (f / fairness
                        :poss (d / decide-01
                              :ARG0 (ii3 / individual))))))


# ::id 30
# ::snt Fairness can also mean that equal  cases are treated equally (equality).Fairness can also refer to the concept  of social equality, the idea that the weaker should be given priority over  those who benefit from institutions that produce inequality.
(p / possible-01
      :ARG1 (m / mean-01
            :ARG1 (f / fairness)
            :ARG2 (t / treat-01
                  :ARG1 (c / case-03
                        :ARG1-of (e / equal-01))
                  :ARG1-of (e2 / equal-01))
            :ARG3 (e3 / equal-01))
      :mod (a / also)
      :ARG1-of (m2 / mean-01
            :ARG2 (c2 / concept
                  :topic (e4 / equal-01
                        :mod (s / society))
                  :domain (r / recommend-01
                        :ARG1 (p2 / prioritize-01
                              :ARG1 (p3 / person
                                    :ARG1-of (h / have-degree-91
                                          :ARG2 (w / weak-02)
                                          :ARG3 (m3 / more)
                                          :ARG4 (p4 / person
                                                :ARG1-of (b / benefit-01
                                                      :ARG0 (ii / institution
                                                            :ARG0-of (p5 / produce-01
                                                                  :ARG1 e4
                                                                  :polarity -))))))))))
      :mod (a2 / also))


# ::id 30
# ::snt Fairness can mean that people get what  they earn according to relevant criteria.
(p / possible-01
      :ARG1 (m / mean-01
            :ARG1 (f / fairness)
            :ARG2 (g / get-01
                  :ARG0 (p2 / person)
                  :ARG1 (t / thing
                        :ARG1-of (e / earn-01
                              :ARG0 p2))
                  :ARG1-of (a / accord-02
                        :ARG2 (c / criteria
                              :ARG1-of (r / relevant-01))))))


# ::id 30
# ::snt Fairness, equal access and solidarity: AI must contribute to  fairness, equal opportunities and solidarity 23  Fairness has various definitions.
(m / multi-sentence
      :snt1 (a / and
            :op1 (f / fairness)
            :op2 (a2 / access-01
                  :ARG1-of (e / equal-01))
            :op3 (s / solidarity))
      :snt2 (o / obligate-01
            :li 23
            :ARG1 (ii / intelligent-01
                  :mod (a3 / artificial))
            :ARG2 (c / contribute-01
                  :ARG0 ii
                  :ARG2 (a4 / and
                        :op1 (f2 / fairness)
                        :op2 (o2 / opportunity
                              :ARG1-of (e2 / equal-01))
                        :op3 (s2 / solidarity))))
      :snt3 (d / define-01
            :li 23
            :ARG1 (f3 / fairness)
            :mod (v / various)))


# ::id 30
# ::snt We contribute to fairness, equal opportunities and solidarity 5.
(c / contribute-01
      :li 5
      :ARG0 (w / we)
      :ARG2 (a / and
            :op1 (f / fairness)
            :op2 (o / opportunity
                  :ARG1-of (e / equal-01))
            :op3 (s / solidarity)))


# ::id 33
# ::snt 2.1.3 Applying the VCIO approach  to accountability as a value (page 24/25)Transparency as  explainability and  interpretability Justice with aspects of  algorithmic fairness and  inclusion Accountability refers to  questions of assigning  responsibility
20VALUES, CRITERIA, INDICATORS, OBSERVABLES (VCIO) AND THE AI ETHICS LABEL IN DETAIL Value TRANSPARENCY TRANSPARENCY Value Criteria Disclosure of origin of data sets Disclosure of properties of algorithm/model used Accessibility Criteria IndicatorsIs the data’s origin  documented?Is it plausible for  each purpose, which  data is being used?Are the training data  set’s characteristics  documented and  disclosed?
(m / multi-sentence
      :snt1 (a / apply-02
            :li 2
            :ARG1 (a2 / approach-02
                  :ARG1 (v / value
                        :mod (v2 / variable
                              :name (n / name
                                    :op1 "VCIO"))))
            :ARG2 (a3 / accountable-02)
            :ARG1-of (d / describe-01
                  :ARG0 (p / publication
                        :ARG1-of (c / cite-01
                              :ARG2 (p2 / page
                                    :mod 24)))))
      :snt2 (a4 / and
            :op1 (t / transparent
                  :prep-as (a5 / and
                        :op1 (p3 / possible-01
                              :ARG1 (e / explain-01))
                        :op2 (p4 / possible-01
                              :ARG1 (ii / interpret-01))))
            :op2 (ii2 / include-01
                  :ARG1 (j / justice)
                  :ARG2 (a6 / aspect
                        :topic (a7 / and
                              :op1 (f / fairness
                                    :mod (a8 / algorithmic))
                              :op2 (ii3 / include-01)))))
      :snt3 (r / refer-01
            :ARG1 (q / question-01
                  :ARG1 (a9 / and
                        :op1 (a10 / assign-01
                              :ARG1 (r2 / responsible-03))
                        :op2 (c2 / criteria
                              :mod (v3 / variable
                                    :name (n2 / name
                                          :op1 "ACA")))
                        :op3 (c3 / criteria
                              :mod (v4 / variable
                                    :name (n3 / name
                                          :op1 "CITER")))
                        :op4 (ii4 / identifier)
                        :op5 ii4)
                  :op6 (l / label
                        :mod (e2 / ethics)
                        :mod (a11 / artificial))))
      :ARG2 (a12 / and
            :op1 (d2 / disclose-01
                  :ARG1 (c4 / characteristic-02
                        :ARG1 (s / set
                              :mod (d3 / data)
                              :purpose (t2 / train-01))))
            :op2 (d4 / disclose-01
                  :ARG1 (p5 / property
                        :poss (s2 / slash
                              :op1 (a13 / algorithm)
                              :op2 (m2 / model)
                              :ARG1-of (u / use-01))))
            :polarity (a14 / amr-unknown))
      :polarity (a15 / amr-unknown))


# ::id 33
# ::snt For application fields Our report has shown how  to apply AI ethics in practice
42CONCLUSION AND WHERE TO GO  FROM HERE classified in one of the higher risk levels, they may demand that an AI system must (1)  carry an ethics label that shows the rating for values such as transparency, robustness,  or fairness and (2) satisfy certain minimum levels within the rating.
(m / multi-sentence
      :snt1 (s / show-01
            :ARG0 (t / thing
                  :ARG1-of (r / report-01
                        :ARG0 (w / we)))
            :ARG1 (t2 / thing
                  :manner-of (a / apply-02
                        :ARG1 (e / ethics
                              :mod (a2 / artificial))
                        :ARG2 (f / field
                              :mod (a3 / application))
                        :ARG1-of (p / practice-01))))
      :snt2 (a4 / and
            :op1 (t3 / thing
                  :ARG1-of (c / conclude-01))
            :op2 (l / location
                  :ARG4-of (g / go-01
                        :ARG1 w)
                  :source (h / here)))
      :snt3 (p2 / possible-01
            :ARG1 (d / demand-01
                  :ARG0 (t4 / they)
                  :ARG1 (a5 / and
                        :op1 (o / obligate-01
                              :li 1
                              :ARG1 (s2 / system
                                    :mod (ii / intelligent-01
                                          :mod a2)
                                    :ARG1-of (c2 / classify-01
                                          :ARG2 (l2 / level
                                                :ARG1-of (ii2 / include-91
                                                      :ARG2 (l3 / level
                                                            :ARG1-of (h2 / have-degree-91
                                                                  :ARG2 (h3 / high-02
                                                                        :ARG1 l3)
                                                                  :ARG3 (m2 / more)))))))
                              :ARG2 (l4 / label-01
                                    :ARG1 s2
                                    :mod (e2 / ethics)
                                    :ARG0-of (s3 / show-01
                                          :ARG1 (r2 / rate-01
                                                :ARG1 (v / value
                                                      :example (o2 / or
                                                            :op1 (t5 / transparency)
                                                            :op2 (r3 / robustness)
                                                            :op3 (f2 / fairness)))))))
                        :op2 (o3 / obligate-01
                              :li 2
                              :ARG1 s2
                              :ARG2 (s4 / satisfy-01
                                    :ARG0 s2
                                    :ARG1 (l5 / level
                                          :mod (m3 / minimum)
                                          :mod (c3 / certain)
                                          :ARG1-of ii2
                                          :ARG2 l2)))))))


# ::id 33
# ::snt To date, well over a hundred different AI ethics guidelines have been published.1 Nearly  all of them mention values such as privacy, fairness or non-discrimination, transparency,  safety, and accountability.
(m / multi-sentence
      :snt1 (p / publish-01
            :ARG1 (g / guideline
                  :topic (e / ethics)
                  :ARG1-of (d / differ-02)
                  :mod (a / artificial)
                  :quant (o / over
                        :op1 100
                        :degree (w / well)))
            :time (t / to-date))
      :snt2 (m2 / mention-01
            :li 1
            :ARG0 (t2 / they
                  :mod (a2 / all
                        :ARG1-of (n / near-02)))
            :ARG1 (v / value
                  :example (a3 / and
                        :op1 (p2 / privacy)
                        :op2 (o2 / or
                              :op1 (f / fairness)
                              :op2 (d2 / discriminate-01
                                    :polarity -))
                        :op3 (t3 / transparency)
                        :op4 (s / safe-01)
                        :op5 (a4 / accountable-02)))))


# ::id 33
# ::snt So how we implement and prioritise values such as  justice (here includes fairness or non-discrimination) and transparency in practice,  depends to some extent on the field of application and the cultural context an AI system  operates in.
(d / depend-01
      :ARG0 (a / and
            :op1 (ii / implement-01
                  :ARG0 (w / we)
                  :ARG1 (v / value
                        :example (a2 / and
                              :op1 (j / justice)
                              :op2 (ii2 / include-01
                                    :ARG1 (o / or
                                          :op1 (f / fairness)
                                          :op2 (d2 / discriminate-01
                                                :polarity -))
                                    :location (h / here))
                              :op3 (t / transparency))))
            :op2 (p / prioritize-01
                  :ARG0 w
                  :ARG1 v)
            :ARG1-of (p2 / practice-01))
      :ARG1 (a3 / and
            :op1 (f2 / field
                  :mod (a4 / apply-02))
            :op2 (c / context
                  :mod (c2 / culture)
                  :location-of (o2 / operate-01
                        :ARG0 (s / system
                              :ARG1-of (ii3 / intelligent-01
                                    :mod (a5 / artificial))))))
      :degree (e / extent
            :mod (s2 / some)))


# ::id 33
# ::snt A system used in the justice sector must necessarily exhibit higher levels of  privacy and fairness than a system used in the organisation of industrial production.
(o / obligate-01
      :ARG2 (e / exhibit-01
            :ARG0 (s / system
                  :ARG1-of (u / use-01
                        :ARG2 (s2 / sector
                              :mod (j / justice))))
            :ARG1 (a / and
                  :op1 (p / private-02)
                  :op2 (f / fair-01)
                  :ARG1-of (h / have-degree-91
                        :ARG2 (h2 / high-02)
                        :ARG3 (m / more)
                        :ARG4 (s3 / system
                              :ARG1-of (u2 / use-01
                                    :ARG2 (o2 / organize-01
                                          :ARG1 (p2 / produce-01
                                                :mod (ii / industry)))))))
            :ARG1-of (n / need-01)))


# ::id 33
# ::snt 2.1.1 Applying the VCIO  approach to transparency as a value (page 20/21) Justice The criteria subsumed under the value of justice in this example pertain to classic aspects  of algorithmic fairness such as bias prevention and assessment but emphasise a process  perspective to include a broader set of ethical considerations.
(m / multi-sentence
      :snt1 (a / apply-02
            :li 1
            :ARG1 (a2 / approach-02
                  :ARG1 (t / transparency)
                  :ARG2 (v / value)
                  :mod (t2 / thing
                        :name (n / name
                              :op1 "VCIO")))
            :ARG2 v
            :ARG1-of (d / describe-01
                  :ARG0 (a3 / and
                        :op1 (p / page
                              :mod 20)
                        :op2 (p2 / page
                              :mod 21)
                        :topic (j / justice))))
      :snt2 (c / contrast-01
            :ARG1 (p3 / pertain-01
                  :ARG0 (c2 / criteria
                        :ARG1-of (s / subsume-01
                              :ARG2 (v2 / value
                                    :mod (j2 / justice)
                                    :example (t3 / this))))
                  :ARG1 (a4 / aspect
                        :mod (c3 / classic)
                        :topic (f / fairness
                              :mod (a5 / algorithmic))
                        :example (a6 / and
                              :op1 (p4 / prevent-01
                                    :ARG1 (b / bias-01))
                              :op2 (a7 / assess-01))))
            :ARG2 (e / emphasize-01
                  :ARG0 c2
                  :ARG1 (p5 / perspective
                        :mod (p6 / process-02))
                  :purpose (ii / include-01
                        :ARG0 c2
                        :ARG1 (s2 / set
                              :consist-of (t4 / thing
                                    :ARG1-of (c4 / consider-02)
                                    :mod (e2 / ethics))
                              :ARG1-of (h / have-degree-91
                                    :ARG2 (b2 / broad-02
                                          :ARG1 s2)
                                    :ARG3 (m2 / more)))))))


# ::id 33
# ::snt In this sense, justice refers to a broader set  of ethical considerations than the often-used term fairness, which mostly focuses on  algorithmic outcomes themselves.
(r / refer-01
      :ARG1 (j / justice)
      :ARG2 (s / set
            :consist-of (c / consider-02
                  :ARG1 (e / ethics))
            :ARG1-of (h / have-degree-91
                  :ARG2 (b / broad-02
                        :ARG1 s)
                  :ARG3 (m / more)
                  :ARG4 (t / term
                        :mod (f / fairness)
                        :ARG1-of (u / use-01
                              :mod (o / often))
                        :ARG0-of (f2 / focus-01
                              :ARG2 (o2 / outcome
                                    :mod (a / algorithm))
                              :mod (m2 / most)))))
      :mod (s2 / sense
            :mod (t2 / this)))


# ::id 33
# ::snt 22VALUES, CRITERIA, INDICATORS, OBSERVABLES (VCIO) AND THE AI ETHICS LABEL IN DETAIL ValueJUSTICE JUSTICE ValueCriteriaIdentifying  and assessing  trade-offsAssessment of different sources of potential biases to ensure fairness1Social justice  considerationsDetection and prevention of biases to ensure fairness Participatory procedures CriteriaIndicatorsHave tradeoffs been  identified and  assessed?Has the  training data  been analysed  for potential  biases?Has the  input design  (sensors, user  interface) and  input data  been reviewed  for potential  biases?Have the  requirements,  goals and task  definitions  been examined  for implicit  and explicit  discriminatory  effects?Were  possible selfreinforcing  processes  considered?Has due care  been taken  with regard to  discriminatory  effects caused  by the design  of the data  output?Have the  applied  methods (e.g.
(m / multi-sentence
      :snt1 (a / and
            :op1 (c / criteria
                  :quant 22)
            :op2 (c2 / criteria
                  :mod (s / selective))
            :op3 (ii / indicator)
            :op4 (l / label-01
                  :ARG2 (o / organization
                        :name (n / name
                              :op1 "Veterans"
                              :op2 "Anonymous"
                              :op3 "International"
                              :op4 "Defense"
                              :op5 "Organization")))
            :polarity (a2 / amr-unknown))
      :snt2 (a3 / and
            :op1 (ii2 / identify-01
                  :ARG1 (t / trade-off-02))
            :op2 (a4 / assess-01
                  :ARG1 t)
            :purpose (e / ensure-01
                  :ARG0 t
                  :ARG1 (f / fair-01))
            :polarity (a5 / amr-unknown)
            :snt3 (r / review-01
                  :ARG1 (a6 / and
                        :op1 (d / design-01
                              :ARG1 (ii3 / input)
                              :ARG1-of (m2 / mean-01
                                    :ARG2 (s2 / sensor)))
                        :op2 (d2 / design-01
                              :ARG1 (ii4 / input)
                              :ARG1-of (m3 / mean-01
                                    :ARG2 (ii5 / interface
                                          :poss (p / person
                                                :ARG0-of (u / use-01))))))
                  :ARG3 (b / bias-01
                        :ARG1-of (p2 / possible-01)
                        :mod (ii6 / implicit))
                  :polarity (a7 / amr-unknown))
            :snt4 (c3 / consider-02
                  :ARG1 (s3 / source-02
                        :ARG1 (b2 / bias-01
                              :ARG1-of (p3 / possible-01)
                              :ARG1-of (e2 / explicit-03))
                        :ARG1-of (d3 / differ-02))
                  :purpose (e3 / ensure-01
                        :ARG1 (f2 / fair-01)))))


# ::id 33
# ::snt 2.1.2  Applying the VCIO approach to justice as a value
23VALUES, CRITERIA, INDICATORS, OBSERVABLES (VCIO) AND THE AI ETHICS LABEL IN DETAIL ValueJUSTICE JUSTICE ValueCriteriaIdentifying  and assessing  trade-offsAssessment of different sources of potential biases to ensure fairness1Social justice  considerationsDetection and prevention of biases to ensure fairness Participatory procedures CriteriaIndicatorsHave tradeoffs been  identified and  assessed?Has the  training data  been analysed  for potential  biases?Has the  input design  (sensors, user  interface) and  input data  been reviewed  for potential  biases?Have the  requirements,  goals and task  definitions  been examined  for implicit  and explicit  discriminatory  effects?Were  possible selfreinforcing  processes  considered?Has due care  been taken  with regard to  discriminatory  effects caused  by the design  of the data  output?Have the  applied  methods (e.g.
(m / multi-sentence
      :snt1 (a / and
            :li 2
            :op1 (ii / identify-01
                  :ARG1 (s / source-02
                        :ARG1 (b / bias-01
                              :mod (p / potential))
                        :ARG1-of (d / differ-02)))
            :op2 (a2 / assess-01
                  :ARG1 s)
            :purpose (e / ensure-01
                  :ARG1 (f / fairness)))
      :snt2 (a3 / and
            :op1 (c / criteria
                  :mod (v / value))
            :op2 (c2 / criteria
                  :mod (c3 / criteria))
            :op3 (ii2 / indicator)
            :op4 (c4 / clue)
            :op5 (ii3 / identifier)
            :op6 (l / label-01
                  :ARG1 (o / organization
                        :name (n / name
                              :op1 "Veterans"
                              :op2 "Anonymous"
                              :op3 "International"
                              :op4 "Liberation"
                              :op5 "Organization"))
                  :ARG2 (v2 / value
                        :mod (j / justice)))
            :polarity (a4 / amr-unknown))
      :snt3 (a5 / analyze-01
            :ARG1 (d2 / data
                  :mod (t / train-01))
            :ARG2 (b2 / bias-01
                  :mod (p2 / potential)
                  :ARG1-of (c5 / cause-01
                        :ARG0 (d3 / design-01
                              :ARG1 (o2 / output
                                    :mod (d4 / data)))))
            :polarity (a6 / amr-unknown))
      :snt4 (c6 / consider-02
            :ARG1 (a7 / and
                  :op1 (r / require-01)
                  :op2 (g / goal)
                  :op3 (d5 / define-01
                        :ARG1 (t2 / task)))
            :ARG2 (p3 / possible-01
                  :ARG1 (r2 / reinforce-01
                        :ARG0 a7
                        :ARG1 a7))))


# ::id 33
# ::snt Mittelstadt, B., Russell, C., Wachter, S. (2019) ‘Explaining Explanations in AI’ Proceedings  of the Conference on Fairness, Accountability, and Transparency – FAT*‚ 19, pp.
(p / publication-91
      :ARG0 (a / and
            :op1 (p2 / person
                  :name (n / name
                        :op1 "Mittelstadt")
                  :ARG0-of (h / have-rel-role-91
                        :ARG2 (m / member)))
            :op2 (p3 / person
                  :name (n2 / name
                        :op1 "Russell")
                  :ARG0-of (h2 / have-rel-role-91
                        :ARG2 (m2 / member)))
            :op3 (p4 / person
                  :name (n3 / name
                        :op1 "Wachter")
                  :ARG0-of (h3 / have-rel-role-91
                        :ARG2 (m3 / member))))
      :ARG1 (p5 / publication
            :name (n4 / name
                  :op1 "Explaining"
                  :op2 "of"
                  :op3 "Explained"
                  :op4 "In"
                  :op5 "AI"
                  :op6 " Proceedings")
            :part-of (c / conference
                  :name (n5 / name
                        :op1 "Conference"
                        :op2 "on"
                        :op3 "Fairness"
                        :op4 "and"
                        :op5 "Accountability"
                        :op6 "and"
                        :op7 "Transparency")))
      :ARG7 (v / value-interval
            :op1 19
            :op2 19)
      :time (d / date-entity
            :year 2019))


# ::id 33
# ::snt While existing AI ethics guidelines typically focus on questions of algorithmic nondiscrimination and often frame those in terms of fairness, the VCIO approach broadens this  perspective.
(b / broaden-01
      :ARG0 (a / approach-02
            :ARG1 (t / thing
                  :name (n / name
                        :op1 "VCIO")))
      :ARG1 (p / perspective
            :mod (t2 / this))
      :concession (a2 / and
            :op1 (f / focus-01
                  :ARG0 (g / guideline
                        :ARG1-of (e / exist-01)
                        :topic (e2 / ethics
                              :mod (a3 / artificial)))
                  :ARG2 (q / question-01
                        :ARG1 (d / discriminate-02
                              :polarity -
                              :mod (a4 / algorithmic)))
                  :ARG1-of (t3 / typical-02))
            :op2 (f2 / frame-06
                  :ARG0 g
                  :ARG1 q
                  :ARG2 (f3 / fairness)
                  :frequency (o / often))))


# ::id 35
# ::snt It  centers on themes of equity, opportunity, partnerships, and accountability, and specifically reflects community  members’ desire for racial fairness and social justice across all public programs.
(a / and
      :op1 (c / center-01
            :ARG1 (ii / it)
            :ARG2 (t / theme
                  :consist-of (a2 / and
                        :op1 (e / equity)
                        :op2 (o / opportunity)
                        :op3 (p / partner-01)
                        :op4 (a3 / accountable-02))))
      :op2 (r / reflect-01
            :ARG1 ii
            :ARG2 (d / desire-01
                  :ARG0 (p2 / person
                        :ARG0-of (h / have-org-role-91
                              :ARG1 (c2 / community)
                              :ARG2 (m / member)))
                  :ARG1 (a4 / and
                        :op1 (f / fairness
                              :mod (r2 / race))
                        :op2 (j / justice
                              :mod (s / social)))
                  :location (a5 / across
                        :op1 (p3 / program
                              :ARG1-of (p4 / public-02)
                              :mod (a6 / all))))
            :ARG1-of (s2 / specific-02)))


# ::id 35
# ::snt The five guidelines (summarized below) from Fairness, Accountability, and Transparency in Machine  Learning19 can be used as a starting place to inform the development and use of algorithmic tools in  ways that are accountable to the public.
(p / possible-01
      :ARG1 (u / use-01
            :ARG1 (g / guideline
                  :quant 5
                  :ARG1-of (s / summarize-01
                        :location (b / below))
                  :ARG1-of (ii / include-91
                        :ARG2 (a / and
                              :op1 (f / fairness)
                              :op2 (a2 / accountable-02)
                              :op3 (t / transparency)
                              :op4 (p2 / publication
                                    :name (n / name
                                          :op1 "Machine"
                                          :op2 "Learning19")))))
            :ARG2 (p3 / place
                  :location-of (s2 / start-01
                        :ARG1 (ii2 / inform-01
                              :ARG0 g
                              :ARG1 (a3 / and
                                    :op1 (d / develop-02
                                          :ARG1 (t2 / tool
                                                :mod (a4 / algorithm)))
                                    :op2 (u2 / use-01
                                          :ARG1 t2
                                          :manner (a5 / accountable-02
                                                :ARG0 t2
                                                :ARG2 (p4 / public)))))))))


# ::id 35
# ::snt Fairness:  Ensure that algorithmic decisions do not create discriminatory or unjust impacts In order to formalize a commitment to the public and to these principles, it is recommended that  those developing and using algorithms draft a Social Impact Statement  that describes how these  five principles (or others relevant to the local context) will be operationalized in practice.
(m / multi-sentence
      :snt1 (e / ensure-01
            :ARG1 (c / create-01
                  :polarity -
                  :ARG0 (t / thing
                        :ARG1-of (d / decide-01)
                        :instrument (a / algorithm))
                  :ARG1 (o / or
                        :op1 (ii / impact-01
                              :ARG0 t
                              :mod (d2 / discriminate-02))
                        :op2 (ii2 / impact-01
                              :ARG0 t
                              :ARG1-of (j / just-02
                                    :polarity -)))))
      :snt2 (r / recommend-01
            :ARG1 (d3 / draft-01
                  :ARG0 (p / person
                        :ARG0-of (d4 / develop-02
                              :ARG1 (a2 / algorithm))
                        :ARG0-of (u / use-01
                              :ARG1 a2))
                  :ARG1 (t2 / thing
                        :name (n / name
                              :op1 "Social"
                              :op2 "Impact"
                              :op3 "Statement")
                        :ARG0-of (d5 / describe-01
                              :ARG1 (t3 / thing
                                    :manner-of (o2 / operate-01
                                          :ARG1 (o3 / or
                                                :op1 (p2 / principle
                                                      :quant 5
                                                      :mod (t4 / this))
                                                :op2 (p3 / principle
                                                      :ARG1-of (r2 / relevant-01
                                                            :ARG2 (c2 / context
                                                                  :ARG1-of (l / local-02)))
                                                      :mod (o4 / other)))
                                          :ARG1-of (p4 / practice-01)))))
                  :purpose (f / formalize-01
                        :ARG0 p
                        :ARG1 (c3 / commit-01
                              :ARG2 (a3 / and
                                    :op1 (p5 / public)
                                    :op2 p2))))))


# ::id 35
# ::snt A number of tools and frameworks may be helpful in this process, including  Algorithmic Fairness: A Code-based Primer for Public-sector Data Scientists ,20 the Ethics &  Algorithms Toolkit ,21 the D ata Nutrition Project  prototype  and white paper,22 and Model Cards.23 17   See asemio.com 18   Benjamin, R. (2019) 19   Diakopoulos, N., et al.
(m / multi-sentence
      :snt1 (p / possible-01
            :ARG1 (h / helpful-04
                  :ARG0 (a / and
                        :op1 (t / tool)
                        :op2 (f / framework)
                        :quant (n / number)
                        :ARG2-of (ii / include-01
                              :ARG1 (a2 / and
                                    :op1 (p2 / publication
                                          :name (n2 / name
                                                :op1 "Algorithmic"
                                                :op2 "Fairness"
                                                :op3 "A"
                                                :op4 "Code"
                                                :op5 "based"
                                                :op6 "Primer"
                                                :op7 "for"
                                                :op8 "Data"
                                                :op9 "Scientists"))
                                    :op2 (p3 / publication
                                          :name (n3 / name
                                                :op1 "Ethics"
                                                :op2 "&"
                                                :op3 "Algorithms"
                                                :op4 "Toolkit")
                                          :ord (o / ordinal-entity
                                                :value 20))
                                    :op3 (p4 / publication
                                          :name (n4 / name
                                                :op1 "D"
                                                :op2 "ata"
                                                :op3 "Nutrition"
                                                :op4 "Project")
                                          :mod (p5 / prototype))
                                    :op4 (p6 / paper
                                          :ARG1-of (w / white-03))
                                    :op5 (c / card
                                          :mod (m2 / model)))))
                  :ARG1 (p7 / process-02
                        :mod (t2 / this))))
      :snt2 (s / see-01
            :ARG0 (y / you)
            :ARG1 (u / url-entity
                  :value "asemio.com"))
      :snt3 (p8 / publication
            :ARG0 (a3 / and
                  :op1 (p9 / person
                        :name (n5 / name
                              :op1 "Benjamin"))
                  :op2 (p10 / person
                        :name (n6 / name
                              :op1 "R."))
                  :op3 (p11 / person
                        :name (n7 / name
                              :op1 "N."
                              :op2 "Diakopoulos"))
                  :op4 (p12 / person
                        :mod (o2 / other)))
            :time (d / date-entity
                  :year 2019)
            :ARG1-of (c2 / cite-01
                  :ARG2 (a4 / and
                        :op1 19
                        :op2 17))))


# ::id 35
# ::snt (2019)
26 CENTERING RACIAL EQUITY THROUGHOUT THE DATA LIFE CYCLERacial Equity in Algorithms and Statistical Tools:   Positive & Problematic Practices POSITIVE PRACTICE PROBLEMATIC PRACTICE Involving diverse stakeholders, including  specific community advisory boards, in early  conversations about the purpose of an algorithm  prior to development and implementationDeveloping and implementing algorithms  for human services without stakeholder  involvement or alignment across multiple  agencies Determining responsibility for oversight of  algorithm development and implementation,  with clear communication channels for inputInadequate opportunities for community  feedback regarding algorithm development and  implementation Mandatory impact assessments that involve  thoroughly thinking through potential intended  and unintended consequencesFailure to think through intended and  unintended outcomes Clearly identifying and communicating potential  benefits and risks to stakeholdersImplementing an algorithm with no clear benefit  to individuals included in the data Human-led algorithm use (i.e., human can  override algorithm at any point in process)Elevating algorithmic decision making over  judgment of seasoned practitioners; no human  involvement Transparency regarding what data drive the  algorithm and how, e.g., description of design  and testing process, list of factors that the  tool uses, thresholds used, outcome data used  to develop and validate the tool, definitions  of what an instrument forecasts and for what  time periodUse of a “black box” or proprietary algorithm that  does not allow for transparency or replication Efforts to improve the quality of data included  within the algorithm, including efforts to balance  the use of risk and protective factorsUse of data with data integrity issues or “dirty”  data that reflect bias in data collection (resulting  in garbage in/garbage out) Using “early warning” indicators to provide  meaningful services and supports to clientsUsing “early warning” indicators for increased  surveillance, punitive action, monitoring, or  “threat” amplification via a risk score Using multiple measures of validity and fairness.
(m / multi-sentence
      :snt2 (a / and
            :op1 (e / elevate-01
                  :ARG1 (m2 / make-01
                        :ARG1 (d / decide-01
                              :ARG3 (a2 / algorithm)))
                  :ARG4 (o / over
                        :op1 (j / judge-01
                              :ARG0 (p / person
                                    :ARG0-of (p2 / practice-01)
                                    :ARG1-of (s / season-01)))))
            :op2 (a3 / and
                  :op1 (a4 / assess-01
                        :ARG1 (ii / impact-01)
                        :ARG2-of (ii2 / involve-01
                              :ARG1 (t / think-through-04
                                    :ARG1 (a5 / and
                                          :op1 (t2 / thing
                                                :ARG1-of (ii3 / intend-01))
                                          :op2 (t3 / thing
                                                :ARG1-of (ii4 / intend-01
                                                      :polarity -)))
                                    :degree (t4 / thorough)))))
            :op2 (c / communicate-01
                  :ARG1 a5
                  :ARG2 (s2 / stakeholder)))
      :snt3 (u / use-01
            :ARG1 (o2 / or
                  :op1 (b / box
                        :ARG1-of (b2 / black-04))
                  :op2 (a6 / algorithm
                        :mod (p3 / proprietary)))
            :ARG2 (a7 / and
                  :op1 (d2 / develop-02
                        :ARG1 a6)
                  :op2 (ii5 / implement-01
                        :ARG1 a6))
            :ARG0-of (c2 / cause-01
                  :ARG1 (b3 / benefit-01
                        :polarity -
                        :ARG1 (ii6 / individual
                              :ARG1-of (ii7 / include-91
                                    :ARG2 (d3 / data))))))
      :snt4 (u2 / use-01
            :ARG1 (t5 / thing
                  :ARG0-of (ii8 / indicate-01
                        :ARG1 (o3 / or
                              :op1 (s3 / surveil-01
                                    :ARG1-of (ii9 / increase-01))
                              :op2 (a8 / act-02
                                    :ARG0-of (p4 / punish-01))
                              :op3 (m3 / monitor-01)
                              :op4 (a9 / amplify-01
                                    :ARG1 (t6 / threaten-01)))))
            :ARG2 (m4 / measure-01
                  :ARG1 (a10 / and
                        :op1 (v / valid-02)
                        :op2 (f / fairness))
                  :quant (m5 / multiple)))
      :snt5 (e2 / exemplify-01
            :ARG0 (a11 / and
                  :op1 (d4 / describe-01
                        :ARG1 (a12 / and
                              :op1 (p5 / process-02
                                    :ARG1 (d5 / design-01))
                              :op2 (t7 / test-01))))
            :op2 (l / list
                  :consist-of (d6 / data
                        :ARG1-of (u3 / use-01
                              :ARG2 (a13 / and
                                    :op1 (d7 / develop-02
                                          :op2 (v2 / validate-01
                                                :ARG1 a12)))))))
      :snt6 (u4 / use-01
            :ARG1 (t8 / thing
                  :ARG0-of (ii10 / indicate-01
                        :ARG1 (w / warn-01
                              :time (e3 / early)))))
      :snt7 (p6 / possible-01
            :ARG1 (o4 / override-01
                  :ARG0 (h / human)
                  :ARG1 (a14 / algorithm)
                  :time (p7 / point
                        :mod (a15 / any)
                        :part-of p5)))
      :snt1 p2
      :ARG1-of (p8 / productive-03)
      :time (d8 / date-entity
            :year 2019))


# ::id 35
# ::snt e.g., testing of metrics that center racial equity  such as false positives/negatives across race  and genderUse of biometric data (specifically facial  recognition)
27 CENTERING RACIAL EQUITY THROUGHOUT THE DATA LIFE CYCLERacial Equity in Algorithms   and Statistical Tools:   WORK IN ACTION  Allegheny County Child Welfare  Office  by AISP with contributions from Katy Collins Allegheny County, Pennsylvania’s Family Screening Tool (AFST) is a predictive analytic algorithm  for child welfare that has received a great deal of study24 and national press, including influential  critiques.25 While we remain concerned about the use of algorithms based upon underlying data  that are rich with systemic bias and a history of discriminatory impact, we acknowledge Allegheny  County’s notable efforts in following best practices of algorithm development, including the five  principles summarized above: responsibility, explainability, accuracy, auditability, and fairness.
(m / multi-sentence
      :snt1 (e / exemplify-01
            :ARG0 (t / test-01
                  :ARG1 (m2 / metric
                        :ARG0-of (c / center-01
                              :ARG1 (e2 / equity
                                    :mod (r / race))
                              :example (s / slash
                                    :op1 (p / positive
                                          :mod (f / false))
                                    :op2 (n / negative-02)
                                    :mod (a / across
                                          :op1 (a2 / and
                                                :op1 (r2 / race)
                                                :op2 (g / gender)))))))
            :ARG1 (u / use-01
                  :ARG1 (d / data
                        :mod (b / biometric)
                        :ARG1-of (s2 / specific-02
                              :ARG2 (r3 / recognize-02
                                    :ARG1 (f2 / face))))))
      :snt2 e
      :ARG0 (a3 / algorithm
            :ARG0-of (a4 / analyze-01)
            :ARG0-of (r4 / receive-01
                  :ARG1 (a5 / and
                        :op1 (s3 / study-01)
                        :op2 (p2 / press
                              :mod (n2 / nation))
                        :ARG2-of (ii / include-01
                              :ARG1 (c2 / critique-01
                                    :ARG0-of (ii2 / influence-01))))
                  :ARG1-of (c3 / cite-01
                        :ARG2 24))
            :domain (a6 / algorithm
                  :ARG1-of (b2 / base-02
                        :ARG2 (d2 / data
                              :ARG0-of (u2 / underlie-01)
                              :ARG1-of (r5 / rich-02
                                    :ARG2 (a7 / and
                                          :op1 (b3 / bias-01
                                                :mod (s4 / systemic))
                                          :op2 (ii3 / impact-01
                                                :ARG0-of (d3 / discriminate-01)))))))
            :location (c4 / county
                  :name (n3 / name
                        :op1 "Allegheny"
                        :op2 "County")))
      :ARG1 (w / work-01
            :ARG0 (o / organization
                  :name (n4 / name
                        :op1 "AISP"))
            :ARG1 (o2 / organization
                  :name (n5 / name
                        :op1 "Allegheny"
                        :op2 "County"
                        :op3 "Child"
                        :op4 "Welfare"
                        :op5 "Office"))
            :ARG1-of (c5 / contribute-01
                  :ARG0 (p3 / person
                        :name (n6 / name
                              :op1 "Katy"
                              :op2 "Collins"))))
      :snt3 (a8 / acknowledge-01
            :ARG0 (w2 / we)
            :ARG1 (e3 / effort-01
                  :ARG0 c4
                  :ARG1 (f3 / follow-02
                        :ARG0 c4
                        :ARG1 (p4 / practice-01
                              :ARG1 (d4 / develop-02
                                    :ARG1 (a9 / algorithm))
                              :ARG1-of (h / have-degree-91
                                    :ARG2 (g2 / good-02
                                          :ARG1 p4)
                                    :ARG3 (m3 / most)))
                        :ARG2-of (s5 / summarize-01
                              :ARG1 (p5 / principle
                                    :quant 5))))))


# ::id 35
# ::snt In Proceedings of the conference on fairness,  accountability, and transparency (pp.
(b / be-located-at-91
      :ARG1 (p / publication-91
            :ARG1 (a / and
                  :op1 (f / fairness)
                  :op2 (a2 / accountable-02)
                  :op3 (t / transparency))
            :ARG2 (p2 / Proceedings
                  :part-of (c / conference))
            :ARG7 (v / value-interval
                  :op1 67
                  :op2 89)))


# ::id 35
# ::snt Algorithmic fairness: A code-based primer for public-sector data  scientists .
(m / multi-sentence
      :snt1 (f / fairness
            :mod (a / algorithm))
      :snt2 (p / primer
            :purpose (s / scientist
                  :mod (s2 / sector
                        :ARG1-of (p2 / public-02)))
            :ARG1-of (b / base-02
                  :ARG2 (c / code))))


# ::id 35
# ::snt Proceedings of the 2020 Conference on Fairness, Accountability, and  Transparency.
(p / proceed-01
      :ARG1 (c / conference
            :name (n / name
                  :op1 "Conference"
                  :op2 "on"
                  :op3 "Fairness"
                  :op4 "Accountability"
                  :op5 "and"
                  :op6 "Transparency")
            :time (d / date-entity
                  :year 2020)))


# ::id 35
# ::snt Proceedings of the 2020 Conference on Fairness, Accountability, and  Transparency.
(p / proceed-01
      :ARG1 (c / conference
            :name (n / name
                  :op1 "Conference"
                  :op2 "on"
                  :op3 "Fairness"
                  :op4 "Accountability"
                  :op5 "and"
                  :op6 "Transparency")
            :time (d / date-entity
                  :year 2020)))


# ::id 38
# ::snt The project started in 2014 as a collaboration between UCM, the Carlos III University of Ma-drid, the University of Rome “La Sapienza” , and the Ministry of Home Affairs of Spain.LINKS: You can find a list of all URLs in the report compiled online at:   www.algorithmwatch.org/   automating-societyexternal T aking Stock of Automated Decision-Making in the EU  page 121
/ Predictive evaluation by SAVRY  The goal of this investigation is to evaluate the predictive power and fairness of an expert  assessment instrument called the Structured Assessment of Violence in Youth (SAVRY) and to compare it against standard machine learning (ML) algorithms.
(m / multi-sentence
      :snt1 (s / start-01
            :ARG1 (p / project)
            :ARG2 (c / collaborate-01
                  :ARG0 (a / and
                        :op1 (u / university
                              :name (n / name
                                    :op1 "UCM"))
                        :op2 (u2 / university
                              :name (n2 / name
                                    :op1 "Carlos"
                                    :op2 "III"
                                    :op3 "University"
                                    :op4 "of"
                                    :op5 "Ma-drid"))
                        :op3 (u3 / university
                              :name (n3 / name
                                    :op1 "La"
                                    :op2 "Sapienza"))
                        :op4 (g / government-organization
                              :name (n4 / name
                                    :op1 "Ministry"
                                    :op2 "of"
                                    :op3 "Home"
                                    :op4 "Affairs")
                              :poss (c2 / country
                                    :name (n5 / name
                                          :op1 "Spain")))))
            :time (d / date-entity
                  :year 2014))
      :snt2 (h / have-purpose-91
            :ARG1 (ii / investigate-01
                  :mod (t / this))
            :ARG2 (a2 / and
                  :op1 (e / evaluate-01
                        :ARG1 (a3 / and
                              :op1 (p2 / power
                                    :ARG1-of (p3 / predict-01)
                                    :poss (ii2 / instrument
                                          :ARG0-of (a4 / assess-01)
                                          :ARG1-of (c3 / call-01
                                                :ARG2 (n6 / name
                                                      :op1 "Structured"
                                                      :op2 "Assessment"
                                                      :op3 "of"
                                                      :op4 "Violence"
                                                      :op5 "in"
                                                      :op6 "Youth"))))
                              :op2 (f / fair-01
                                    :ARG1 ii2)))
                  :op2 (c4 / compare-01
                        :ARG1 ii2
                        :ARG2 (a5 / algorithm
                              :mod (m2 / machine
                                    :ARG1-of (l / learn-01))
                              :ARG1-of (s2 / standard-02)))))
      :snt3 (p4 / possible-01
            :ARG1 (f2 / find-01
                  :ARG0 (y / you)
                  :ARG1 (l2 / list
                        :consist-of (u4 / url
                              :mod (a6 / all))
                        :ARG1-of (c5 / compile-01
                              :medium (o / online)))
                  :location (u5 / url-entity
                        :value "www.algorithmwatch.org/Automating-societyexternal T aking Stock"
                        :topic (m3 / make-01
                              :ARG1 (d2 / decide-01)
                              :ARG1-of (a7 / automate-01)))
                  :location (p5 / page
                        :mod 121
                        :part-of (p6 / publication
                              :name (n7 / name
                                    :op1 "EU"))))))


# ::id 38
# ::snt The Fairness Measures Project is a group of data scientists from Chile, Germany and Spain, led by Carlos Castillo (Pompeu Fabra Uni-versity, Barcelona).
(g / group
      :consist-of (s / scientist
            :mod (d / data)
            :source (a / and
                  :op1 (c / country
                        :name (n / name
                              :op1 "Chile"))
                  :op2 (c2 / country
                        :name (n2 / name
                              :op1 "Germany"))
                  :op3 (c3 / country
                        :name (n3 / name
                              :op1 "Spain"))))
      :domain (p / project
            :name (n4 / name
                  :op1 "Fairness"
                  :op2 "Measures"
                  :op3 "Project"))
      :ARG1-of (l / lead-02
            :ARG0 (p2 / person
                  :name (n5 / name
                        :op1 "Carlos"
                        :op2 "Castillo")
                  :ARG0-of (h / have-org-role-91
                        :ARG1 (u / university
                              :name (n6 / name
                                    :op1 "Pompeu"
                                    :op2 "Fabra"
                                    :op3 "Univeristy")
                              :location (c4 / city
                                    :name (n7 / name
                                          :op1 "Barcelona")))))))


# ::id 38
# ::snt The Communication explicitly addresses the risk posed by automated decision-making: “Some AI applications may raise new ethical and legal questions, related to liability or fairness of decision-making” .
(a / address-01
      :ARG0 (c / communicate-01)
      :ARG1 (r / risk-01
            :ARG1 (m / make-01
                  :ARG1 (d / decide-01)
                  :ARG1-of (a2 / automate-01)))
      :ARG2 (p / possible-01
            :ARG1 (r2 / raise-01
                  :ARG0 (a3 / application
                        :mod (ii / intelligent-01
                              :mod (a4 / artificial))
                        :mod (s / some))
                  :ARG1 (q / question-01
                        :ARG1 (a5 / and
                              :op1 (e / ethics)
                              :op2 (l / legal-02))
                        :ARG1-of (n / new-01)
                        :ARG1-of (r3 / relate-01
                              :ARG2 (o / or
                                    :op1 (l2 / liable-01
                                          :ARG2 (m2 / make-01
                                                :ARG1 (d2 / decide-01)))
                                    :op2 (f / fair-01
                                          :ARG1 m2))))))
      :ARG1-of (e2 / explicit-03))


# ::id 38
# ::snt By the summer of 2019, the Communication foresees the creation of a framework for all  relevant stakeholders and experts—see European AI Alliance and High-Level Expert Group in this chapter—who will draft the guidelines, addressing issues such as future of work, fairness, safety, security, social inclusion and algorithmic transparency.
(m / multi-sentence
      :snt1 (f / foresee-01
            :ARG0 (o / organization
                  :name (n / name
                        :op1 "Communication"))
            :ARG1 (c / create-01
                  :ARG1 (f2 / framework
                        :beneficiary (a / and
                              :op1 (s / stake-01
                                    :ARG1-of (r / relevant-01))
                              :op2 (p / person
                                    :ARG1-of (e / expert-01))
                              :mod (a2 / all))
                        :ARG0-of (d / draft-01
                              :ARG1 (g / guideline
                                    :ARG0-of (a3 / address-02
                                          :ARG1 (ii / issue-02
                                                :example (a4 / and
                                                      :op1 (f3 / future
                                                            :poss (w / work-01))
                                                      :op2 (f4 / fairness)
                                                      :op3 (s2 / safe-01)
                                                      :op4 (s3 / security)
                                                      :op5 (ii2 / include-01
                                                            :ARG2 (s4 / society))
                                                      :op6 (t / transparency
                                                            :mod (a5 / algorithm)))))))))
            :time (b / by
                  :op1 (d2 / date-entity
                        :year 2019
                        :season (s5 / summer))))
      :snt2 (s6 / see-01
            :mode imperative
            :ARG0 (y / you)
            :ARG1 (a6 / and
                  :op1 (o2 / organization
                        :name (n2 / name
                              :op1 "European"
                              :op2 "AI"
                              :op3 "Alliance"))
                  :op2 (o3 / organization
                        :name (n3 / name
                              :op1 "High-Level"
                              :op2 "Expert"
                              :op3 "Group")))
            :location (c2 / chapter
                  :mod (t2 / this))))


# ::id 38
# ::snt The guidelines will cover issues such as fairness, safety, transparency, the future of work, democracy and more broadly the impact of AI and automated decision-making on the application of the Charter of Fundamental Rights, including: privacy and personal data protection, dignity, consumer protection and non-discrimination.
(c / cover-01
      :ARG0 (g / guideline)
      :ARG1 (ii / issue-02
            :ARG0 (a / and
                  :op1 (f / fairness)
                  :op2 (s / safe-01)
                  :op3 (t / transparency)
                  :op4 (f2 / future
                        :poss (w / work-01))
                  :op5 (d / democracy)
                  :op6 (ii2 / impact-01
                        :ARG0 (a2 / and
                              :op1 (ii3 / intelligent-01
                                    :mod (a3 / artificial))
                              :op2 (m / make-01
                                    :ARG1 (d2 / decide-01)
                                    :ARG1-of (a4 / automate-01)))
                        :ARG1 (a5 / apply-02
                              :ARG1 (l / law
                                    :name (n / name
                                          :op1 "Charter"
                                          :op2 "of"
                                          :op3 "Fundamental"
                                          :op4 "Rights")
                                    :ARG2-of (ii4 / include-01
                                          :ARG1 (a6 / and
                                                :op1 (p / privacy)
                                                :op2 (p2 / protect-01
                                                      :ARG1 (d3 / data
                                                            :ARG1-of (p3 / personal-02)))
                                                :op3 (d4 / dignity)
                                                :op4 (p4 / protect-01
                                                      :ARG1 (p5 / person
                                                            :ARG0-of (c2 / consume-01)))
                                                :op5 (d5 / discriminate-01
                                                      :polarity -)))))
                        :ARG1-of (h / have-degree-91
                              :ARG2 (b / broad-02
                                    :ARG1 ii2)
                              :ARG3 (m2 / more))))))


# ::id 38
# ::snt And how can the benefits brought about by new digital technologies be equitably shared among all?”  external [eU 54]   W The Declar ation on Ethics and Data Protection in Artificial Intelligence, adopted at  the 2018 International Conference of Data  p rotection and  p rivacy Commissioners,  endorses guiding principles in regard to AI—including elaborations on fairness, accountability, systems transparency and intelligibility, ethics and privacy by design, empowerment and public engagement and the reduction and mitigation of unlawful bias and discrimination  external [eU 55]  W An early opinion b y the ED p S from 2014 analyses the EU Commission's proposal for a  Regulation of the European  parliament and of the Council on a European network of  Emplo yment Services, workers’ access to mobility services and the further integration  of labour markets, among others, on the use of ADM in job matching at the EURES portal  external [eU 56]page 32 a utomating s ociety european Union
/ eU employment equality law On the basis of three directives (2000/43/EC, 2000/78/EC, 2002/73/EC) and various court  decisions especially dealing with Art.
(m / multi-sentence
      :snt1 (a / and
            :op2 (p / possible-01
                  :ARG1 (s / share-01
                        :ARG0 (a2 / all)
                        :ARG1 (b / benefit
                              :ARG1-of (b2 / bring-about-05
                                    :ARG0 (t / technology
                                          :mod (d / digital)
                                          :ARG1-of (n / new-01))))
                        :manner (e / equitable))))
      :snt2 (a3 / and
            :op1 (e2 / external
                  :mod 54)
            :op2 (e3 / external
                  :mod 55))
      :snt3 (a4 / analyze-01
            :ARG0 (t2 / thing
                  :ARG1-of (o / opine-01
                        :ARG0 (o2 / organization
                              :name (n2 / name
                                    :op1 "ED")))
                  :time (d2 / date-entity
                        :year 2014))
            :ARG1 (p2 / propose-01
                  :ARG0 (a5 / and
                        :op1 (o3 / organization
                              :name (n3 / name
                                    :op1 "European"
                                    :op2 "Commission"))
                        :op2 (o4 / organization
                              :name (n4 / name
                                    :op1 "Council"
                                    :op2 "on"
                                    :op3 "a"
                                    :op4 "European"
                                    :op5 "Network"
                                    :op6 "of"
                                    :op7 "Emplo-yment"
                                    :op8 "Services")))
                  :ARG1 (l / law
                        :name (n5 / name
                              :op1 "European"
                              :op2 "Union"
                              :op3 "Health"
                              :op4 "Equality"
                              :op5 "Law")))
            :ARG1-of (b3 / base-02
                  :ARG2 (a6 / and
                        :op1 (d3 / directive
                              :quant 3
                              :mod (o5 / organization
                                    :name (n6 / name
                                          :op1 "European"
                                          :op2 "Union")))
                        :op2 (d4 / directive
                              :quant (v / various)
                              :mod (c / court)
                              :ARG0-of (d5 / deal-01
                                    :ARG1 (l2 / law
                                          :name (n7 / name
                                                :op1 "Art"))
                                    :mod (e4 / especially))))))
      :snt4 (a7 / and
            :op1 (f / fairness)
            :op2 (a8 / accountable-02)
            :op3 (t3 / transparency)
            :op4 (ii / intelligibility)
            :op5 (e5 / ethics)
            :op6 (p3 / privacy)
            :manner (d6 / design-01))
      :op7 (e6 / empower-01)
      :op8 (e7 / engage-01
            :ARG1 (p4 / public))
      :snt5 (a9 / and
            :op1 (r / reduce-01
                  :ARG1 (b4 / bias-01
                        :mod (l3 / law
                              :polarity -)))
            :op2 (m2 / mitigate-01
                  :ARG1 (d7 / discriminate-02))))


# ::id 38
# ::snt Implications of particular relevance to the participatory and fairness aspects of automated  decision-making—which also have strong effects on the regulatory and policy framework for the application of ADM—can be found in the following paragraphs: Section 3.1, focusing on Strengthening Research in Germany and Europe and section 3.8, Making data available and facilitating (re-)use introduce a “systematic approach to technology” that promotes research on methods for “monitoring and traceability of algorithmic predic-tion and decision-making systems” , as well as research on pseudonymisation and anonymi-sation methods, and on the compilation of synthetic training data.
(p / possible-01
      :ARG1 (f / find-01
            :ARG1 (ii / implicate-01
                  :ARG0-of (a / affect-01
                        :ARG1 (a2 / and
                              :op1 (f2 / framework
                                    :ARG0-of (r / regulate-01))
                              :op2 (f3 / framework
                                    :mod (p2 / policy-01)))
                        :ARG2 (a3 / apply-02
                              :ARG1 (p3 / product
                                    :name (n / name
                                          :op1 "ADM")))
                        :ARG1-of (s / strong-02)
                        :mod (a4 / also))
                  :ARG1-of (r2 / relevant-01
                        :ARG2 (a5 / aspect
                              :mod (p4 / particular)
                              :mod (p5 / participate-01)
                              :mod (m / make-01
                                    :ARG1 (d / decide-01)
                                    :ARG1-of (a6 / automate-01)))))
            :location (p6 / paragraph
                  :ARG1-of (f4 / follow-04)
                  :ARG2-of (ii2 / include-91
                        :ARG1 (a7 / and
                              :op1 (s2 / section
                                    :mod 3.1
                                    :ARG0-of (f5 / focus-01
                                          :ARG1 (s3 / strengthen-01
                                                :ARG1 (r3 / research-01
                                                      :location (a8 / and
                                                            :op1 (c / country
                                                                  :name (n2 / name
                                                                        :op1 "Germany"))
                                                            :op2 (c2 / continent
                                                                  :name (n3 / name
                                                                        :op1 "Europe")))))))
                              :op2 (s4 / section
                                    :mod 3.8
                                    :topic (a9 / and
                                          :op1 (m2 / make-02
                                                :ARG1 (a10 / available-02
                                                      :ARG2 (d2 / data)))
                                          :op2 (f6 / facilitate-01
                                                :ARG1 (u / use-01
                                                      :ARG1 d2
                                                      :mod (a11 / again)))
                                          :ARG0-of (ii3 / introduce-02
                                                :ARG1 (a12 / approach-02
                                                      :ARG1 (t / technology)
                                                      :mod (s5 / systematic))
                                                :ARG0-of (p7 / promote-02
                                                      :ARG1 (r4 / research-01
                                                            :ARG1 (a13 / and
                                                                  :op1 (m3 / method
                                                                        :purpose (m4 / monitor-01
                                                                              :ARG1 (a14 / and
                                                                                    :op1 (s6 / system
                                                                                          :mod (p8 / predict-01
                                                                                                :manner (a15 / algorithm)))
                                                                                    :op2 (s7 / system
                                                                                          :mod (m5 / make-01
                                                                                                :ARG1 (d3 / decide-01)))))))
                                                            :op2 (m6 / method
                                                                  :purpose (a16 / and
                                                                        :op1 (p9 / pseudonymize-00)
                                                                        :op2 (m7 / method
                                                                              :mod (a17 / anonymity)))))
                                                      :op3 (c3 / compile-01
                                                            :ARG1 (d4 / data
                                                                  :mod (t2 / train-01)
                                                                  :mod (s8 / synthetic))))))))))))


# ::id 38
# ::snt external [NL 20] Notable recent research includes a report external [NL 21]  that is part of a research collaboration  between the Universities of Amsterdam, Tilburg, Radboud, Utrecht and Eindhoven (TU/e) on automated decision-making, and which forms part of the groups’ research on fairness in automated decision-making.
(ii / include-01
      :ARG1 (r / report-01
            :ARG1 (c / collaborate-01
                  :ARG0 (a / and
                        :op1 (u / university
                              :name (n / name
                                    :op1 "University"
                                    :op2 "of"
                                    :op3 "Amsterdam"))
                        :op2 (u2 / university
                              :name (n2 / name
                                    :op1 "Tilburg"))
                        :op3 (u3 / university
                              :name (n3 / name
                                    :op1 "Radboud"))
                        :op4 (u4 / university
                              :name (n4 / name
                                    :op1 "Utrecht"))
                        :op5 (u5 / university
                              :name (n5 / name
                                    :op1 "Eindhoven")))
                  :ARG1 (r2 / research-01
                        :ARG1 (m / make-01
                              :ARG1 (d / decide-01)
                              :ARG1-of (a2 / automate-01))))
            :part-of (r3 / research-01
                  :ARG0 a))
      :ARG2 (r4 / research-01
            :time (r5 / recent)
            :ARG1-of (n6 / notable-04))
      :ARG1-of (d2 / describe-01
            :ARG0 (p / publication
                  :name (n7 / name
                        :op1 "NL"
                        :op2 20))))


# ::id 38
# ::snt Lenart J. Ku čić  LINKS: You can find a list  of all URLs in the report compiled online at:   www.algorithmwatch.org/   automating-societyexternalpage 116 a utomating Society Slovenia
barcelona spainT aking Stock of   Automated DecisionMaking in the EUautomating Society – The Fairness Measures  Project is an international group of data scientists who develop fairness-aware algorithms and systems and provide relevant software and data sets to the research community.
(m / multi-sentence
      :snt1 (l / link-01
            :ARG2 (p / person
                  :name (n / name
                        :op1 "Lenart"
                        :op2 "J."
                        :op3 "Ku"
                        :op4 " čić")))
      :snt2 (p2 / possible-01
            :ARG1 (f / find-01
                  :ARG0 (y / you)
                  :ARG1 (l2 / list-01
                        :ARG1 (u / url
                              :mod (a / all)))
                  :location (r / report
                        :ARG1-of (c / compile-01
                              :medium (o / online)))))
      :snt3 (o2 / organization
            :name (n2 / name
                  :op1 "Automating"
                  :op2 "Society"
                  :op3 "externalpage"
                  :op4 116
                  :op5 "116"
                  :op6 "a"
                  :op7 " utomating"
                  :op8 "Society")
            :domain (g / group
                  :consist-of (s / scientist
                        :mod (d / data))
                  :ARG0-of (d2 / develop-02
                        :ARG1 (a2 / and
                              :op1 (a3 / algorithm)
                              :op2 (s2 / system)
                              :ARG0-of (r2 / realize-01
                                    :ARG1 (f2 / fairness))))
                  :ARG0-of (p3 / provide-01
                        :ARG1 a2
                        :op1 (s3 / software)
                        :op2 (s4 / set
                              :consist-of d))
                  :ARG1-of (r3 / relevant-01))
            :ARG2 (c2 / community
                  :mod (r4 / research-01)))
      :mod (ii / international)
      :location a2
      :op1 (c3 / city
            :name (n3 / name
                  :op1 "Slovenia"))
      :op2 c3
      :name (n4 / name
            :op1 "Barcelona")
      :op3 o2
      :name (n5 / name
            :op1 "Automated"
            :op2 "Decision"
            :op3 "Making"
            :op4 "in"
            :op5 "the"
            :op6 "EUautomating"
            :op7 "Society"
            :op8 ","
            :op9 "The"
            :op10 "Fairness"
            :op11 "Measures"))


# ::id 38
# ::snt Political de Bate S on a SPect S of automation -  civil  S ociety and academia / The Fairness Measures Project   The growing use of automated decision-making has the potential to increase the risk of discrimination against disadvantaged groups.
(p / potential
      :domain (ii / increase-01
            :ARG0 (u / use-01
                  :ARG1 (m / make-01
                        :ARG1 (d / decide-01)
                        :ARG1-of (a / automate-01))
                  :ARG1-of (g / grow-01))
            :ARG1 (r / risk-01
                  :ARG1 (d2 / discriminate-02
                        :ARG1 (g2 / group
                              :ARG1-of (a2 / advantage-01
                                    :polarity -)))))
      :prep-on (t / topic
            :name (n / name
                  :op1 "Political"
                  :op2 "de"
                  :op3 "Bate")
            :topic (a3 / automate-01)
            :example (a4 / and
                  :op1 (c / civil)
                  :op2 (e / economy)
                  :op3 (a5 / academia))
            :example (p2 / project
                  :name (n2 / name
                        :op1 "The"
                        :op2 "Fairness"
                        :op3 "Measures"
                        :op4 "Project"))))


# ::id 38
# ::snt The main goal of this group is to develop fairness-aware algorithms and systems  external [SP 8] , and to provide relevant software and datasets to the research community  through the website fairness-measures.org.
(h / have-purpose-91
      :ARG1 (g / group
            :mod (t / this))
      :ARG2 (a / and
            :op1 (d / develop-02
                  :ARG0 g
                  :ARG1 (a2 / and
                        :op1 (a3 / algorithm
                              :ARG0-of (r / realize-01
                                    :ARG1 (f / fairness)))
                        :op2 (s / system
                              :mod (e / external)
                              :ARG1-of (d2 / describe-01
                                    :ARG0 (p / publication
                                          :name (n / name
                                                :op1 "SP"
                                                :op2 8))))))
            :op2 (p2 / provide-01
                  :ARG0 g
                  :ARG1 (a4 / and
                        :op1 (s2 / software
                              :ARG1-of (r2 / relevant-01))
                        :op2 (d3 / dataset
                              :ARG1-of (r3 / relevant-01)))
                  :ARG2 (c / community
                        :mod (r4 / research-01))
                  :medium (w / website
                        :name n
                        :op1 "fair-measures.org")))
      :mod (m / main))


# ::id 38
# ::snt The data sets cover several fields and applica-tions such as finance, law and human resources, and provide common fairness definitions for machine learning.
(a / and
      :op1 (c / cover-01
            :ARG0 (s / set
                  :consist-of (d / data))
            :ARG1 (a2 / and
                  :op1 (f / field)
                  :op2 (a3 / apply-02
                        :ARG1 f)
                  :example (a4 / and
                        :op1 (f2 / finance)
                        :op2 (l / law)
                        :op3 (r / resource
                              :mod (h / human)))
                  :quant (s2 / several)))
      :op2 (p / provide-01
            :ARG0 s
            :ARG1 (d2 / define-01
                  :ARG1 (f3 / fair-01)
                  :ARG1-of (s3 / share-01)
                  :topic (l2 / learn-01
                        :ARG1 (m / machine)))))


# ::id 38
# ::snt The HUMA in T (HUmanity vs MAchine  in T elligence) external [SP 18]  is an interdisciplinary  research project that proposes evaluating fairness, taking into account the uncertainty  of some predictions.
(p / project
      :mod (r / research-01
            :mod (ii / interdisciplinary))
      :ARG0-of (p2 / propose-01
            :ARG1 (e / evaluate-01
                  :ARG1 (f / fairness)
                  :manner (t / take-into-account-04
                        :ARG1 (c / certain
                              :polarity -
                              :domain (p3 / predict-01
                                    :mod (s / some))))))
      :domain (p4 / publication
            :name (n / name
                  :op1 "HUMA"
                  :op2 "in"
                  :op3 "T"
                  :op4 "Elligence")
            :ARG1-of (d / describe-01
                  :ARG0 (p5 / publication
                        :name (n2 / name
                              :op1 "SP"
                              :op2 18)))
            :mod (e2 / external)))


# ::id 38
# ::snt In addition, it discusses the implications of different sources of bias for fairness and performance analysis.
(a / and
      :op2 (d / discuss-01
            :ARG0 (ii / it)
            :ARG1 (ii2 / implicate-01
                  :ARG1 (s / source-02
                        :ARG1 (b / bias-01
                              :ARG1 (a2 / and
                                    :op1 (f / fairness)
                                    :op2 (a3 / analyze-01
                                          :ARG1 (p / perform-02))))
                        :ARG1-of (d2 / differ-02)))))


# ::id 39
# ::snt Ethics and fairness   Besides constraints stemming from sector -specific and cross -cutting regulations, ethical issues lie at  the core of the ever -increasing usage of AI in business p rocesses which impact individuals and groups  of people.
(l / lie-07
      :ARG1 (ii / issue-02
            :ARG0 (e / ethics))
      :ARG2 (c / core
            :poss (u / use-01
                  :ARG1 (ii2 / intelligent-01
                        :mod (a / artificial))
                  :ARG2 (p / problem
                        :mod (b / business)
                        :ARG0-of (ii3 / impact-01
                              :ARG1 (a2 / and
                                    :op1 (ii4 / individual)
                                    :op2 (g / group
                                          :consist-of (p2 / person)))))
                  :ARG1-of (ii5 / increase-01
                        :time (e2 / ever))))
      :ARG1-of (ii6 / instead-of-91
            :ARG2 (s / stem-01
                  :ARG1 (c2 / constrain-01)
                  :ARG2 (r / regulate-01
                        :ARG1-of (s2 / specific-02
                              :ARG2 (s3 / sector))
                        :ARG0-of (c3 / cut-01
                              :ARG0-of (c4 / cross-02)))))
      :example (a3 / and
            :op1 e
            :op2 (f / fairness)))


# ::id 39
# ::snt The presence of a statistical bias may lead to a fairness bias, but this  is neither a sufficient nor a necessary condition.
(c / contrast-01
      :ARG1 (p / possible-01
            :ARG1 (l / lead-03
                  :ARG0 (p2 / present-02
                        :ARG1 (b / bias-01
                              :ARG1 (s / statistics)))
                  :ARG2 (b2 / bias-01
                        :ARG3 (f / fair-01))))
      :ARG2 (a / and
            :op1 (s2 / suffice-01
                  :polarity -
                  :ARG0 p2)
            :op2 (n / need-01
                  :polarity -
                  :ARG1 p2)
            :domain (c2 / condition)))


# ::id 39
# ::snt Besides,  counterfactual explanations constitute a particularly interesting case of post -modelling  explanatory  method insofar as they can  contribute to assessing that the appropriate data management principle  described in this document has been followed, both in terms of regulatory compliance (specifically  with respect to GDPR) and in terms of ethics and fairness.
(c / constitute-01
      :ARG0 (e / explain-01
            :mod (c2 / counterfactual))
      :ARG1 (c3 / case-04
            :ARG1 (m / method
                  :ARG0-of (e2 / explain-01)
                  :time (a / after
                        :op1 (m2 / model-01)))
            :ARG0-of (ii / interest-01
                  :mod (p / particular)
                  :ARG1-of (c4 / cause-01
                        :ARG0 (p2 / possible-01
                              :ARG1 (c5 / contribute-01
                                    :ARG0 m
                                    :ARG2 (a2 / assess-01
                                          :ARG1 (f / follow-02
                                                :ARG1 (p3 / principle
                                                      :ARG1-of (a3 / appropriate-02)
                                                      :topic (m3 / manage-01
                                                            :ARG1 (d / data))
                                                      :ARG1-of (d2 / describe-01
                                                            :ARG0 (d3 / document
                                                                  :mod (t / this))))
                                                :topic (a4 / and
                                                      :op1 (c6 / comply-01
                                                            :ARG1 (r / regulate-01)
                                                            :ARG1-of (s / specific-02
                                                                  :ARG2 (l / law
                                                                        :name (n / name
                                                                              :op1 "GDPR"))))
                                                      :op2 (a5 / and
                                                            :op1 (e3 / ethics)
                                                            :op2 (f2 / fairness))))))))))
      :mod (b / besides))


# ::id 39
# ::snt Those issues include social and ethical concerns in the broadest sense, and particularly  questions of fairness raised by any automated or computer -aided decision process.
(ii / include-01
      :ARG1 (a / and
            :op1 (c / concern-01
                  :ARG0 (a2 / and
                        :op1 (s / society)
                        :op2 (e / ethics))
                  :manner (s2 / sense
                        :ARG1-of (h / have-degree-91
                              :ARG2 (b / broad-02
                                    :ARG1 s2)
                              :ARG3 (m / most))))
            :op2 (q / question-01
                  :ARG1 (f / fair-01)
                  :mod (p / particular)
                  :ARG1-of (r / raise-01
                        :ARG0 (o / or
                              :op1 (p2 / process-02
                                    :ARG1 (d / decide-01)
                                    :ARG1-of (a3 / automate-01))
                              :op2 (a4 / assist-01
                                    :ARG0 (c2 / computer))
                              :mod (a5 / any)))))
      :ARG2 (ii2 / issue-02
            :mod (t / that)))


# ::id 39
# ::snt 10   into the less -critical business processes (and those which bear little ethics and fairness risks): it can  thus be anticipated that the progressive industrialization of a dditional AI use cases in the sector will  benefit the currently very active research on those topics.
(a / and
      :li 10
      :op1 (c / concern-02
            :ARG1 (a2 / and
                  :op1 (p / process-02
                        :ARG1 (b / business)
                        :ARG1-of (h / have-degree-91
                              :ARG2 (c2 / critical-02
                                    :ARG1 p)
                              :ARG3 (l / less)))
                  :op2 (p2 / process-02
                        :ARG0-of (b2 / bear-01
                              :ARG1 (r / risk-01
                                    :ARG2 (a3 / and
                                          :op1 (e / ethics)
                                          :op2 (f / fairness))
                                    :quant (l2 / little))))))
      :op2 (p3 / possible-01
            :ARG1 (a4 / anticipate-01
                  :ARG1 (b3 / benefit-01
                        :ARG0 (ii / industrialize-01
                              :ARG1 (c3 / case-04
                                    :ARG1 (u / use-01
                                          :ARG1 (ii2 / intelligent-01
                                                :mod (a5 / artificial)))
                                    :mod (d / diverse))
                              :ARG1-of (p4 / progress-01)
                              :location (s / sector))
                        :ARG1 (r2 / research-01
                              :ARG1 a2
                              :ARG1-of (a6 / activity-06
                                    :degree (v / very)
                                    :time (c4 / current))))
                  :mod (t / thus))))


# ::id 39
# ::snt Diversity, non -discrimination and fairness   6.
(a / and
      :li 6
      :op1 (d / diversity)
      :op2 (d2 / discriminate-01
            :polarity -)
      :op3 (f / fair-01))


# ::id 39
# ::snt Ethical considerations, such  as fairness  of processing and the  absence of discriminatory bias, have to be taken into account  in this regard.
(o / obligate-01
      :ARG2 (c / consider-02
            :ARG1 (a / and
                  :op1 (f / fairness
                        :domain (p / process-01))
                  :op2 (a2 / absent-01
                        :ARG1 (b / bias-01
                              :ARG0-of (d / discriminate-01))))
            :mod (e / ethics))
      :topic (t / this))


# ::id 39
# ::snt Principles to Promote Fairness, Ethics, Accountability and  Transparency in the Use of Artificial Intelligence and Data Analytics in Singapore ’s Financial Sector.
(p / principle
      :purpose (p2 / promote-02
            :ARG1 (a / and
                  :op1 (f / fairness)
                  :op2 (e / ethics)
                  :op3 (a2 / accountable-02)
                  :op4 (t / transparency))
            :manner (u / use-01
                  :ARG1 (a3 / and
                        :op1 (ii / intelligent-01
                              :mod (a4 / artificial))
                        :op2 (a5 / analyze-01
                              :ARG1 (d / data)))
                  :ARG2 (s / sector
                        :mod (f2 / finance)
                        :poss (c / country
                              :name (n / name
                                    :op1 "Singapore"))))))


# ::id 39
# ::snt Exploratory works conducted by the ACPR, along with a broader analysis of the financial sector,  showed that bias detection and mitigation were at an early stage in the industry: as of now, the  emphasis is put on internal validation of AI systems and on the ir regulatory compliance, without  pushing the analysis of algorithmic fairness further than was the case with traditional methods – in  particular, the risk of reinforcing pre-existing  biases tends to be neglected.
(m / multi-sentence
      :snt1 (s / show-01
            :ARG0 (w / work-01
                  :ARG1 (e / explore-01)
                  :ARG1-of (c / conduct-01
                        :ARG0 (o / organization
                              :name (n / name
                                    :op1 "ACPR"))
                        :accompanier (a / analyze-01
                              :ARG1 (s2 / sector
                                    :mod (f / finance))
                              :ARG1-of (h / have-degree-91
                                    :ARG2 (b / broad-02
                                          :ARG1 a)
                                    :ARG3 (m2 / more)))))
            :ARG1 (b2 / be-located-at-91
                  :ARG1 (a2 / and
                        :op1 (d / detect-01
                              :ARG1 (b3 / bias-01))
                        :op2 (m3 / mitigate-01
                              :ARG1 b3))
                  :ARG2 (s3 / stage
                        :mod (e2 / early)
                        :part-of (ii / industry))))
      :snt2 (e3 / emphasize-01
            :ARG1 (a3 / and
                  :op1 (v / validate-01
                        :ARG1 (s4 / system
                              :mod (ii2 / intelligent-01
                                    :mod (a4 / artificial)))
                        :ARG1-of (ii3 / internal-02))
                  :op2 (c2 / comply-01
                        :ARG1 (r / regulate-01)))
            :time (a5 / as-of
                  :op1 (n2 / now))
            :manner (p / push-05
                  :polarity -
                  :ARG1 (a6 / analyze-01
                        :ARG1 (f2 / fairness
                              :mod (a7 / algorithm)))
                  :ARG2 (f3 / further-01
                        :ARG1 a6)
                  :ARG1-of (r2 / resemble-01
                        :polarity -
                        :ARG2 (c3 / case-04
                              :ARG1 (b4 / bias-01
                                    :ARG1-of (p2 / preexist-01))
                              :mod (m4 / method
                                    :mod (t / traditional)))))
            :example (t2 / tend-02
                  :ARG1 (r3 / risk-01
                        :ARG2 (r4 / reinforce-01
                              :ARG1 (b5 / bias-01
                                    :ARG1-of (e4 / exist-01
                                          :time (b6 / before)))))
                  :ARG2 (n3 / neglect-01
                        :ARG1 r3))))


# ::id 39
# ::snt Rejecting the understanding of interpretability as a monolithic concept, Lipton  introduces a continuum based on a number of logical c riteria: trust in the algorithm’s results, causality,  transferability of knowledge, information contained in the decision, fairness of the decision.
(ii / introduce-02
      :ARG0 (p / person
            :name (n / name
                  :op1 "Lipton")
            :ARG0-of (r / reject-01
                  :ARG1 (u / understand-01
                        :ARG1 (p2 / possible-01
                              :ARG1 (ii2 / interpret-01))
                        :ARG3 (c / concept
                              :mod (m / monolith)))))
      :ARG1 (c2 / continuum
            :ARG1-of (b / base-02
                  :ARG2 (n2 / number
                        :quant-of (f / formula
                              :mod (l / logic)
                              :ARG1-of (m2 / mean-01
                                    :ARG2 (a / and
                                          :op1 (t / trust-02
                                                :ARG1 (r2 / result-01
                                                      :ARG1 (a2 / algorithm)))
                                          :op2 (c3 / causality)
                                          :op3 (p3 / possible-01
                                                :ARG1 (t2 / transfer-01
                                                      :ARG1 (k / knowledge)))
                                          :op4 (ii3 / information
                                                :ARG1-of (c4 / contain-01
                                                      :ARG0 (d / decide-01)))
                                          :op5 (f2 / fair-01
                                                :ARG1 d))))))))


# ::id 39
# ::snt ZestFinance Using AI To Bring Fairness To Mortgage Lending  (2019).
(p / publication-91
      :ARG0 (c / company
            :name (n / name
                  :op1 "ZestFinance"))
      :ARG1 (u / use-01
            :ARG0 c
            :ARG1 (a / artificial-intelligence)
            :ARG2 (b / bring-01
                  :ARG0 c
                  :ARG1 (f / fairness)
                  :ARG2 (l / lend-01
                        :ARG1 (m / mortgage-01))))
      :time (d / date-entity
            :year 2019))


# ::id 39
# ::snt Fairness -Aware Classifier with  Prejudice Remover Regularizer.
(a / and
      :op1 (t / thing
            :ARG0-of (c / classify-01)
            :ARG0-of (r / realize-01))
      :op2 (t2 / thing
            :ARG0-of (r2 / remove-01
                  :ARG1 (p / prejudice-01))
            :ARG0-of (r3 / regularize-01)))


# ::id 39
# ::snt In  Proceedings of the Conference on Fairness, Accountability, and Transparency (FAT* ’19) (2019).
(b / be-located-at-91
      :ARG2 (p / proceeding-02
            :ARG2 (c / conference
                  :name (n / name
                        :op1 "Conference"
                        :op2 "on"
                        :op3 "Fairness"
                        :op4 "Accountability"
                        :op5 "and"
                        :op6 "Transparency")
                  :time (d / date-entity
                        :year 2019)))
      :polarity (a / amr-unknown))


# ::id 39
# ::snt More precisely, which fairness metrics enable the identification of bi ases, for example those with a  discriminatory nature ?
(e / enable-01
      :ARG0 (m / metric
            :mod (f / fairness)
            :mod (a / amr-unknown))
      :ARG1 (ii / identify-01
            :ARG1 (a2 / affirmative-action
                  :example (o / organization
                        :ARG0-of (d / discriminate-02
                              :mod (n / nature)))))
      :ARG1-of (h / have-degree-91
            :ARG2 (p / precise)
            :ARG3 (m2 / more)))


# ::id 40
# ::snt ALLAI proposes to add the ‘missing’ requirements from the Ethics Guidelines for Trustworthy AI to the requirements of Chapter 2 of Title III of the AIA, to improve the ability of the AIA to effectively protect our health, safety and fundamental rights from adverse impact of AI: •Human Agency (part of  EGTAI requirement “Human Agency and Oversight”) •Privacy (part of EGTAI requirement “Privacy and Data Governance”) •Diversity, Non-discrimination and Fairness  •Explainability (part of EGTAI requirement “Transparency”) •Societal and Environmental Well-being Finally, the AIA is unclear as regards the situation where the requirements for high-risk AI are not met.
(m / multi-sentence
      :snt1 (p / propose-01
            :ARG0 (o / organization
                  :name (n / name
                        :op1 "AllAI"))
            :ARG1 (a / add-02
                  :ARG0 o
                  :ARG1 (t / thing
                        :ARG1-of (r / require-01)
                        :ARG1-of (m2 / miss-01))
                  :ARG2 (t2 / thing
                        :ARG1-of (r2 / require-01
                              :ARG0 (o2 / organization
                                    :name (n2 / name
                                          :op1 "EGTAI")))
                        :ARG1-of (ii / include-91
                              :ARG2 (a2 / and
                                    :op1 (a3 / agency
                                          :mod (h / human)
                                          :part-of (t3 / thing
                                                :ARG1-of (r3 / require-01
                                                      :ARG0 o2)
                                                :ARG1 (a4 / and
                                                      :op1 a3
                                                      :mod h)
                                                :op2 (o3 / oversee-01
                                                      :ARG1 h))))
                              :op2 (a5 / and
                                    :op1 (p2 / privacy)
                                    :op2 (g / govern-01
                                          :ARG1 (d / data)))
                              :op3 (d2 / diversity)
                              :op4 (d3 / discriminate-02
                                    :polarity -)
                              :op5 (f / fairness)))
                  :part-of (c / chapter
                        :mod 2
                        :part-of (l / law
                              :name (n3 / name
                                    :op1 "Title"
                                    :op2 "III")
                              :part-of l))))
      :purpose (ii2 / improve-01
            :ARG0 o
            :ARG1 (c2 / capable-01
                  :ARG1 o
                  :ARG2 (p3 / protect-01
                        :ARG0 o
                        :ARG1 (a6 / and
                              :op1 (h2 / healthy)
                              :op2 (s / safe-01)
                              :op3 (r4 / right-05
                                    :mod (f2 / fundamental))
                              :poss (w / we))
                        :ARG2 (ii3 / impact-01
                              :ARG0 (ii4 / intelligent-01
                                    :mod (a7 / artificial))
                              :mod (a8 / adverse))
                        :ARG1-of (e / effective-04))))
      :snt2 (c3 / clear-06
            :polarity -
            :ARG0 (o4 / organization
                  :name (n4 / name
                        :op1 "AIA"))
            :ARG1 (s2 / situation
                  :topic (m3 / meet-01
                        :polarity -
                        :ARG1 (t4 / thing
                              :ARG1-of (r5 / require-01
                                    :ARG0 o4
                                    :ARG1 ii4
                                    :mod (h3 / high-02))))))
      :time (f3 / final))


# ::id 40
# ::snt A number of requirements are however not included in the AIA, which we think is a missed opportunity: (i) human agency (ii) privacy (iii) diversity, non-discrimination and fairness, (iv) explainability and (v) environmental and social well being.
(c / contrast-01
      :ARG2 (ii / include-01
            :polarity -
            :ARG1 (t / thing
                  :ARG1-of (r / require-01)
                  :quant (n / number)
                  :example (a / and
                        :op1 (a2 / agency
                              :li "i"
                              :mod (h / human))
                        :op2 (p / privacy
                              :li "ii")
                        :op3 (d / diversity
                              :li "iii")
                        :op4 (d2 / discriminate-02
                              :polarity -)
                        :op5 (f / fair-01)
                        :op6 (p2 / possible-01
                              :li "iv"
                              :ARG1 (e / explain-01))
                        :op7 (a3 / and
                              :op1 (w / well-09
                                    :ARG1 (e2 / environment))
                              :op2 (w2 / well-09
                                    :ARG1 (s / society)))))
            :ARG2 t
            :name (n2 / name
                  :op1 "AIA"))
      :ARG1-of (m / miss-02
            :ARG0 t
            :ARG1-of (t2 / think-01
                  :ARG0 (w3 / we))))


# ::id 40
# ::snt ALLAI welcomes the alignment of the the requirements for high risk AI with elements of the Ethics Guidelines for Trustworthy AI (the “Ethics Guidelines”), however, five important requirements of the Ethics Guidelines are not specifically dealt with in the AIA namely: (i) human agency (ii) privacy (iii) diversity, non-discrimination and fairness, (iv) explainability and (v) environmental and social well being.
(c / contrast-01
      :ARG1 (w / welcome-01
            :ARG0 (o / organization
                  :name (n / name
                        :op1 "Alliance"
                        :op2 "of"
                        :op3 "Technology"
                        :op4 "Associations"))
            :ARG1 (a / align-01
                  :ARG1 (t / thing
                        :ARG1-of (r / require-01
                              :ARG1 (ii / intelligent-01
                                    :mod (a2 / artificial)
                                    :ARG1-of (r2 / risk-01
                                          :ARG1-of (h / high-02)))))
                  :ARG2 (e / element
                        :part-of (t2 / thing
                              :name (n2 / name
                                    :op1 "Ethics"
                                    :op2 "Guidelines"
                                    :op3 "for"
                                    :op4 "Trustworthy"
                                    :op5 "AI")))))
      :ARG2 (d / deal-01
            :polarity -
            :ARG0 (o2 / organization
                  :name (n3 / name
                        :op1 "AIA"))
            :ARG2 (t3 / thing
                  :quant 5
                  :ARG1-of (r3 / require-01)
                  :ARG1-of (ii2 / important-01)
                  :ARG1-of (m / mean-01
                        :ARG2 (a3 / and
                              :op1 (a4 / agency
                                    :mod (h2 / human)
                                    :li "i")
                              :op2 (p / privacy
                                    :li "ii")
                              :op3 (a5 / and
                                    :op1 (d2 / diversity)
                                    :op2 (d3 / discriminate-01
                                          :polarity -)
                                    :op3 (f / fairness)
                                    :op4 (e2 / explain-01
                                          :li "iv")
                                    :op5 (w2 / well-09
                                          :ARG1 (a6 / and
                                                :op1 (e3 / environment)
                                                :op2 (s / society)))))))
            :ARG1-of (s2 / specific-02)))


# ::id 45
# ::snt In addition, Singapore has released nonbinding guidance to help organizations  navigate data ethics and governance principles, such as transparency, fairness,  and explainability.
(a / and
      :op2 (r / release-01
            :ARG0 (c / country
                  :name (n / name
                        :op1 "Singapore"))
            :ARG1 (g / guide-01
                  :ARG1-of (b / binding-07
                        :polarity -)
                  :ARG0-of (h / help-01
                        :ARG1 (n2 / navigate-01
                              :ARG0 (o / organization)
                              :ARG1 (a2 / and
                                    :op1 (e / ethics
                                          :mod (d / data))
                                    :op2 (p / principle
                                          :topic (g2 / govern-01)
                                          :example (a3 / and
                                                :op1 (t / transparency)
                                                :op2 (f / fairness)
                                                :op3 (e2 / explain-01
                                                      :ARG1-of (p2 / possible-01))))))
                        :ARG2 o))))


# ::id 45
# ::snt An integral element of its AI governance is its Society  5.0, a conceptual vision document guiding actions in science, technology,  and innovation aimed at synergies for a prosperous future.79 The Society 5.0  framework frames Japan’s AI principles (human-centricity, education/literacy,  privacy, security, fair competition, fairness, accountability and transparency,  and innovation) mainly in relation to cultural and social aspects of its society.80  The Cabinet Office Council on Industrial Competitiveness81 has targeted selfdriving cars, drones, and production management, including smart factories,  all powered by AI, as key opportunities to increase Japan’s productivity.
(m / multi-sentence
      :snt2 (t / target-01
            :li 80
            :ARG0 (g / government-organization
                  :name (n / name
                        :op1 "Cabinet"
                        :op2 "Office"
                        :op3 "Council"
                        :op4 "on"
                        :op5 "Industrial"
                        :op6 "Competitive"
                        :op7 "Compitiveness"))
            :ARG1 (o / opportunity
                  :ARG1-of (k / key-02)
                  :purpose (ii / increase-01
                        :ARG0 g
                        :ARG1 (p / productive-03
                              :ARG0 (c / country
                                    :name (n2 / name
                                          :op1 "Japan"))))
                  :domain (a / and
                        :op1 (c2 / car
                              :ARG0-of (d / drive-01
                                    :ARG1 (s / self)))
                        :op2 (d2 / drone)
                        :op3 (m2 / manage-01
                              :ARG1 (p2 / produce-01)
                              :ARG2-of (ii2 / include-01
                                    :ARG1 (f / factory
                                          :ARG1-of (s2 / smart-06)
                                          :ARG1-of (p3 / power-01
                                                :ARG0 (a2 / artificial-03)))))))
            :ARG2 (s3 / synergize-01
                  :ARG1-of (a3 / aim-02
                        :ARG2 (f2 / future
                              :ARG0-of (p4 / prosper-01)))))
      :snt1 (f3 / frame-06
            :ARG0 (e / element
                  :mod (ii3 / integral)
                  :part-of (g2 / govern-01
                        :ARG0 (ii4 / it)
                        :ARG1 (a4 / artificial-03))
                  :domain (d3 / document
                        :name (n3 / name
                              :op1 "Society"
                              :op2 "5.0")
                        :mod (e2 / envision-01)))
            :ARG1 (p5 / principle
                  :mod (a5 / artificial-03)
                  :poss c
                  :ARG1-of (r / relate-01
                        :ARG2 (a6 / aspect
                              :mod (c3 / culture)
                              :mod (s4 / society))
                        :mod (m3 / main))
                  :example (a7 / and
                        :op1 (f4 / focus-01
                              :ARG2 (h / human))
                        :op2 (e3 / educate-01)
                        :op3 (l / literacy)
                        :op4 (s5 / security)
                        :op5 (c4 / compete-01
                              :ARG1-of (f5 / fair-01))
                        :op6 (a8 / and
                              :op1 (a9 / accountable-02)
                              :op2 (t2 / transparency))
                        :op7 (ii5 / innovate-01)))))


# ::id 45
# ::snt In addition, Canada has issued regulations to address certain risks related to  artificial intelligence and the processing of personal information in the federal  government; the Directive on Automated Decision-Making came into effect  April 19, 2019, requiring federal government bodies to complete algorithmic  impact assessments prior to utilizing automated decisionmaking tools, notify  affected parties both before and after automated decisions, and analyze  all results for potential bias.57 In the private sector, the federal Personal  Information Protection and Electronic Documents Act (PIPEDA) regulates  how businesses handle personal information, setting out ten fair information  principles that include safeguards to maintain privacy, accuracy, and fairness  in data processing and minimize potential harms or discrimination to 
STRENGTHENING INTERNATIONAL COOPERATION  ON AI29individuals.58 These regulations—together with Canada’s Digital Charter,  a government initiative to build public trust in emerging technologies— contribute to the government’s objectives to maximize the economic and  social benefits of AI while minimizing any potential pitfalls or risks.59 Canada has made working with the international community on collective  ways to harness the benefits of AI a feature of its AI strategy.
(m / multi-sentence
      :snt1 (a / and
            :li 58
            :op2 (r / regulate-01
                  :ARG0 (c / country
                        :name (n / name
                              :op1 "Canada"))
                  :ARG0-of (a2 / address-02
                        :ARG1 (r2 / risk-01
                              :ARG1-of (r3 / relate-01
                                    :ARG2 (a3 / and
                                          :op1 (ii / intelligent-01
                                                :mod (a4 / artificial))
                                          :op2 (p / process-01
                                                :ARG0 (b / business)
                                                :ARG1 (ii2 / information
                                                      :ARG1-of (p2 / personal-02)))))
                              :mod (c2 / certain)))))
      :snt2 (r4 / regulate-01
            :li 57
            :ARG0 (l / law
                  :name (n2 / name
                        :op1 "Directed"
                        :op2 "on"
                        :op3 "Automated"
                        :op4 "Decision-Making")
                  :ARG0-of (r5 / require-01
                        :ARG1 (a5 / and
                              :op1 (c3 / complete-01
                                    :ARG0 (b2 / body
                                          :mod (g / government-organization
                                                :ARG0-of (g2 / govern-01)
                                                :mod (f / federal)))
                                    :ARG1 (a6 / assess-01
                                          :ARG1 (ii3 / impact-01
                                                :mod (a7 / algorithm)))
                                    :time (p3 / prior
                                          :op1 (u / utilize-01
                                                :ARG0 b2
                                                :ARG1 (t / tool
                                                      :instrument-of (m2 / make-01
                                                            :ARG1 (d / decide-01))
                                                      :ARG1-of (a8 / automate-01)))))
                              :op2 (n3 / notify-01
                                    :ARG0 b2
                                    :ARG1 (p4 / party
                                          :ARG1-of (a9 / affect-01))
                                    :time (a10 / and
                                          :op1 (b3 / before
                                                :op1 d)
                                          :op2 (a11 / after
                                                :op1 d)))
                              :op3 (a12 / analyze-01
                                    :ARG0 b2
                                    :ARG1 (r6 / result-01
                                          :mod (a13 / all))
                                    :ARG2 (b4 / bias-01
                                          :mod (p5 / potential)))))
                  :ARG0-of (s / set-out-06
                        :ARG1 (p6 / principle
                              :quant 10
                              :topic (ii4 / information
                                    :ARG1-of (f2 / fair-01))
                              :ARG2-of (ii5 / include-01
                                    :ARG1 (a14 / and
                                          :op1 (m3 / maintain-01
                                                :ARG1 (a15 / and
                                                      :op1 (p7 / private-03)
                                                      :op2 (a16 / accurate))))
                                    :op2 (m4 / minimize-01
                                          :ARG1 (o / or
                                                :op1 (h / harm-01)
                                                :op2 (d2 / discriminate-02))))))))
      :ARG0-of (c4 / contribute-01
            :ARG2 (o2 / objective
                  :poss g
                  :topic (m5 / maximize-01
                        :ARG0 g
                        :ARG1 (b5 / benefit-01
                              :ARG0 ii
                              :ARG1 (a17 / and
                                    :op1 (e / economy)
                                    :op2 (s2 / society)))))))


# ::id 45
# ::snt Other common principles  include human oversight, explainability or interpretability, legal status of  AI systems, and the equitable economic effect of AI.31 A separate analysis  of 84 AI ethics documents done in 2019 found that there has been a global  convergence around “transparency, justice and fairness, non-maleficence,  responsibility and privacy.”32 While much progress has been made aligning on responsible AI, there remain  differences—even among FCAI participants.
(m / multi-sentence
      :li 31
      :snt1 (ii / include-01
            :ARG1 (a / and
                  :op1 (o / oversight
                        :mod (h / human))
                  :op2 (o2 / or
                        :op1 (p / possible-01
                              :ARG1 (e / explain-01))
                        :op2 (p2 / possible-01
                              :ARG1 (ii2 / interpret-01)))
                  :op3 (s / status
                        :ARG1-of (l / legal-02)
                        :poss (s2 / system
                              :mod (a2 / artificial)))
                  :op4 (a3 / affect-01
                        :ARG0 (ii3 / intelligent-01
                              :mod (a4 / artificial))
                        :ARG1 (e2 / economy)
                        :mod (e3 / equitable)))
            :ARG2 (p3 / principle
                  :ARG1-of (s3 / share-01)
                  :mod (o3 / other)))
      :snt2 (f / find-01
            :ARG0 (a5 / analyze-01
                  :ARG1 (d / document
                        :quant 84
                        :topic (e4 / ethics)
                        :mod (ii4 / intelligent-01
                              :mod (a6 / artificial)))
                  :ARG1-of (s4 / separate-02)
                  :time (d2 / date-entity
                        :year 2019))
            :ARG1 (c / converge-01
                  :ARG1 (a7 / and
                        :op1 (t / transparency)
                        :op2 (j / justice)
                        :op3 (f2 / fairness)
                        :op4 (m2 / malice
                              :polarity -)
                        :op5 (r / responsible-02)
                        :op6 (p4 / privacy))
                  :mod (g / globe)))
      :snt3 (c2 / contrast-01
            :ARG1 (p5 / progress-01
                  :ARG1 (a8 / align-01
                        :ARG2 ii4)
                  :quant (m3 / much))
            :ARG2 (r2 / remain-01
                  :ARG1 (d3 / differ-02
                        :ARG1 (p6 / person
                              :ARG0-of (p7 / participate-01
                                    :ARG1 (e5 / event
                                          :name (n / name
                                                :op1 "FCAI")))))
                  :mod (e6 / even))))


# ::id 45
# ::snt An analysis of 22 AI ethics principles found that the  values of accountability, privacy, fairness, transparency, and cybersecurity  appeared in over 70 percent of the documents.
(f / find-01
      :ARG0 (a / analyze-01
            :ARG1 (p / principle
                  :quant 22
                  :topic (e / ethics)
                  :mod (a2 / artificial)))
      :ARG1 (a3 / appear-01
            :ARG1 (v / value
                  :example (a4 / and
                        :op1 (a5 / accountable-02)
                        :op2 (p2 / privacy)
                        :op3 (f2 / fairness)
                        :op4 (t / transparency)
                        :op5 (c / cybersecurity)))
            :location (d / document
                  :quant (o / over
                        :op1 (p3 / percentage-entity
                              :value 70)))))


# ::id 45
# ::snt The Asilomar AI  Principles, developed in 2017, were signed by nearly 6,000 AI experts and  adopted as informal guiding principles by the state of California.28 The IEEE’s  Ethically Aligned Design is a comprehensive exploration of AI developed 
STRENGTHENING INTERNATIONAL COOPERATION  ON AI21over a three-year period, also involving several thousand experts.29 AI Now  was an early civil society mover in propounding recommendations for  government policies.30 There is considerable overlap among these various sets of principles, including  on the importance of fairness, privacy preservation, and respect for human  rights and autonomy.
(m / multi-sentence
      :snt1 (a / and
            :li 28
            :op1 (s / sign-01
                  :ARG0 (p / person
                        :ARG1-of (e / expert-01
                              :ARG2 (ii / intelligent-01
                                    :mod (a2 / artificial)))
                        :quant (n / nearly
                              :op1 6000))
                  :ARG1 (p2 / principle
                        :name (n2 / name
                              :op1 "Asilomar"
                              :op2 "AI"
                              :op3 "Principles")
                        :ARG1-of (d / develop-02
                              :time (d2 / date-entity
                                    :year 2017))))
            :op2 (a3 / adopt-01
                  :ARG0 (s2 / state
                        :name (n3 / name
                              :op1 "California"))
                  :ARG1 p2
                  :ARG3 (p3 / principle
                        :ARG0-of (g / guide-01)
                        :mod (f / formal
                              :polarity -))))
      :snt2 (o / overlap-01
            :li 30
            :ARG0 (s3 / set
                  :consist-of (p4 / principle)
                  :mod (v / various)
                  :ARG2-of (ii2 / include-01
                        :ARG1 (a4 / and
                              :op1 (ii3 / important-01
                                    :ARG1 (f2 / fairness))
                              :op2 (p5 / preserve-01
                                    :ARG1 (p6 / privacy))
                              :op3 (r / respect-01
                                    :ARG1 (a5 / and
                                          :op1 (r2 / right-05
                                                :ARG1 (h / human))
                                          :op2 (a6 / autonomy
                                                :poss h))))))
            :degree (c / considerable))
      :snt3 (e2 / explore-01
            :ARG0 (p7 / publication
                  :name (n4 / name
                        :op1 "IAI's"
                        :op2 "Ethical"
                        :op3 "Aligned"
                        :op4 "Design"))
            :ARG1 (ii4 / intelligent-01
                  :mod (a7 / artificial)
                  :ARG1-of (d3 / develop-02)
                  :ARG0-of (s4 / strengthen-01
                        :ARG1 (c2 / cooperate-01
                              :ARG2 ii4
                              :mod (ii5 / international))
                        :duration (t / temporal-quantity
                              :quant 3
                              :unit (y / year))
                        :ARG2-of (ii6 / involve-01
                              :ARG1 (p8 / person
                                    :ARG1-of (e3 / expert-01)
                                    :quant (s5 / several
                                          :op1 1000))
                              :mod (a8 / also)))))
      :snt4 (m2 / move-02
            :ARG0 (p9 / publication
                  :name (n5 / name
                        :op1 "AI"
                        :op2 "Now"))
            :ARG1 (p10 / propound-01
                  :ARG1 (r3 / recommend-01
                        :ARG1 (p11 / policy-01
                              :ARG0 (g2 / government-organization
                                    :ARG0-of (g3 / govern-01)))))
            :time (e4 / early)
            :mod (s6 / society
                  :mod (c3 / civil))))


# ::id 45
# ::snt In sectors like finance, key  criteria such as fairness, discrimination, and transparency have long been  subject to extensive regulatory intervention, and sectoral regulation  must ensure continuity while accounting for the increasing use of AI.
(a / and
      :op1 (s / subject-01
            :ARG1 (c / criteria
                  :ARG1-of (k / key-02)
                  :example (a2 / and
                        :op1 (f / fairness)
                        :op2 (d / discriminate-02)
                        :op3 (t / transparency)))
            :ARG2 (ii / intervene-01
                  :ARG0 (r / regulate-01)
                  :ARG1-of (e / extensive-03))
            :ARG1-of (l / long-03)
            :location (s2 / sector
                  :example (f2 / finance)))
      :op2 (o / obligate-01
            :ARG1 (r2 / regulate-01
                  :mod (s3 / sector))
            :ARG2 (e2 / ensure-01
                  :ARG0 r2
                  :ARG1 (c2 / continue-01)
                  :time (a3 / account-01
                        :ARG0 r2
                        :ARG1 (u / use-01
                              :ARG1 (ii2 / intelligent-01
                                    :mod (a4 / artificial))
                              :ARG1-of (ii3 / increase-01))))))


# ::id 45
# ::snt “AI Fairness 360,” IBM Research Trusted AI, accessed August 27, 2021, https://aif360.mybluemix.net/; “AI Explainability 360,” IBM Research  Trusted AI, accessed August 27, 2021, https://aix360.mybluemix.net/.
(a / and
      :op1 (p / publication
            :name (n / name
                  :op1 "AI"
                  :op2 "Fairness"
                  :op3 360)
            :medium (p2 / publication
                  :name (n2 / name
                        :op1 "IBM"
                        :op2 "Research"
                        :op3 "Trusted"
                        :op4 "AI"))
            :ARG1-of (a2 / access-01
                  :time (d / date-entity
                        :day 27
                        :month 8
                        :year 2021))
            :mod (u / url-entity
                  :value "https://aif360.mybluemix.net/"))
      :op2 (p3 / publication
            :name (n3 / name
                  :op1 "AI"
                  :op2 "Explainedability"
                  :op3 360)
            :medium p2
            :ARG1-of (a3 / access-01
                  :time d)))


# ::id 45
# ::snt Inioluwa Debora Raji, et al., “Closing the AI accountability gap: Defining an end-to-end framework for internal algorithmic auditing,”  Conference on Fairness, Accountability, and Transparency (FAT* ’20), January 27-30, 2020, https://arxiv.org/pdf/2001.00973.pdf.
(p / publication-91
      :ARG0 (a / and
            :op1 (p2 / person
                  :name (n / name
                        :op1 "Inioluwa"
                        :op2 "Debora"
                        :op3 "Raji"))
            :op2 (p3 / person
                  :mod (o / other)))
      :ARG1 (p4 / publication
            :name (n2 / name
                  :op1 "Closing"
                  :op2 "the"
                  :op3 "Accountability"
                  :op4 "Gap")
            :ARG1-of (m / mean-01
                  :ARG2 (d / define-01
                        :ARG1 (f / framework
                              :mod (e / end-to-end)
                              :purpose (a2 / audit-01
                                    :mod (a3 / algorithm)
                                    :ARG1-of (ii / internal-02))))))
      :ARG4 (c / conference
            :name (n3 / name
                  :op1 "Conference"
                  :op2 "on"
                  :op3 "Fairness,"
                  :op4 "Accountability"
                  :op5 "and"
                  :op6 "Transparency")
            :time (d2 / date-interval
                  :op1 (d3 / date-entity
                        :month 1
                        :day 27
                        :year 2020)
                  :op2 (d4 / date-entity
                        :month 1
                        :day 30
                        :year 2020)))
      :ARG4 (u / url-entity
            :value "https://arxiv.org/pdf/2001.00973.pdf"))


# ::id 45
# ::snt AI activities in the 7 administrations participating in the FCAI  AI ethics Frameworks Existing AI  regulation AI standards Public  Investment Australia Australia’s AI Ethics Framework Review of existing  regulations per  the AI Action Plan Standards Australia focuses on  by-design and standards testing;  AI Standards Roadmap AUD 124.1 million  (USD 90.9 million)  2021-2022  Canada CIFAR Pan-Canadian AI  Strategy 2017; Digital  Charter 2017/2021; Montreal  Declaration for Responsible  Development of AI Directive on  Automated  Decision Making;  Algorithmic  Impact  Assessment CIO Strategy Council develops  AI Standards and is accredited  by Standards Council of Canada,  focusing on ethical design and  ADM audits; $8.6 million over  five years, starting in 2021–22,  to advance the development and  adoption of AI standards CAD 125 million  (USD 100 million)  2017-2022  EU Ethics Guidelines for  Trustworthy AI; White Paper on  AI; Proposal for a regulation on  AI; National ethics guidelines Coordinated Plan  on AI; Proposal  for a regulation on  AI; Digital Decade  package CEN-CENELC Joint Technical  Committee 21 ‘Artificial  Intelligence’; national standards  focus on EU interoperability,  ethics, fundamental rights, and  safety EUR 20 billion  (USD 23.3 billion)  per year until 2030,  national funding Japan R&D Guidelines 2018; Social  Principles of Human-Centric AI  2019; AI Utilization Guidelines  2019; Society 5.0 framework Draft AI Utilization  Principles  Guidelines 2019;  AI Technology  Strategy 2017 Ministry of Economy, Trade  and Industry (METI), Japanese  Industrial Standards Committee  and Information Technology  Standards Commission focus  on developing sector-specific  standards in transportation,  safety, and patents Yen 77 billion  (USD 70 billion)  2018  Singapore Model AI Governance  Framework, 2nd Edition,  2020; Implementation and  Self-Assessment Guide for  Organizations; Principles  to Promote Fairness,  Ethics, Accountability and  Transparency National AI  Strategy Voluntary Horizontal Model  Framework contributes to global  standards for AI-related policies  and guidelines Up to SG$150  million  (USD 110.8  million)  2017-2022   U.K.
(m / multi-sentence
      :snt1 (a / and
            :op1 (f / focus-01
                  :ARG0 (g / government-organization
                        :quant 7
                        :ARG0-of (a2 / administrate-01)
                        :ARG0-of (p / participate-01
                              :ARG1 (c / conference
                                    :name (n / name
                                          :op1 "FCAI"))))
                  :ARG1 (a3 / and
                        :op1 (s / standard
                              :mod (e / ethics)
                              :mod (a4 / artificial))
                        :op2 (s2 / standard
                              :mod (e2 / ethics)
                              :ARG1-of (e3 / exist-01))))
            :op2 (p2 / publication
                  :name (n2 / name
                        :op1 "Public"
                        :op2 "Investment"
                        :op3 "Board")
                  :mod (c2 / country
                        :name (n3 / name
                              :op1 "Australia")))
            :op3 (r / review-01
                  :ARG0 (g2 / government-organization
                        :name (n4 / name
                              :op1 "Standards"
                              :op2 "Council"
                              :op3 "of"
                              :op4 "Canada"))
                  :ARG1 (r2 / regulate-01
                        :ARG1 (ii / intelligent-01
                              :mod (a5 / artificial)))
                  :time (d / date-entity
                        :year 2017))
            :op4 (p3 / publication
                  :name (n5 / name
                        :op1 "Model"
                        :op2 "AI"
                        :op3 "Governance"
                        :op4 "Framework")
                  :time (d2 / date-entity
                        :year 2020)
                  :mod (c3 / country
                        :name (n6 / name
                              :op1 "Singapore")))
            :op5 (p4 / publication
                  :name (n7 / name
                        :op1 "Social"
                        :op2 "Principles"
                        :op3 "of"
                        :op4 "Human-Centric"
                        :op5 "AI")
                  :time (d3 / date-entity
                        :year 2019))
            :op6 (p5 / publication
                  :name (n8 / name
                        :op1 "Draft"
                        :op2 "AI"
                        :op3 "Utilization"
                        :op4 "Principles")
                  :time (d4 / date-entity
                        :year 2019))
            :op7 (p6 / publication
                  :name (n9 / name
                        :op1 "CEN-CENELC"
                        :op2 "Joint"
                        :op3 "Technical"
                        :op4 "Committee")
                  :time (d5 / date-entity
                        :year 2017))
            :op8 (p7 / publication
                  :name (n10 / name
                        :op1 "Public"
                        :op2 "Accountability"
                        :op3 "and"
                        :op4 "Transparency"
                        :op5 "Guide"
                        :op6 "for"
                        :op7 "Organization"))
            :op9 (p8 / publication
                  :name (n11 / name
                        :op1 "Information"
                        :op2 "Technology"
                        :op3 "Standards"
                        :op4 "Commission")
                  :time (d6 / date-entity
                        :year 2017))
            :op10 (p9 / publication
                  :name (n12 / name
                        :op1 "Yuan"
                        :op2 "Billion")
                  :ARG1-of (r3 / rate-entity-91
                        :ARG2 (t / temporal-quantity
                              :quant 1
                              :unit (y / year))))
            :op11 (m2 / monetary-quantity
                  :quant 125000000
                  :unit (d7 / dollar)))
      :op12 (m3 / monetary-quantity
            :quant 100000000
            :unit (d8 / dollar))
      :op13 (m4 / monetary-quantity
            :quant 1550000000
            :unit (d9 / dollar))
      :purpose (a6 / advance-01
            :ARG1 (a7 / and
                  :op1 (d10 / develop-02
                        :ARG1 s)
                  :op2 (a8 / adopt-01
                        :ARG1 s))))


# ::id 45
# ::snt AI policies and investment by FCAI participants   | ⮌  contents AI ethical  framework307Existing AI  regulation308Data governance309 AI Standards310Computing power Privacy IP Cyber SI315Model AI Governance  Framework, 2nd Edition,  2020;  Implementation and SelfAssessment Guide for  Organisations (ISAGO);  Principles to Promote  Fairness, Ethics,  Accountability and  Transparency (FEAT) National AI Strategy Personal Data  Protection Act 2012  (PDPA) (amended in  2020);  Trusted Data  Sharing Framework  (voluntary)Patents Act;  Copyright Act;  AI2 Scheme  for fast-track  examinationCybersecurity  Act 2018;  Computer  Misuse ActSpring SG: Voluntary  Horizontal Model  Framework also  contributes to global  standards for AI-related  policies and guidelinesNSCC cooperates with  Japan’s RIKEN and RIST  to access Fugaku;  National Research  Foundation builds second  national supercomputer  system (SG$200 million) U.K.316Guidance on Ethics,  Transparency  Accountability for ADMNational AI Strategy Data Protection Act  2018 (U.K. GDPR);  U.K. eIDAS RegulationCopyright, Designs  and Patents Act  1988Cyber Security  Information  Sharing  Partnership, UK  GDPRBritish Standard Institute  focuses on international  cooperation and  healthcare standardsGBP 20 million funding  for DiRAC (academic);  High Performance  Computing facility US317Principles in Executive  Order 13859 and  Executive Order 13960;  Agency specific  frameworks, statespecific guidelines  Government  agencies assessing  where AI regulation  is needed, where  existing regulation  applies, and roles  for self assessment,  codes, etc.
(m / multi-sentence
      :snt1 (a / and
            :op1 (a2 / and
                  :op1 (p / policy-01
                        :ARG0 (o / organization
                              :ARG0-of (p2 / participate-01
                                    :ARG1 (e / event
                                          :name (n / name
                                                :op1 "FCAI")))))
                  :op2 (ii / invest-01
                        :ARG0 o))
            :op2 (f / focus-01
                  :ARG0 o
                  :ARG2 (a3 / and
                        :op1 (c / cooperate-01
                              :mod (ii2 / international))
                        :op2 (s / standard
                              :mod (h / healthcare)))))
      :snt2 (a4 / and
            :op1 (r / regulate-01
                  :ARG1 (ii3 / intelligent-01
                        :mod (a5 / artificial))
                  :ARG1-of (e2 / exist-01)
                  :ARG1-of (c2 / cite-01
                        :ARG2 308))
            :op2 (r2 / regulate-01
                  :ARG1 (p3 / power
                        :mod (c3 / computer))
                  :ARG1-of (m2 / mean-01
                        :ARG2 (l / law
                              :name (n2 / name
                                    :op1 "Cybersecurity"
                                    :op2 "Act")
                              :time (d / date-entity
                                    :year 2018))))
            :op3 (o2 / organization
                  :name (n3 / name
                        :op1 "European"
                        :op2 "Digital"
                        :op3 "Standard"
                        :op4 "Institute"))
            :op4 (f2 / fund-01
                  :ARG1 (f3 / facility
                        :name (n4 / name
                              :op1 "DiRAC")
                        :mod (a6 / academia))
                  :ARG3 (m3 / monetary-quantity
                        :quant 200000000
                        :unit (p4 / pound)))
            :op5 (a7 / and
                  :op1 (p5 / publication
                        :name (n5 / name
                              :op1 "Executive"
                              :op2 "Order"
                              :op3 "13859"))
                  :op2 (p6 / publication
                        :name (n6 / name
                              :op1 "Executive"
                              :op2 "Order"
                              :op3 "13960"))
                  :op6 (f4 / framework
                        :ARG1-of (s2 / specific-02
                              :ARG2 (a8 / agency
                                    :mod (g / government-organization
                                          :ARG0-of (g2 / govern-01
                                                :ARG1 (c4 / country
                                                      :name (n7 / name
                                                            :op1 "U.K."))))))))
            :op7 (g3 / guide
                  :name (n8 / name
                        :op1 "ISAGO")))
      :op8 (p7 / principle
            :ARG0-of (p8 / promote-02
                  :ARG1 (a9 / and
                        :op1 (f5 / fairness)
                        :op2 (e3 / ethics
                              :op3 (a10 / accountable-02)
                              :op4 (a11 / accountable-02))))
            :ARG1-of (m4 / mean-01
                  :ARG2 (l2 / law
                        :name (n9 / name
                              :op1 "FEAT"))))
      :snt3 (a12 / and
            :op1 (l3 / law
                  :name (n10 / name
                        :op1 "Cyber"
                        :op2 "Security"
                        :op3 "Information"
                        :op4 "Sharing"
                        :op5 "Agreement"))
            :op2 (l4 / law
                  :name (n11 / name
                        :op1 "Computer"
                        :op2 "Misuse"
                        :op3 "Act"))
            :time (d2 / date-entity
                  :year 1988)))


# ::id 45
# ::snt Some examples  are discussed below:196 • In sectors like finance, key criteria such as fairness, discrimination, and  transparency have been subject to extensive regulatory intervention  in the past, and sectoral regulation must ensure continuity while at  the same time accounting for the increasing use of AI.
(d / discuss-01
      :li 196
      :ARG1 (e / example
            :quant (s / some))
      :location (b / below)
      :example-of (a / and
            :op1 (s2 / subject-01
                  :ARG1 (c / criteria
                        :ARG1-of (k / key-02)
                        :example (a2 / and
                              :op1 (f / fairness)
                              :op2 (d2 / discriminate-02)
                              :op3 (t / transparency)))
                  :ARG2 (ii / intervene-01
                        :ARG0 (r / regulate-01)
                        :ARG1-of (e2 / extensive-03))
                  :time (p / past)
                  :location (s3 / sector
                        :example (f2 / finance)))
            :op2 (o / obligate-01
                  :ARG1 (r2 / regulate-01
                        :mod (s4 / sector))
                  :ARG2 (e3 / ensure-01
                        :ARG0 r2
                        :ARG1 (c2 / continue-01)
                        :time (t2 / time
                              :ARG1-of (s5 / same-01
                                    :ARG2 (a3 / account-01
                                          :ARG1 (u / use-01
                                                :ARG1 (ii2 / intelligent-01
                                                      :mod (a4 / artificial))
                                                :ARG1-of (ii3 / increase-01)))))))))


# ::id 45
# ::snt The key themes of the Guidance are accountability and governance; data protection; lawfulness, fairness, and transparency;  security and data minimisation; and ensuring that individuals can effectively exercise their rights relating to their data.
(t / theme
      :ARG1-of (k / key-02)
      :poss (g / government-organization
            :name (n / name
                  :op1 "Guarding"))
      :domain (a / and
            :op1 (a2 / and
                  :op1 (a3 / accountable-02)
                  :op2 (g2 / govern-01))
            :op2 (p / protect-01
                  :ARG1 (d / data))
            :op3 (l / lawfulness)
            :op4 (f / fairness)
            :op5 (t2 / transparency)
            :op6 (a4 / and
                  :op1 (s / security)
                  :op2 (m / minimize-01
                        :ARG1 (d2 / data)))
            :op7 (e / ensure-01
                  :ARG1 (p2 / possible-01
                        :ARG1 (e2 / exercise-01
                              :ARG0 (ii / individual)
                              :ARG1 (r / right-05
                                    :ARG1 ii
                                    :ARG1-of (r2 / relate-01
                                          :ARG2 d2))
                              :ARG1-of (e3 / effective-04))))))


# ::id 45
# ::snt It covers best practices for compliance with  data protection laws in development and deployment of AI systems  and focuses on accountability and governance; data protection impact  assessment; lawfulness, fairness, and transparency; security and data  minimization; and individual rights in AI systems.184 • The Partnership on AI hosted important discussions for the development  of an end-to-end approach to internal algorithmic auditing, including an  analysis of how to learn across industries.185
STRENGTHENING INTERNATIONAL COOPERATION  ON AI55• At the sectoral level, work on algorithmic auditing is intensifying  with several sector-specific frameworks being developed in finance,  health care, and intelligence.
(m / multi-sentence
      :li 185
      :snt1 (a / and
            :op1 (c / cover-01
                  :ARG0 (ii / it)
                  :ARG1 (a2 / and
                        :op1 (p / practice-01
                              :ARG1 (c2 / comply-01
                                    :ARG1 (l / law
                                          :topic (p2 / protect-01
                                                :ARG1 (d / data)))))
                        :op2 (d2 / deploy-01
                              :ARG1 (s / system
                                    :mod (a3 / artificial)))))
            :op2 (f / focus-01
                  :ARG0 ii
                  :ARG2 (a4 / and
                        :op1 (a5 / and
                              :op1 (a6 / accountable-02)
                              :op2 (g / govern-01))
                        :op2 (a7 / assess-01
                              :ARG1 (ii2 / impact-01
                                    :ARG0 (l2 / law
                                          :topic (p3 / protect-01
                                                :ARG1 (d3 / data)))))
                        :op3 (f2 / fairness)
                        :op4 (t / transparency)
                        :op5 (a8 / and
                              :op1 (s2 / security)
                              :op2 (m2 / minimize-01
                                    :ARG1 (d4 / data)))
                        :op6 (r / right-05
                              :ARG1 (ii3 / individual)
                              :location (s3 / system
                                    :mod (a9 / artificial))))))
      :snt2 (ii4 / intensify-01
            :ARG1 (w / work-01
                  :ARG1 (a10 / audit-01
                        :mod (a11 / algorithm))
                  :ARG1-of (ii5 / internal-02))
            :ARG1-of (c3 / cause-01
                  :ARG0 (d5 / develop-02
                        :ARG1 (f3 / framework
                              :ARG1-of (s4 / specific-02
                                    :ARG2 (s5 / sector))
                              :quant (s6 / several))))
            :location (l3 / level
                  :mod (s7 / sector)))
      :snt3 (h / host-01
            :ARG0 (o / organization
                  :name (n / name
                        :op1 "Partnership"
                        :op2 "on"
                        :op3 "AI"))
            :ARG1 (d6 / discuss-01
                  :ARG1 (d7 / develop-02
                        :ARG1 (a12 / approach-02
                              :ARG1 (a13 / audit-01
                                    :mod (e / end-to-end)))
                        :ARG2-of (ii6 / include-01
                              :ARG1 (a14 / analyze-01
                                    :ARG1 (t2 / thing
                                          :manner-of (l4 / learn-01
                                                :location (a15 / across
                                                      :op1 (ii7 / industry))))))))
            :ARG1-of (ii8 / important-01)))


# ::id 45
# ::snt It names the Alan Turing Institute as  the national AI research center to “remain a globally leading player in AI,”  “promote the U.K.’s interests through collaborations with international  partners,” and “attract world-leading talent to the U.K..”98Singapore has  released nonbinding  guidance to help  organizations  navigate data ethics  and governance  principles, such  as transparency,  fairness, and  explainability.
(a / and
      :li 98
      :op1 (n / name-01
            :ARG0 (ii / it)
            :ARG1 (r / research-institute
                  :name (n2 / name
                        :op1 "Alan"
                        :op2 "Turing"
                        :op3 "Institute"))
            :ARG2 (c / center
                  :mod (n3 / nation)
                  :purpose (a2 / and
                        :op1 (r2 / remain-01
                              :ARG1 (c2 / country
                                    :name (n4 / name
                                          :op1 "U.K."))
                              :ARG3 (p / play-02
                                    :ARG0 c2
                                    :ARG1 (ii2 / intelligent-01
                                          :mod (a3 / artificial))
                                    :mod (g / globe)))
                        :op2 (p2 / promote-02
                              :ARG0 c2
                              :ARG1 (ii3 / interest-01
                                    :ARG1 c2)
                              :manner (c3 / collaborate-01
                                    :ARG0 c2
                                    :ARG1 (c4 / country
                                          :ARG0-of (p3 / partner-01
                                                :ARG1 c2)
                                          :mod (ii4 / international))))
                        :op3 (a4 / attract-01
                              :ARG0 c2
                              :ARG1 (t / talent
                                    :ARG0-of (l / lead-02
                                          :ARG1 (w / world)))
                              :ARG2 c2))))
      :op2 (r3 / release-01
            :ARG0 (c5 / country
                  :name (n5 / name
                        :op1 "Singapore"))
            :ARG1 (g2 / guide-01
                  :ARG1 c5
                  :ARG2 (h / help-01
                        :ARG0 c5
                        :ARG1 (n6 / navigate-01
                              :ARG0 (o / organization)
                              :ARG1 (p4 / principle
                                    :example (a5 / and
                                          :op1 (e / ethics
                                                :mod (d / data))
                                          :op2 (g3 / govern-01))))
                        :ARG2 o)
                  :ARG1-of (b / binding-07
                        :polarity -))))


# ::id 45
# ::snt In addition to identifying a  need for review of regulations, the report also recognized a need to address  the threat of unintended consequences by ensuring an ethical governance  approach that emphasizes fairness and safety.
(a / and
      :op1 (r / recognize-01
            :ARG0 (r2 / report)
            :ARG1 (n / need-01
                  :ARG1 (a2 / address-02
                        :ARG1 (t / threaten-01
                              :ARG0 (c / consequence-03
                                    :ARG1-of (ii / intend-01
                                          :polarity -)))
                        :ARG2 (e / ensure-01
                              :ARG1 (a3 / approach-02
                                    :ARG1 (g / govern-01
                                          :manner (e2 / ethics))
                                    :ARG0-of (e3 / emphasize-01
                                          :ARG1 (a4 / and
                                                :op1 (f / fairness)
                                                :op2 (s / safe-01)))))))
            :mod (a5 / also))
      :op2 (ii2 / identify-01
            :ARG0 r2
            :ARG1 (n2 / need-01
                  :ARG1 (r3 / review-01
                        :ARG1 (r4 / regulate-01)))))


# ::id 45
# ::snt Because of the importance to these governments  and others of fairness, due process, nondiscrimination, and humancentered AI, they face a heightened need to practice what they preach by  ensuring the AI that they deploy is ethical and trustworthy.
(c / cause-01
      :ARG0 (ii / important-01
            :ARG1 (a / and
                  :op1 (f / fairness)
                  :op2 (d / due-process)
                  :op3 (d2 / discriminate-02
                        :polarity -)
                  :op4 (ii2 / intelligent-01
                        :mod (a2 / artificial)
                        :ARG1-of (c2 / center-01
                              :ARG2 (h / human))))
            :ARG2 (a3 / and
                  :op1 (g / government-organization
                        :ARG0-of (g2 / govern-01)
                        :mod (t / this))
                  :op2 (o / other)))
      :ARG1 (f2 / face-01
            :ARG0 (t2 / they)
            :ARG1 (n / need-01
                  :ARG1 (p / practice-01
                        :ARG0 t2
                        :ARG1 (t3 / thing
                              :ARG1-of (p2 / preach-01
                                    :ARG0 t2))
                        :manner (e / ensure-01
                              :ARG0 t2
                              :ARG1 (a4 / and
                                    :op1 (e2 / ethics
                                          :domain (ii3 / intelligent-01
                                                :mod a2))
                                    :op2 (t4 / trustworthy
                                          :domain ii3))))
                  :ARG1-of (h2 / heighten-01))))


# ::id 45
# ::snt STRENGTHENING INTERNATIONAL COOPERATION  ON AI45Table 2. International frameworks for the development of responsible AI Values Definitions EU Australia Japan Singapore OECD Human  centeredAI systems should be designed to be  inclusive, accommodating the needs of the  individuals that interact with it, and used in  a manner that is aligned with the values of  the community in which it is deployed.✔ ✔ ✔ ✔ ✔ Mitigate risks  and promote  benefitsAI systems should be designed and  deployed for the benefit of end users and  avoid unintended negative impacts on third  parties.✔ ✔ ✔ ✔ ✔ Fairness Governance and technical safeguards are  important to identify and mitigate risks of  unfair biases, particularly in circumstances  where an AI system could have a  consequential impact on people.✔ ✔ ✔ ✔ ✔ Explainability AI systems should be understandable;  context will dictate the appropriate  mechanisms for providing transparency  about a particular system’s decisionmaking  processes.✔ ✔ ✔ ✔ ✔ Privacy and  securityAI systems should be secure and enable  users to make informed choices regarding  use of personal information.✔ ◑ ✔ ○ ✔ Safety and  reliabilityAI systems should be designed to mitigate  foreseeable safety risks and adequately  tested to ensure that they operate as  intended.✔ ✔ ◑ ✔ ✔ Accountability A lifecycle approach to AI accountability,  including appropriate governance  structures for the design phase and redress  mechanisms following deployment is  important.✔ ✔ ◑ ✔ ✔ Riskbased and  proportionateRisks are context-specific and encourage  stakeholders to deploy risk management  techniques that are tailored to specific use  cases.✔ ✔ ○ ✔ ✔ Multiple  stakeholdersMultiple stakeholders have important roles  to play in mitigating risks involved in the  development, deployment, and use of AI.◑ ◑ ✔ ✔ ✔ Promotes  innovationGovernment is a key enabler of AI  innovation, and promotes a policy  environment that is conducive to crossborder data flows, value-added data  services, access to non-sensitive  government data, R&D, and workforce  development initiatives.○ ◑ ✔ ○ ✔ ✔  Satisfactory       ◑ Partial       ○ Unaddressed Source: BSA/The Software Alliance153
3.
(m / multi-sentence
      :snt1 (a / and
            :op1 (s / strengthen-01
                  :ARG1 (c / cooperate-01
                        :mod (ii / international)
                        :topic (p / product
                              :name (n / name
                                    :op1 "AI45"))))
            :op2 (d / dictate-01
                  :ARG0 (c2 / context)
                  :ARG1 (a2 / and
                        :op1 (a3 / approach-02
                              :ARG1 (a4 / accountable-02
                                    :ARG1 p)
                              :mod (l / lifecycle)
                              :ARG2-of (ii2 / include-01
                                    :ARG1 (a5 / and
                                          :op1 (p2 / phase
                                                :mod (d2 / design-01))
                                          :op2 (m2 / mechanism
                                                :ARG0-of (r / redress-01
                                                      :ARG1 p2))))))))
      :snt2 (a6 / and
            :op1 (r2 / recommend-01
                  :ARG1 (a7 / and
                        :op1 (s2 / secure-02
                              :ARG1 (s3 / system
                                    :mod (ii3 / intelligent-01
                                          :mod (a8 / artificial))))
                        :op2 (e / enable-01
                              :ARG0 s3
                              :ARG1 (c3 / choose-01
                                    :ARG0 (p3 / person
                                          :ARG0-of (u / use-01))
                                    :ARG1 (u2 / use-01
                                          :ARG1 (ii4 / information
                                                :ARG1-of (p4 / personal-02))))
                              :ARG2 p3)))
            :op2 (r3 / recommend-01
                  :ARG1 (a9 / and
                        :op1 (d3 / design-01
                              :ARG1 s3
                              :ARG3 (a10 / and
                                    :op1 (ii5 / inclusive)
                                    :op2 (a11 / accommodate-01
                                          :ARG0 s3
                                          :ARG1 (n2 / need-01
                                                :ARG0 (p5 / person
                                                      :ARG0-of (ii6 / interact-01
                                                            :ARG1 s3)))))
                              :op2 (u3 / use-01
                                    :ARG1 s3
                                    :manner (a12 / align-01
                                          :ARG1 s3
                                          :ARG2 (v / value
                                                :poss (c4 / community
                                                      :location-of (d4 / deploy-01
                                                            :ARG1 s3))))))
                        :op3 (t / test-01
                              :ARG1 s3
                              :ARG2 (e2 / ensure-01
                                    :ARG1 (o / operate-01
                                          :ARG1 s3
                                          :ARG1-of (ii7 / intend-01)))))))
      :snt3 (a13 / and
            :op1 (e3 / enforce-01
                  :ARG0 (g / government-organization
                        :ARG0-of (g2 / govern-01)))
            :op2 (e4 / enforce-01
                  :ARG0 (g3 / government-organization
                        :ARG0-of (g4 / govern-01))))
      :op3 (p6 / promote-02
            :ARG0 g
            :ARG1 (ii8 / innovate-01))
      :snt4 (a14 / and
            :op1 (e5 / enforce-01
                  :ARG0 (g5 / government-organization
                        :name (n3 / name
                              :op1 "BSA")))
            :op2 (o2 / organization
                  :name (n4 / name
                        :op1 "The"
                        :op2 "Software"
                        :op3 "Alliance")))
      :snt5 (ii9 / important-01))


# ::id 45
# ::snt “Model artificial intelligence governance framework: second edition,” Personal Data Protection Commission Singapore, January 2020,  https://www.pdpc.gov.sg/help-and-resources/2020/01/second-edition-of-model-artificial-intelligence-governance-framework ; “Singapore’s  approach to AI governance,” Personal Data Protection Commissioner Singapore, accessed September 1, 2021, https://www.pdpc.gov.sg/ Help-and-Resources/2020/01/Model-AI-Governance-Framework; “Principles to promote fairness, ethics, accountability, and transparency  (FEAT) in the use of artificial intelligence and data analytics in Singapore’s financial sector,” Monetary Authority of Singapore, http://www.
(a / and
      :op1 (p / publication
            :name (n / name
                  :op1 "Model"
                  :op2 "Artificial"
                  :op3 "Intelligence"
                  :op4 "Governance"
                  :op5 "Framework")
            :mod (e / edition
                  :ord (o / ordinal-entity
                        :value 2))
            :medium (p2 / publication
                  :name (n2 / name
                        :op1 "Personal"
                        :op2 "Data"
                        :op3 "Protection"
                        :op4 "Commission"
                        :op5 "Singapore"))
            :time (d / date-entity
                  :month 1
                  :year 2020))
      :op2 (p3 / publication
            :name (n3 / name
                  :op1 "Monetary"
                  :op2 "Authority"
                  :op3 "of"
                  :op4 "Singapore")
            :ARG1-of (a2 / access-01
                  :time (d2 / date-entity
                        :month 9
                        :day 1
                        :year 2021))
            :medium (u / url-entity
                  :value "https://www.pdpc.gov.sg/help-and-resources/2020/01/Model-AI-Governance-Framework"))
      :op3 (p4 / publication
            :name (n4 / name
                  :op1 "FEAT")
            :ARG0-of (p5 / promote-02
                  :ARG1 (a3 / and
                        :op1 (f / fairness)
                        :op2 (e2 / ethics)
                        :op3 (a4 / accountable-02)
                        :op4 (t / transparency))
                  :topic (u2 / use-01
                        :ARG1 (a5 / and
                              :op1 (ii / intelligent-01
                                    :mod (a6 / artificial))
                              :op2 (a7 / analyze-01
                                    :ARG1 (d3 / data)))
                        :ARG2 (s / sector
                              :mod (f2 / finance)
                              :location (c / country
                                    :name (n5 / name
                                          :op1 "Singapore")))))))


# ::id 45
# ::snt pdf; “Launch of AI: Accelerated initiative for artificial intelligence—an accelerated application-to-grant service for patent applications in  artificial intelligence,” Intellectual Property Office of Singapore, April 26, 2019, https://www.ipos.gov.sg/docs/default-source/resourceslibrary/patents/circulars/(2019)-circular-no-2---ai2-initiative_final.pdf?sfvrsn=2; “Singapore’s approach to AI governance,” Personal Data  Protection Commissioner Singapore, accessed September 1, 2021, https://www.pdpc.gov.sg/Help-and-Resources/2020/01/Model-AIGovernance-Framework; “Principles to promote fairness, ethics, accountability, and transparency (FEAT) in the use of artificial intelligence  and data analytics in Singapore’s financial sector,” Monetary Authority of Singapore, http://www.mas.gov.sg/~/media/MAS/News%20 and%20Publications/Monographs%20and%20Information%20Papers/FEAT%20Principles%20Final.pdf; Singapore researchers plug in to  world’s fastest supercomputer,” HPC Wire, November 30, 2020, https://www.hpcwire.com/off-the-wire/singapore-researchers-plug-in-toworlds-fastest-supercomputer/;  316.
(a / and
      :op1 (p / publication-91
            :ARG1 (p2 / publication
                  :name (n / name
                        :op1 "Launch"
                        :op2 "of"
                        :op3 "Accelerated"
                        :op4 "Initiative"
                        :op5 "for"
                        :op6 "Artificial"
                        :op7 "Intelligence")
                  :ARG1-of (m / mean-01
                        :ARG2 (p3 / publication
                              :name (n2 / name
                                    :op1 "Public"
                                    :op2 "Data"
                                    :op3 "Protection"
                                    :op4 "Commission")
                              :mod (c / country
                                    :name (n3 / name
                                          :op1 "Singapore"))
                              :ARG1-of (a2 / access-01
                                    :time (d / date-entity
                                          :month 9
                                          :day 1
                                          :year 2021)))))
            :ARG4 (u / url-entity
                  :value "https://www.hpcwire.gov.sg/off-the-wire/singapore-researchers-plug-in-toworlds-fastest-supercomputer/"))
      :op2 (p4 / publication
            :name (n4 / name
                  :op1 "Model-AIGovernance-Framework"))
      :op3 (p5 / publication
            :name (n5 / name
                  :op1 "Principles"
                  :op2 "to"
                  :op3 "Promoting"
                  :op4 "for"
                  :op5 "Artificial"
                  :op6 "Intelligence"
                  :op7 "and"
                  :op8 "Ethics"
                  :op9 "and"
                  :op10 "Data"
                  :op11 "Accountability"
                  :op12 "and"
                  :op13 "Transparency"))
      :time (d2 / date-entity
            :month 11
            :day 30
            :year 2020))


# ::id 47
# ::snt Section 2   Basic Philosophy   Dignity: A society that has respect for human dignity   Diversity & Inclusion: A society where people with diverse backgrounds can pursue  their well- being   Sustainability: A sustainable society   Section 3  Social Changes Needed to Realize Society 5.0 - "AI-Ready Society5"  Human Potential, Social Systems, Industrial Structures,   Innovation Systems (environments that support innovation),  Governance   Section 4  Social Principles of Human -Centric AI   4.1 Social Principles of AI     (1) Human -Centric, (2) Education/Literacy, (3) Privacy Protection,     (4) Ensur ing Security,  (5) Fair Competition,     (6) Fairness, Accountability, and Transparency,  (7) Innovation   4.2 R&D and Utilization Principles of AI     Figure 1:  Overall Structure of This Document                                                          5 An "AI -Ready Society" means that society as a whole has undergone the necessary changes to maximize the  benefits of AI, enjoys the benefits of AI, or has introduced AI i mmediately when needed and is in a state of being  able to receive the benefits.
(m / multi-sentence
      :snt1 (m2 / mean-01
            :ARG1 (s / society
                  :ARG1-of (r / ready-02
                        :ARG2 (ii / intelligent-01
                              :mod (a / artificial))))
            :ARG2 (o / or
                  :op1 (u / undergo-28
                        :ARG1 s
                        :ARG2 (c / change-01
                              :ARG1-of (n / need-01
                                    :purpose (m3 / maximize-01
                                          :ARG0 c
                                          :ARG1 (b / benefit-01
                                                :ARG0 ii)))))
                  :op2 (e / enjoy-01
                        :ARG0 s
                        :ARG1 (b2 / benefit-01
                              :ARG0 ii))
                  :op3 (ii2 / introduce-02
                        :ARG0 s
                        :ARG1 ii
                        :time (ii3 / immediate)
                        :time n
                        :ARG1 ii2)))
      :snt2 (a2 / and
            :op1 (p / philosophy
                  :mod (b3 / basic))
            :op2 (d / dignity
                  :mod (h / human))
            :op3 (a3 / and
                  :op1 (d2 / diversity)
                  :op2 (ii4 / inclusion)
                  :domain (s2 / society
                        :location-of (p2 / possible-01
                              :ARG1 (p3 / pursue-01
                                    :ARG0 (p4 / person
                                          :ARG0-of (h2 / have-03
                                                :ARG1 (b4 / background
                                                      :mod (d3 / diverse))))
                                    :ARG1 (w / well-09
                                          :ARG1 p4)))))
            :op4 (s3 / sustain-01
                  :ARG1 s2
                  :ARG1-of (p5 / possible-01)))
      :snt3 (s4 / section
            :mod 3)
      :snt4 (a4 / and
            :op1 (p6 / principle
                  :mod (s5 / social)
                  :topic (ii5 / intelligent-01
                        :mod (c2 / center
                              :mod h)))
            :op2 (p7 / principle
                  :mod (s6 / social)
                  :topic ii5)
            :op3 (p8 / principle
                  :mod (t / technology
                        :name (n2 / name
                              :op1 "Advanced"
                              :op2 "Intelligence"
                              :op3 "Systems")))
            :op4 (p9 / principle
                  :mod (d4 / develop-02
                        :ARG1 t))
            :op5 (p10 / principle
                  :mod (o2 / operate-01
                        :ARG1 t))
            :op6 (p11 / principle
                  :mod (f / fair-01))
            :op7 (a5 / accountable-02)
            :op8 (t2 / transparency))
      :snt5 (f2 / figure
            :mod 1))


# ::id 47
# ::snt These characteristics include  bias in data, the possibility of causing   bias depending on how AI is used , and  issues of fairness, impartiality , and p rivacy  protection that are inherent ly needed  in the use of AI.
(ii / include-01
      :ARG1 (a / and
            :op1 (b / bias-01
                  :ARG1 (d / data))
            :op2 (p / possible-01
                  :ARG1 (c / cause-01
                        :ARG1 (b2 / bias-01)
                        :ARG0-of (d2 / depend-01
                              :ARG1 (t / thing
                                    :manner-of (u / use-01
                                          :ARG1 (ii2 / intelligent-01
                                                :mod (a2 / artificial)))))))
            :op3 (ii3 / issue-02
                  :ARG0 (a3 / and
                        :op1 (f / fairness)
                        :op2 (ii4 / impartiality)
                        :op3 (p2 / protect-01
                              :ARG1 (p3 / prejudice-01))
                        :ARG1-of (n / need-01
                              :ARG0 u
                              :mod (ii5 / inherent)))))
      :ARG2 t
      :ARG2-of (c2 / characteristic-02)
      :mod (t2 / this))


# ::id 47
# ::snt (6) The Principle  of Fairness, Accountability, and Transparency     In an "AI- Ready Society", it is necessary to ensure fairness and transparency in  decision -making , appropriate accountability for the results , and trust in  the technology ,  so that people who use AI are not subject to  undue discriminatio n with regard to  personal  background, or to unfair treatment  in terms  of human dignity.
(n / need-01
      :li 6
      :ARG1 (e / ensure-01
            :ARG0 (s / society
                  :ARG1-of (r / ready-02
                        :ARG2 (ii / intelligent-01
                              :mod (a / artificial))))
            :ARG1 (a2 / and
                  :op1 (f / fairness)
                  :op2 (t / transparency)
                  :topic (a3 / and
                        :op1 (d / decide-01)
                        :op2 (a4 / accountable-02
                              :ARG1 (r2 / result-01)
                              :ARG1-of (a5 / appropriate-02))
                        :op3 (t2 / trust-02
                              :ARG1 (t3 / technology)))))
      :purpose (s2 / subject-01
            :polarity -
            :ARG1 (p / person
                  :ARG0-of (u / use-01
                        :ARG1 ii))
            :ARG2 (o / or
                  :op1 (d2 / discriminate-02
                        :ARG1 p
                        :ARG2 (b / background
                              :ARG1-of (p2 / personal-02)))
                  :op2 (t4 / treat-01
                        :ARG1 p
                        :ARG1-of (f2 / fair-01
                              :polarity -)
                        :topic (d3 / dignity
                              :mod (h / human))))))


# ::id 48
# ::snt I. Definitions    The GDPR  introduces provisions to ensure  that profiling and automated individual decision -making  (whether or not this in cludes  profiling) are not used in ways that have an unjustified impact on   individuals’ rights ; for example:      specific transparency and fairness  requirements;    greater accountability obligations;    specified legal bases for the processing;    rights for individuals to op pose profiling and specifically profiling for marketing; and    if certain conditions are met, a need to carry out a data protection impact assessment.
(ii / introduce-02
      :li "I"
      :ARG0 (l / law
            :name (n / name
                  :op1 "GDPR"))
      :ARG1 (p / provision
            :purpose (e / ensure-01
                  :ARG0 l
                  :ARG1 (u / use-01
                        :polarity -
                        :ARG1 (a / and
                              :op1 (p2 / profile-01
                                    :ARG1 (ii2 / individual))
                              :op2 (a2 / automate-01
                                    :ARG1 (d / decide-01
                                          :ARG0 ii2)))
                        :manner (w / way
                              :ARG0-of (ii3 / impact-01
                                    :ARG1 (r / right-05
                                          :ARG1 (ii4 / individual))
                                    :ARG1-of (j / justify-01
                                          :polarity -))
                              :example (a3 / and
                                    :op1 (r2 / require-01
                                          :ARG1 (a4 / and
                                                :op1 (t / transparency)
                                                :op2 (f / fairness))
                                          :ARG1-of (s / specific-02))
                                    :op2 (o / obligate-01
                                          :ARG2 (a5 / accountable-02)
                                          :ARG1-of (h / have-degree-91
                                                :ARG2 (g / great)
                                                :ARG3 (m / more)))
                                    :op3 (b / base-02
                                          :ARG1 (p3 / process-01)
                                          :ARG1-of (s2 / specify-01))
                                    :op4 (r3 / right-05
                                          :ARG1 (ii5 / individual)
                                          :ARG2 (a6 / and
                                                :op1 (p4 / profile-01
                                                      :ARG1 ii5
                                                      :ARG1-of (s3 / specific-02)
                                                      :purpose (m2 / market-01
                                                            :ARG1 ii5))
                                                :op2 (p5 / profile-01
                                                      :ARG1 ii5
                                                      :ARG1-of (l2 / label-01
                                                            :ARG2 (a7 / amr-unknown)))))))
                        :condition (m3 / meet-01
                              :ARG1 (c / condition
                                    :mod (c2 / certain)))))))


# ::id 48
# ::snt Performance of a  contract     Controllers may wish to use  automated decision -making , for example, because it :      potentially allow s for greater consistency or fairness in the decision making process  (e.g.
(p / possible-01
      :ARG1 (w / wish-01
            :ARG0 (p2 / person
                  :ARG0-of (c / control-01))
            :ARG1 (u / use-01
                  :ARG0 p2
                  :ARG1 (m / make-01
                        :ARG1 (d / decide-01)
                        :ARG1-of (a / automate-01))))
      :ARG0-of (e / exemplify-01)
      :ARG1-of (c2 / cause-01
            :ARG0 (a2 / allow-01
                  :ARG0 m
                  :ARG1 (o / or
                        :op1 (c3 / consistent-02
                              :ARG1 (p3 / process-02
                                    :ARG1 (d2 / decide-01))
                              :ARG1-of (h / have-degree-91
                                    :ARG2 (g / great)
                                    :ARG3 (m2 / more)))
                        :op2 (f / fair-01
                              :ARG1 p3
                              :ARG1-of (h2 / have-degree-91
                                    :ARG2 (g2 / great)
                                    :ARG3 (m3 / more)))
                        :example (t / thing))
                  :mod (p4 / potential))))


# ::id 48
# ::snt The following are particularly relevant:    the le vel of detail of the profile ( a data subject profiled within a broadly described cohort  such  as ‘native English teachers li ving in Paris’ , or segmented a nd targeted on a granular level );   the comprehensiveness of the profile ( whether the profile only describe s a small aspect of the  data subject, or paints  a more comprehensive picture );    the impact of the profiling (the effects on th e data subject ); and    the safeguards aimed at ensuring fairness, non -discrim ination and accuracy in the profiling  process.
(r / relevant-01
      :ARG1 (t / thing
            :ARG1-of (f / follow-04)
            :ARG2-of (ii / include-91
                  :ARG1 (a / and
                        :op1 (p / profile
                              :mod (s / subject
                                    :mod (d / data))
                              :ARG1-of (p2 / profile-01
                                    :location (c / cohort
                                          :ARG1-of (d2 / describe-01
                                                :ARG1-of (b / broad-02))
                                          :example (p3 / person
                                                :ARG0-of (t2 / teach-01)
                                                :mod (c2 / country
                                                      :name (n / name
                                                            :op1 "England"))
                                                :ARG1-of (n2 / native-02))
                                          :location (c3 / city
                                                :name (n3 / name
                                                      :op1 "Paris")))))
                        :op2 (a2 / and
                              :op1 (s2 / segment-01
                                    :ARG1 p)
                              :op2 (t3 / target-01
                                    :ARG1 p)
                              :manner (l / level
                                    :mod (g / granular)))
                        :op3 (e / effective-04
                              :ARG0 (s3 / safeguard-01)
                              :ARG1 (e2 / ensure-01
                                    :ARG0 s3
                                    :ARG1 (a3 / and
                                          :op1 (f2 / fairness)
                                          :op2 (d3 / discriminate-01
                                                :polarity -)
                                          :op3 (a4 / accurate))
                                    :time (p4 / process-02
                                          :ARG1 p))))))
      :op2 (h / have-condition-91
            :ARG1 (p5 / possible-01
                  :ARG1 (o / or
                        :op1 d2
                        :ARG0 p
                        :ARG1 (a5 / aspect
                              :mod (s4 / small)
                              :part-of s))
                  :op2 (p6 / paint-03
                        :ARG0 p
                        :ARG1 (p7 / picture
                              :ARG1-of (h2 / have-degree-91
                                    :ARG2 (c4 / comprehensive)
                                    :ARG3 (m / more))))))
      :ARG2 (ii2 / impact-01
            :ARG0 p
            :ARG1 s)
      :mod (p8 / particular))


# ::id 54
# ::snt Fairness requires that even careful decision processes provide some avenue for redress of grievances.
(r / require-01
      :ARG0 (f / fairness)
      :ARG1 (p / provide-01
            :ARG0 (p2 / process-02
                  :ARG1 (d / decide-01)
                  :ARG1-of (c / care-04)
                  :mod (e / even))
            :ARG1 (a / avenue
                  :mod (s / some))
            :ARG2 (r2 / redress-01
                  :ARG1 (c2 / complain-01))))


# ::id 57
# ::snt The complex interactions between bias, fairness and transparency in AI enabled predictive systems are  exemplified in the COMPAS case study.
(e / exemplify-01
      :ARG0 (s / study
            :name (n / name
                  :op1 "CompAS")
            :mod (c / case))
      :ARG1 (ii / interact-01
            :ARG0 (b / bias-01)
            :ARG1 (f / fairness)
            :ARG2 (t / transparency)
            :mod (c2 / complex)
            :location (s2 / system
                  :ARG0-of (e2 / enable-01
                        :ARG1 (ii2 / intelligent-01
                              :mod (a / artificial)))
                  :ARG0-of (p / predict-01))))


# ::id 57
# ::snt Although the use of AI enabled predictive systems to help support  decision making processes poses great potential benefits in boosting replicability and reducing human error  and bias, there are inherent ethical issues that must be addressed , particular ly the effects of indirect  discrimination  and fairness .
(h / have-concession-91
      :ARG1 (p / pose-02
            :ARG0 (u / use-01
                  :ARG1 (ii / intelligent-01
                        :mod (a / artificial))
                  :ARG2 (h2 / help-01
                        :ARG0 (s / system
                              :ARG0-of (p2 / predict-01))
                        :ARG1 (s2 / support-01
                              :ARG0 s
                              :ARG1 (p3 / process-02
                                    :ARG1 (d / decide-01)))))
            :ARG1 (b / benefit-01
                  :ARG0 u
                  :ARG1 (a2 / and
                        :op1 (b2 / boost-01
                              :ARG0 u
                              :ARG1 (r / replicate-01))
                        :op2 (r2 / reduce-01
                              :ARG0 u
                              :ARG1 (a3 / and
                                    :op1 (e / err-01
                                          :ARG0 (h3 / human))
                                    :op2 (b3 / bias-01
                                          :ARG0 h3))))
                  :mod (p4 / potential)
                  :mod (g / great)))
      :ARG2 (o / obligate-01
            :ARG2 (a4 / address-02
                  :ARG1 (ii2 / issue-02
                        :ARG0 (a5 / and
                              :op1 (a6 / affect-01
                                    :ARG0 (d2 / discriminate-02
                                          :ARG1-of (d3 / direct-02
                                                :polarity -)))
                              :op2 (a7 / affect-01
                                    :ARG0 (f / fair-01))
                              :mod (p5 / particular))
                        :mod (e2 / ethics)
                        :mod (ii3 / inherent)))))


# ::id 57
# ::snt Even when the  information is not flawed, if the priorities of the system are not aligned with expectations of fairness, then  the system can deliver negative outcomes.
(h / have-condition-91
      :ARG1 (p / possible-01
            :ARG1 (d / deliver-01
                  :ARG0 (s / system)
                  :ARG1 (o / outcome
                        :ARG0-of (n / negative-02))))
      :ARG2 (a / align-01
            :polarity -
            :ARG1 (p2 / priority
                  :poss s)
            :ARG2 (e / expect-01
                  :ARG1 (f / fairness)))
      :concession (e2 / even-when
            :op1 (f2 / flaw-01
                  :polarity -
                  :ARG1 (ii / information))))


# ::id 57
# ::snt 5.3 Fairness and predictions   The challenge of ensuring fairness in algorithms is not limited to biased datasets.
(c / challenge-01
      :li 5.3
      :ARG2 (e / ensure-01
            :ARG1 (f / fair-01
                  :ARG1 (a / algorithm)))
      :ARG1-of (l / limit-01
            :polarity -
            :ARG2 (d / dataset
                  :ARG1-of (b / bias-01)))
      :topic (a2 / and
            :op1 (f2 / fairness)
            :op2 (p / predict-01)))


# ::id 57
# ::snt The ability to assess bias in predictive systems is intrinsically linked w ith how fairness is measured along  with the level of transparency involved.
(l / link-01
      :ARG1 (c / capable-01
            :ARG2 (a / assess-01
                  :ARG1 (b / bias-01
                        :ARG0 (s / system
                              :ARG0-of (p / predict-01)))))
      :ARG2 (a2 / and
            :op1 (t / thing
                  :manner-of (m / measure-01
                        :ARG1 (f / fair-01)))
            :op2 (l2 / level
                  :degree-of (t2 / transparency
                        :ARG1-of (ii / involve-01))))
      :manner (ii2 / intrinsic))


# ::id 57
# ::snt So what is “fairness?” It really depends who you ask.
(m / multi-sentence
      :snt1 (f / fair-01
            :domain (a / amr-unknown))
      :snt2 (d / depend-01
            :ARG0 (ii / it)
            :ARG1 (p / person
                  :ARG1-of (a2 / ask-01
                        :ARG0 (y / you)))
            :ARG1-of (r / real-04)))


# ::id 57
# ::snt Researchers have come up with many dozens  of mathematical definitions to define what fairness means in  an algorithm and many of them perform extremely well when measured from one angle, but from a  different angle can produce very different results.
(a / and
      :op1 (c / come-up-11
            :ARG0 (p / person
                  :ARG0-of (r / research-01))
            :ARG1 (t / thing
                  :ARG2-of (d / define-01
                        :ARG0 p
                        :ARG1 (t2 / thing
                              :ARG2-of (m / mean-01
                                    :ARG1 (f / fairness)
                                    :location (a2 / algorithm))))
                  :quant (m2 / multiple
                        :op1 12)
                  :mod (m3 / mathematics))
            :purpose (d2 / define-01
                  :ARG0 p
                  :ARG1 t2))
      :op2 (p2 / perform-02
            :ARG0 (t3 / thing
                  :quant (m4 / many)
                  :ARG2-of (d3 / define-01
                        :ARG0 p)
                  :ARG1-of (ii / include-91
                        :ARG2 t))
            :ARG1 (w / well-09
                  :degree (e / extreme))
            :time (m5 / measure-01
                  :ARG2 (a3 / angle
                        :quant 1))
            :concession-of (p3 / possible-01
                  :ARG1 (p4 / produce-01
                        :ARG0 (a4 / angle
                              :ARG1-of (d4 / differ-02
                                    :degree (v / very)))
                        :ARG1 (t4 / thing
                              :ARG2-of (r2 / result-01)
                              :ARG1-of (d5 / differ-02
                                    :degree v))))))


# ::id 57
# ::snt This concept of differing perspectives of fairness is  exemplified in the COMPAS case study.
(e / exemplify-01
      :ARG0 (s / study-01
            :ARG1 (c / case-04
                  :ARG1 (c2 / company
                        :name (n / name
                              :op1 "Compass"))))
      :ARG1 (c3 / concept
            :mod (t / this)
            :topic (p / perspective
                  :ARG1-of (d / differ-02)
                  :topic (f / fairness))))


# ::id 57
# ::snt Put simply: it will sometimes be mathematically impossible to meet every single fairness measure because  some of them contradict each other and multiple datasets will be used in systems, and these datasets will  almost never be exactly equal in acc uracy or representativeness.
(p / put-02
      :ARG1 (p2 / possible-01
            :polarity -
            :ARG1 (m / meet-01
                  :ARG1 (m2 / measure-01
                        :ARG1 (f / fair-01)
                        :ARG1-of (s / single-02)
                        :mod (e / every)))
            :mod (m3 / mathematics)
            :frequency (s2 / sometimes)
            :ARG1-of (c / cause-01
                  :ARG0 (a / and
                        :op1 (c2 / contradict-01
                              :ARG0 (s3 / some
                                    :ARG1-of (ii / include-91
                                          :ARG2 m2))
                              :ARG1 s3)
                        :op2 (u / use-01
                              :ARG1 (d / dataset
                                    :quant (m4 / multiple))
                              :location (s4 / system))
                        :op3 (e2 / equal-01
                              :polarity -
                              :ARG1 (d2 / dataset
                                    :mod (t / this))
                              :ARG3 (o / or
                                    :op1 (a2 / accurate)
                                    :op2 (r / represent-01))
                              :mod (e3 / exact)
                              :time (e4 / ever
                                    :mod (a3 / almost))))))
      :ARG1-of (s5 / simple-02))


# ::id 57
# ::snt It is important for government and society to give consideration to the degree of flexibility that designers of  AI systems should have when it comes to making trade -offs between fairness m easures and other priorities  like profit .
(ii / important-01
      :ARG1 (c / consider-02
            :ARG0 (a / and
                  :op1 (g / government-organization
                        :ARG0-of (g2 / govern-01))
                  :op2 (s / society))
            :ARG1 (d / degree
                  :degree-of (f / flexibility
                        :ARG1-of (h / have-03
                              :ARG0 (p / person
                                    :ARG0-of (d2 / design-01
                                          :ARG1 (s2 / system
                                                :mod (a2 / artificial))))
                              :ARG1-of (r / recommend-01)
                              :time (c2 / come-12
                                    :ARG1 (t / trade-off-02
                                          :ARG0 p
                                          :ARG1 (f2 / fairness)
                                          :ARG2 (p2 / priority
                                                :mod (o / other)
                                                :example (p3 / profit-01)))))))))


# ::id 57
# ::snt If a company prioritises profit ahead of various forms of fairness, can they justify it?
(p / possible-01
      :ARG1 (j / justify-01
            :ARG0 (c / company)
            :ARG1 (p2 / prioritize-01
                  :ARG0 c
                  :ARG1 (p3 / profit-01
                        :ARG0 c)
                  :ARG1-of (ii / instead-of-91
                        :ARG2 (f / fairness
                              :mod (f2 / form
                                    :mod (v / various))))))
      :polarity (a / amr-unknown))


# ::id 57
# ::snt E ven when the  information is not flawed, if the priorities of the system are not aligned with expectations of fairness, then  the system can deliver negative outcomes.
(p / possible-01
      :ARG1 (d / deliver-01
            :ARG0 (s / system)
            :ARG1 (o / outcome
                  :ARG0-of (n / negative-02)))
      :condition (f / flaw-01
            :polarity -
            :ARG1 (ii / information))
      :condition (a / align-01
            :polarity -
            :ARG1 (p2 / priority
                  :poss s)
            :ARG2 (e / expect-01
                  :ARG1 (f2 / fairness))))


# ::id 57
# ::snt Fairness.
(f / fairness)


# ::id 57
# ::snt In addition, it can assess the algorithm’s accuracy, fairness and  performance.
(a / and
      :op2 (p / possible-01
            :ARG1 (a2 / assess-01
                  :ARG0 (ii / it)
                  :ARG1 (a3 / and
                        :op1 (a4 / accurate
                              :domain (a5 / algorithm))
                        :op2 (f / fair-01
                              :ARG1 a5)
                        :op3 (p2 / perform-02
                              :ARG0 a5)))))


# ::id 57
# ::snt 7.1.7 Monitoring AI   This consists of regular monitoring of AI or automated decision systems for accuracy, fairness and suitability  for the task at hand.
(m / multi-sentence
      :snt1 (m2 / monitor-01
            :li "7.1.7"
            :ARG1 (ii / intelligent-01
                  :mod (a / artificial)))
      :snt2 (c / consist-01
            :ARG1 (t / this)
            :ARG2 (m3 / monitor-01
                  :ARG1 (o / or
                        :op1 (ii2 / intelligent-01
                              :mod a))
                  :op2 (s / system
                        :ARG0-of (d / decide-01)
                        :ARG1-of (a2 / automate-01)))
            :ARG2 (a3 / and
                  :op1 (a4 / accuracy)
                  :op2 (f / fairness)
                  :op3 (s2 / suitable-04
                        :ARG1 o
                        :ARG2 (t2 / task
                              :ARG1-of (a5 / at-hand-14))))
            :ARG1-of (r / regular-02)))


# ::id 57
# ::snt Artificial Intelligence: Australia’s Ethics Framework (A Discussion Paper)   Page 64    Privacy Protection   /Consequence  Fairness   /Consequence  Physical harm   /Consequence  Contestability   /Consequence  Accountability   /Consequence  Regulatory and legal  Compliance   /Likelihood  Transparency and  explainability   /Likelihood  Number of  people affected   Insignificant  – no  private or sensitive  data is used by the AI  Insignificant  – has  no effect on a  person’s human  rights  Insignificant  –  application cannot  control or influence  other systems  Insignificant – the  application operates on  an opt -in basis and no  intervention is required  to reverse outcome in  the event someone  decides to opt out  Insignificant –  clear   accountability of  outcomes  Insignificant  – consent  gained for use of data.
(a / and
      :op1 (p / publication
            :name (n / name
                  :op1 "Artificial"
                  :op2 "Intelligence"
                  :op3 "Framework")
            :medium p
            :name (n2 / name
                  :op1 "Australia's"
                  :op2 "Ethics"
                  :op3 "Framework")
            :ARG1-of (d / describe-01
                  :ARG0 (p2 / paper
                        :mod 64
                        :ARG1-of (d2 / discuss-01))))
      :op2 (a2 / and
            :op1 (p3 / page
                  :mod 64)
            :op2 (p4 / protect-01
                  :ARG1 (p5 / privacy))
            :op3 (c / consequential-01)
            :op4 (f / fair-01)
            :op5 (c2 / consequential-01)
            :op6 (h / harm-01
                  :mod (p6 / physical))
            :op7 (c3 / contest-02)
            :op8 (a3 / accountable-02)
            :op9 (c4 / comply-01
                  :ARG1 (a4 / and
                        :op1 (r / regulate-01)
                        :op2 (l / legal-02)))
            :op10 (a5 / and
                  :op1 (t / transparency)
                  :op2 (e / explain-01))
            :op11 (l2 / likelihood)
            :op12 (n3 / number
                  :quant-of (p7 / person
                        :ARG1-of (a6 / affect-01)))
            :op13 (s / significant-02
                  :polarity -
                  :ARG1 (u / use-01
                        :ARG0 (ii / intelligent-01
                              :mod (a7 / artificial))
                        :ARG1 (d3 / data
                              :ARG1-of (p8 / private-02)
                              :ARG1-of (s2 / sensitive-03))))
            :op14 (s3 / significant-02
                  :polarity -
                  :ARG1 u)
            :op15 (s4 / significant-02
                  :polarity -
                  :ARG1 u)
            :op16 (c5 / consent-01
                  :ARG1 u
                  :ARG1 d3))
      :op17 (s5 / significant-02
            :polarity -
            :ARG1 u)
      :op18 (a8 / and
            :op1 (p9 / possible-01
                  :polarity -
                  :ARG1 (o / or
                        :op1 (c6 / control-01
                              :ARG0 ii
                              :ARG1 (s6 / system
                                    :mod (o2 / other)))
                        :op2 (ii2 / influence-01
                              :ARG0 ii
                              :ARG1 s6)))
            :op2 (r2 / require-01
                  :polarity -
                  :ARG1 (ii3 / intervene-01
                        :purpose (r3 / reverse-01
                              :ARG1 (o3 / outcome)))
                  :condition (d4 / decide-01
                        :ARG0 (s7 / someone)
                        :ARG1 (o4 / opt-01
                              :ARG0 s7
                              :ARG1 (o5 / out-06
                                    :ARG1 o3))))))


# ::id 57
# ::snt Introducing AI Fairness 360.
(ii / introduce-01
      :ARG1 (t / thing
            :name (n / name
                  :op1 "AI"
                  :op2 "Fairness"
                  :op3 360)))


# ::id 57
# ::snt 206 Fairness Accountability and Transparency in Machine Learning.
(a / and
      :li 206
      :op1 (a2 / accountable-02
            :ARG1 (f / fairness))
      :op2 (t / transparent
            :topic (l / learn-01
                  :mod (m / machine))))


# ::id 57
# ::snt These included ensuring that AI adhered to ethical principles such as fairness and  transparency.
(ii / include-01
      :ARG1 (e / ensure-01
            :ARG1 (a / adhere-02
                  :ARG0 (ii2 / intelligent-01
                        :mod (a2 / artificial))
                  :ARG1 (p / principle
                        :mod (e2 / ethics)
                        :example (a3 / and
                              :op1 (f / fairness)
                              :op2 (t / transparency)))))
      :ARG2 (t2 / this))


# ::id 57
# ::snt “Fairness” is a difficult concept to pin down and AI designers essentially have to reduce it to statistics.
(a / and
      :op1 (d / difficult
            :domain (p / pin-down-02
                  :ARG1 (c / concept
                        :domain (f / fairness))))
      :op2 (o / obligate-01
            :ARG1 (p2 / person
                  :ARG0-of (d2 / design-01
                        :ARG1 (a2 / artificial)))
            :ARG2 (r / reduce-01
                  :ARG0 p2
                  :ARG1 f
                  :ARG4 (s / statistics))
            :mod (e / essential)))


# ::id 57
# ::snt It also suggests that internal reviews may miss key problems if they base their analysis on the same  assumptions of fairness as the original design.
(s / suggest-01
      :ARG0 (ii / it)
      :ARG1 (p / possible-01
            :ARG1 (m / miss-02
                  :ARG0 (r / review-01
                        :ARG1-of (ii2 / internal-02))
                  :ARG1 (p2 / problem
                        :ARG1-of (k / key-02)))
            :condition (b / base-02
                  :ARG0 r
                  :ARG1 (a / analyze-01
                        :ARG0 r)
                  :ARG2 (a2 / assume-02
                        :ARG1 (f / fairness)
                        :ARG1-of (s2 / same-01
                              :ARG2 (d / design-01
                                    :mod (o / original))))))
      :mod (a3 / also))


# ::id 57
# ::snt ................................ ....................  36  5 Predicting human behaviour  ................................ ................................ ............................  38  5.2 Bias, predictions and discrimination  ................................ ................................ ... 39  5.3 Fairness and predictions  ................................ ................................ ......................  41  5.4 Transparency, policing and predictions ................................ ...............................  42  5.5 Medical predictions  ................................ ................................ .............................  44  5.6 Predictions and consumer behaviour ................................ ................................ .. 45  6 Current examples of AI in practice  ................................ ................................ ...................  48  6.1 Autonomous vehicles  ................................ ................................ ..........................  48  6.2 Personal identification and surveillance  ................................ .............................  52  6.3 Artificial Intelligence and Employment  ................................ ...............................  54  6.4 Gender Diversity in AI workforces  ................................ ................................ .......  55  6.5 Artificial Intelligence and Indigenous Communities  ................................ ............  55  7 A Proposed Ethics Framework  ................................ ................................ ..........................  57  7.1 Putting principles into practice  ................................ ................................ ...........  58 
Artificial Intelligence: Australia’s Ethics Framework (A Discussion Paper)   Page 13  7.2 Example Risk Assessment Framework for AI Systems  ................................ ........  63  8 Conclusion  ................................ ................................ ................................ .........................  66  9 References  ................................ ................................ ................................ ........................  67  Appendix Stakeholder and E xpert Consultation  ................................ ................................ ...........  75    Figures     Figure 1.
(m / multi-sentence
      :snt1 (a / and
            :op1 (p / publication
                  :name (n / name
                        :op1 "Risk"
                        :op2 "Assessment"
                        :op3 "Framework"
                        :op4 "for"
                        :op5 "AI"
                        :op6 "Systems")
                  :ARG1-of (e / exemplify-01))
            :op2 (p2 / publication
                  :name (n2 / name
                        :op1 "Advanced"
                        :op2 "Intelligence"
                        :op3 "and"
                        :op4 "Indigenous"
                        :op5 "Community"))
            :op3 (p3 / publication
                  :name (n3 / name
                        :op1 "Proposed"
                        :op2 "Ethics"
                        :op3 "Framework"))
            :op4 (p4 / publication
                  :name (n4 / name
                        :op1 "Appendix"
                        :op2 "Stakeholder"
                        :op3 "and"
                        :op4 "E."
                        :op5 "Xpert"
                        :op6 "Consultation"))
            :op5 (p5 / publication
                  :name (n5 / name
                        :op1 "Advanced"
                        :op2 "Intelligence"
                        :op3 "and"
                        :op4 "Autonomous"
                        :op5 "Vehicles")
                  :ARG1-of (e2 / exemplify-01
                        :ARG0 (a2 / and
                              :op1 (b / bias-01)
                              :op2 (p6 / predict-01)
                              :op3 (d / discriminate-02))))
            :op6 (p7 / publication
                  :name (n6 / name
                        :op1 "5.2"))
            :op7 (p8 / publication
                  :name (n7 / name
                        :op1 "6.1"))
            :op8 (p9 / publication
                  :name (n8 / name
                        :op1 "7.2"))
            :op9 (p10 / publication
                  :name (n9 / name
                        :op1 "6.5"))
            :op10 (p11 / publication
                  :name (n10 / name
                        :op1 "7/8"))
            :op11 (p12 / publication
                  :name (n11 / name
                        :op1 "6/5"))
            :op12 (p13 / publication
                  :name (n12 / name
                        :op1 "6/6"))
            :op13 (p14 / publication
                  :name (n13 / name
                        :op1 "7/7"))
            :op14 (p15 / publication
                  :name (n14 / name
                        :op1 "6/6"))
            :op15 "7/7"))


# ::id 57
# ::snt Although the  previously discussed concepts of HITL, transparency and black  box issues and accountability apply, the capability to predict future potential actions poses additional  specific ethical concerns related to bias and fairness that require consideration.
(p / pose-02
      :ARG0 (c / capable-01
            :ARG2 (p2 / predict-01
                  :ARG1 (a / act-02
                        :time (f / future)
                        :mod (p3 / potential))))
      :ARG1 (c2 / concern-01
            :mod (e / ethics)
            :ARG1-of (s / specific-02)
            :ARG1-of (r / relate-01
                  :ARG2 (a2 / and
                        :op1 (b / bias-01)
                        :op2 (f2 / fair-01)))
            :ARG0-of (r2 / require-01
                  :ARG1 (c3 / consider-02
                        :ARG1 c2))
            :mod (a3 / additional))
      :concession (a4 / apply-02
            :ARG1 (c4 / concept
                  :ARG1-of (d / discuss-01
                        :time (p4 / previous))
                  :topic (a5 / and
                        :op1 (l / law
                              :name (n / name
                                    :op1 " HITL"))
                        :op2 (t / transparency)
                        :op3 (ii / issue-02
                              :ARG0 (b2 / box
                                    :ARG1-of (b3 / black-04)))
                        :op4 (a6 / accountable-02)))))


# ::id 57
# ::snt The discrepancy between the two analyses essentially  came down to the way each group assessed and measured fairness and balanced  it with accuracy of the  system.
(c / come-down-23
      :ARG1 (d / discrepancy
            :mod (a / analyze-01
                  :quant 2))
      :ARG2 (a2 / and
            :op1 (w / way
                  :manner-of (a3 / assess-01
                        :ARG0 (g / group
                              :mod (e / each))
                        :ARG1 (f / fair-01)))
            :op2 (w2 / way
                  :manner-of (m / measure-01
                        :ARG0 g
                        :ARG1 f))
            :op3 (w3 / way
                  :manner-of (b / balance-01
                        :ARG0 g
                        :ARG1 f
                        :ARG2 (a4 / accurate
                              :domain (s / system)))))
      :mod (e2 / essential))


# ::id 57
# ::snt Artificial intelligence should operate on principles of intelligibility and fairness.
(r / recommend-01
      :ARG1 (o / operate-01
            :ARG0 (ii / intelligent-01
                  :mod (a / artificial))
            :ARG1 (p / principle
                  :domain (a2 / and
                        :op1 (ii2 / intelligible)
                        :op2 (f / fairness)))))


# ::id 57
# ::snt Other elements of the report cover social licence, inclusion, privacy and data bias in AI, as well  as the differing concepts of fairness in algorithms.
(c / cover-01
      :ARG0 (e / element
            :part-of (r / report)
            :mod (o / other))
      :ARG1 (a / and
            :op1 (l / license-01
                  :mod (s / society))
            :op2 (ii / include-01)
            :op3 (p / private-02)
            :op4 (b / bias-01
                  :ARG2 (d / data))
            :topic (a2 / artificial-intelligence))
      :op5 (c2 / concept
            :ARG1-of (d2 / differ-02)
            :topic (f / fair-01
                  :ARG1 (a3 / algorithm))))


# ::id 57
# ::snt It also makes the key point that a  number of values are often in conflict with each other and there will inevitably be tradeoffs —for example,  quality of services can often  be in conflict with privacy; convenience can be in conflict with dignity and  accuracy can b e in conflict with fairness  [70].
(p / point-04
      :ARG0 (ii / it)
      :ARG1 (a / and
            :op1 (c / conflict-01
                  :ARG0 (n / number
                        :quant-of (v / value)
                        :ARG1-of c)
                  :frequency (o / often))
            :op2 (a2 / avoid-01
                  :polarity -
                  :ARG1 (t / tradeoff-01
                        :example (a3 / and
                              :op1 (p2 / possible-01
                                    :ARG1 (c2 / conflict-01
                                          :ARG0 (q / quality
                                                :poss (s / serve-01))
                                          :ARG1 (p3 / privacy))
                                    :frequency (o2 / often))
                              :op2 (p4 / possible-01
                                    :ARG1 (c3 / conflict-01
                                          :ARG0 (c4 / convenience)
                                          :ARG1 (d / dignity)))
                              :op3 (p5 / possible-01
                                    :ARG1 (c5 / conflict-01
                                          :ARG0 (a4 / accurate)
                                          :ARG1 (f / fairness)))))))
      :ARG1-of (k / key-02)
      :mod (a5 / also)
      :ARG1-of (d2 / describe-01
            :ARG0 (p6 / publication
                  :ARG1-of (c6 / cite-01
                        :ARG2 70))))


# ::id 57
# ::snt It also  highlighted key developments in ethical AI rese arch and emerging strategies to combat bias, such as  recognising allocative and representational harms, new observational fairness strategies, anti -classification  strategies (which focus on appropriate input data and measuring results), classification pari ty (equal  performance across groups, even at a cost to accuracy among certain groups in some cases) and calibration  strategies.
(h / highlight-01
      :ARG0 (ii / it)
      :ARG1 (d / develop-01
            :ARG1 (a / and
                  :op1 (s / strategy
                        :mod (ii2 / intelligent-01
                              :mod (a2 / artificial))
                        :mod (r / remote))
                  :op2 (s2 / strategy
                        :ARG0-of (e / emerge-02)))
            :ARG1-of (k / key-02)
            :purpose (c / combat-01
                  :ARG0 a
                  :ARG1 (b / bias-01))
            :example (a3 / and
                  :op1 (r2 / recognize-02
                        :ARG1 (h2 / harm-01
                              :ARG0 (a4 / and
                                    :op1 (a5 / allocate-01)
                                    :op2 (r3 / represent-01))))
                  :op2 (s3 / strategy
                        :mod (f / fairness)
                        :ARG1-of (n / new-01)
                        :mod (o / observe-01))
                  :op3 (s4 / strategy
                        :ARG0-of (o2 / oppose-01
                              :ARG1 (c2 / classify-01))
                        :ARG0-of (f2 / focus-01
                              :ARG1 (a6 / and
                                    :op1 (d2 / data
                                          :mod (ii3 / input)
                                          :ARG1-of (a7 / appropriate-02))
                                    :op2 (m / measure-01
                                          :ARG1 (r4 / result)))))
                  :op4 (c3 / classify-01
                        :mod (p / proportional)
                        :ARG1-of (m2 / mean-01
                              :ARG2 (p2 / perform-02
                                    :ARG1-of (e2 / equal-01
                                          :ARG3 (g / group))
                                    :concession (c4 / cost-01
                                          :ARG2 (a8 / accurate)
                                          :ARG3 (g2 / group
                                                :mod (c5 / certain))
                                          :time (c6 / case-04
                                                :mod (s5 / some))))))
                  :op5 (s6 / strategy
                        :mod (c7 / calibrate-01))))
      :mod (a9 / also))


# ::id 57
# ::snt 4) Fairness Obligation.
(o / obligate-01
      :li 4
      :ARG2 (f / fairness))


# ::id 57
# ::snt Its site includes six key prin ciples: Fairness, inclusiveness, reliability and  safety, transparency, privacy and security, and accountability.
(ii / include-01
      :ARG1 (p / principle
            :quant 6
            :ARG1-of (k / key-02)
            :domain (a / and
                  :op1 (f / fairness)
                  :op2 (ii2 / inclusiveness)
                  :op3 (a2 / and
                        :op1 (r / rely-01
                              :ARG1-of (p2 / possible-01))
                        :op2 (s / safe-01))
                  :op4 (t / transparency)
                  :op5 (a3 / and
                        :op1 (p3 / privacy)
                        :op2 (s2 / security))
                  :op6 (a4 / accountable-02)))
      :ARG2 (s3 / site
            :poss (ii3 / it)))


# ::id 57
# ::snt Mechanisms for monitoring  and improvement: Regular  monitoring of AI for accuracy,  fairness and suitability for the task  at hand.
(m / mechanism
      :purpose (a / and
            :op1 (m2 / monitor-01)
            :op2 (ii / improve-01))
      :domain (m3 / monitor-01
            :ARG1 (ii2 / intelligent-01
                  :mod (a2 / artificial))
            :ARG2 (a3 / and
                  :op1 (a4 / accuracy)
                  :op2 (f / fairness)
                  :op3 (s / suitable-04
                        :ARG1 ii2
                        :ARG2 (t / task
                              :ARG1-of (a5 / at-hand-14))))
            :ARG1-of (r / regular-02)))


# ::id 57
# ::snt Pathways forward involve a variety  of measures for different situations, ranging from explainable AI technologies  [27], testing, regulation that  requires transparency in the key priorities and fairness measures used in an AI system, through to  measures enabling external review and monitoring  [26].
(ii / involve-01
      :ARG1 (v / variety
            :domain (m / measure-02
                  :ARG2 (s / situation
                        :ARG1-of (d / differ-02)))
            :ARG1-of (r / range-01
                  :ARG3 (a / and
                        :op1 (t / technology
                              :mod (a2 / artificial)
                              :ARG1-of (e / explain-01
                                    :ARG1-of (p / possible-01))
                              :ARG1-of (d2 / describe-01
                                    :ARG0 (p2 / publication
                                          :ARG1-of (c / cite-01
                                                :ARG2 27))))
                        :op2 (t2 / test-01)
                        :op3 (r2 / regulate-01
                              :ARG0-of (r3 / require-01
                                    :ARG1 (t3 / transparency
                                          :topic (a3 / and
                                                :op1 (p3 / priority
                                                      :ARG1-of (k / key-02))
                                                :op2 (m2 / measure-02
                                                      :ARG1 (f / fair-01)
                                                      :ARG1-of (u / use-01
                                                            :ARG2 (s2 / system
                                                                  :mod a2))))))))
                  :ARG4 (m3 / measure-02
                        :ARG0-of (e2 / enable-01
                              :ARG1 (a4 / and
                                    :op1 (r4 / review-01
                                          :mod (e3 / external))
                                    :op2 (m4 / monitor-01
                                          :mod e3)))
                        :ARG1-of (d3 / describe-01
                              :ARG0 (p4 / publication
                                    :ARG1-of (c2 / cite-01
                                          :ARG2 26))))))
      :ARG2 (p5 / path
            :direction (f2 / forward)))


# ::id 57
# ::snt The Automated Decisions Task Force  will examine automated systems through the lens of equity, fairness and accountability, and is set to  release a report in December 2019 that will recommen d procedures for reviewing and assessing  algorithmic tools used by the city  [2].
(a / and
      :op1 (e / examine-01
            :ARG0 (o / organization
                  :name (n / name
                        :op1 "Automated"
                        :op2 "Decisions"
                        :op3 "Task"
                        :op4 "Force"))
            :ARG1 (s / system
                  :ARG1-of (a2 / automate-01))
            :manner (l / lens
                  :topic (a3 / and
                        :op1 (e2 / equity)
                        :op2 (f / fairness)
                        :op3 (a4 / accountable-02))))
      :op2 (s2 / set-08
            :ARG1 o
            :ARG2 (r / release-01
                  :ARG0 o
                  :ARG1 (r2 / report
                        :ARG0-of (r3 / recommen-01
                              :ARG1 (p / procedure
                                    :topic (a5 / and
                                          :op1 (r4 / review-01
                                                :ARG1 (t / tool
                                                      :mod (a6 / algorithm)
                                                      :ARG1-of (u / use-01
                                                            :ARG0 (c / city))))
                                          :op2 (a7 / assess-01
                                                :ARG1 t)))))
                  :time (d / date-entity
                        :month 12
                        :year 2019)))
      :ARG1-of (d2 / describe-01
            :ARG0 (p2 / publication
                  :ARG1-of (c2 / cite-01
                        :ARG2 2))))


# ::id 57
# ::snt Ensuring fairness across the many  different groups in A ustralian society will be challenging, but this cuts right to the heart of ethical AI.
(c / contrast-01
      :ARG1 (c2 / challenge-01
            :ARG2 (e / ensure-01
                  :ARG1 (f / fairness
                        :location (a / across
                              :op1 (g / group
                                    :ARG1-of (d / differ-02)
                                    :quant (m / many)
                                    :part-of (s / society
                                          :mod (c3 / continent
                                                :name (n / name
                                                      :op1 "Australia"))))))))
      :ARG2 (c4 / cut-01
            :ARG0 e
            :ARG4 (h / heart
                  :part-of (ii / intelligent-01
                        :mod (a2 / artificial)
                        :mod (e2 / ethics)))
            :mod (r / right)))


# ::id 57
# ::snt When developers  need to codify  fairness  into AI algorithms , the re are various  challenge s in managing  often inevitable  trade offs and s ometimes there’s no “right” choice  because what is considered optimal may be disputed .
(a / and
      :op1 (c / challenge-01
            :ARG2 (m / manage-01
                  :ARG1 (t / trade-off-02
                        :ARG1-of (a2 / avoid-01
                              :ARG1-of (p / possible-01
                                    :polarity -)
                              :frequency (o / often))))
            :mod (v / various))
      :op2 (c2 / choose-01
            :polarity -
            :ARG1-of (r / right-02)
            :ARG1-of (c3 / cause-01
                  :ARG0 (p2 / possible-01
                        :ARG1 (d / dispute-01
                              :ARG2 (o2 / optimal
                                    :ARG1-of (c4 / consider-01))))
                  :time o))
      :time (n / need-01
            :ARG0 (p3 / person
                  :ARG0-of (d2 / develop-02))
            :ARG1 (c5 / codify-01
                  :ARG0 p3
                  :ARG1 (f / fairness)
                  :location (a3 / algorithm
                        :mod (ii / intelligent-01
                              :mod (a4 / artificial))))))


# ::id 57
# ::snt Sometimes that will mean putting fairness ahead of profit.”   Weapons of Math Destruction, Cathy O’Neil     Humans are faced with tens of thousands of decisions each day.
(m / multi-sentence
      :snt1 (m2 / mean-01
            :ARG1 (t / that)
            :ARG2 (p / put-01
                  :ARG1 (f / fairness)
                  :ARG2 (a / ahead
                        :op1 (p2 / profit-01)))
            :frequency (s / sometimes))
      :snt2 (s2 / say-01
            :ARG0 (p3 / person
                  :name (n / name
                        :op1 "Cathy"
                        :op2 "O'Neil"))
            :ARG1 (f2 / face-01
                  :ARG0 (h / human)
                  :ARG1 (d / decide-01
                        :quant (m3 / multiple
                              :op1 10000)
                        :frequency (t2 / temporal-quantity
                              :quant 1
                              :unit (d2 / day)))))
      :snt3 (w / weapon
            :ARG2-of (d3 / destroy-01
                  :manner (m4 / mathematics))))


# ::id 57
# ::snt The guide focuses on five  key areas for developers: Accountability, Value Alignment, Explainability, Fairness and User Data Rights   [87].
(f / focus-01
      :ARG0 (g / guide)
      :ARG1 (a / area
            :quant 5
            :ARG1-of (k / key-02)
            :beneficiary (p / person
                  :ARG0-of (d / develop-02))
            :ARG1-of (m / mean-01
                  :ARG2 (a2 / and
                        :op1 (a3 / accountable-02)
                        :op2 (a4 / align-01
                              :ARG2 (v / value))
                        :op3 (e / explain-01
                              :ARG1-of (p2 / possible-01))
                        :op4 (f2 / fairness)
                        :op5 (r / right-05
                              :ARG1 (p3 / person
                                    :ARG0-of (u / use-01
                                          :ARG1 (d2 / data)))))))
      :ARG1-of (d3 / describe-01
            :ARG0 (p4 / publication
                  :ARG1-of (c / cite-01
                        :ARG2 87))))


# ::id 57
# ::snt New York has put in place an  automated decisions task force, to review key systems used by government agencies for accountability and  fairness  [2].
(p / put-03
      :ARG0 (s / state
            :name (n / name
                  :op1 "New"
                  :op2 "York"))
      :ARG1 (ii / in-place
            :domain (t / task-01
                  :ARG2 (d / decide-01)
                  :ARG1-of (a / automate-01)))
      :purpose (r / review-01
            :ARG0 t
            :ARG1 (s2 / system
                  :ARG1-of (k / key-02)
                  :ARG1-of (u / use-01
                        :ARG0 (a2 / agency
                              :mod (g / government-organization
                                    :ARG0-of (g2 / govern-01)))
                        :ARG2 (a3 / and
                              :op1 (a4 / accountable-02)
                              :op2 (f / fair-01)))))
      :ARG1-of (d2 / describe-01
            :ARG0 (p2 / publication
                  :ARG1-of (c / cite-01
                        :ARG2 2))))


# ::id 57
# ::snt The designers of algorithms need to pay  careful attention to how their systems come to a prediction and  there may be a role for government bodies in determining general boundaries and review and monitoring  processes —based on existing laws regarding discrimination —for how issues relating to  the separate but  related issues of  bias and fairness  can best be addressed and mitigated.
(a / and
      :op1 (n / need-01
            :ARG0 (p / person
                  :ARG0-of (d / design-01
                        :ARG1 (a2 / algorithm)))
            :ARG1 (a3 / attend-02
                  :ARG0 p
                  :ARG1 (t / thing
                        :manner-of (c / come-01
                              :ARG1 (s / system
                                    :poss p)
                              :ARG4 (p2 / predict-01
                                    :ARG0 p)))
                  :ARG1-of (c2 / care-04)))
      :op2 (p3 / possible-01
            :ARG1 (r / role
                  :mod (b / body
                        :mod (g / government-organization
                              :ARG0-of (g2 / govern-01)))
                  :purpose (a4 / and
                        :op1 (d2 / determine-01
                              :ARG0 b
                              :ARG1 (b2 / boundary
                                    :ARG1-of (g3 / general-02)))
                        :op2 (a5 / and
                              :op1 (r2 / review-01
                                    :ARG0 b
                                    :ARG1 (p4 / process-02))
                              :op2 (m / monitor-01
                                    :ARG0 b
                                    :ARG1 p4)
                              :ARG1-of (b3 / base-02
                                    :ARG2 (l / law
                                          :ARG1-of (e / exist-01)
                                          :topic (d3 / discriminate-02))))
                        :purpose (t2 / thing
                              :manner-of p3
                              :ARG1 (a6 / and
                                    :op1 (a7 / address-02
                                          :ARG1 (ii / issue-02
                                                :ARG1-of (r3 / relate-01
                                                      :ARG2 (ii2 / issue-02
                                                            :ARG1 (a8 / and
                                                                  :op1 (b4 / bias-01)
                                                                  :op2 (f / fair-01))
                                                            :ARG1-of (s2 / separate-01
                                                                  :ARG1-of (c3 / contrast-01
                                                                        :ARG2 (r4 / relate-01
                                                                              :ARG1 ii2)))))))
                                    :op2 (m2 / mitigate-01
                                          :ARG1 ii)
                                    :ARG1-of (h / have-degree-91
                                          :ARG2 (g4 / good-02
                                                :ARG1 a6)
                                          :ARG3 (m3 / most))))))))


# ::id 57
# ::snt Fairness    Transparency  and  explainability   Contestability   Accountability   Predicting  human  behaviour  The COMPAS sentencing tool   COMPAS is a tool used in the US to give recommendations to judges about  whether prospective parolee will re -offend.
(a / and
      :op1 (f / fairness)
      :op2 a
      :op1 (t / transparency)
      :op2 (e / explain-01)
      :op3 (c / contest-02)
      :op4 (a2 / accountable-02)
      :op5 (p / predict-01
            :ARG1 (b / behave-01
                  :ARG0 (h / human)))
      :op6 (t2 / tool
            :instrument-of (s / sentence-01)
            :domain (t3 / thing
                  :name (n / name
                        :op1 "Compass")))
      :op7 (t4 / tool
            :ARG1-of (u / use-01
                  :ARG2 (r / recommend-01
                        :ARG0 t3
                        :ARG1 (t5 / truth-value
                              :polarity-of (o / offend-03
                                    :ARG0 (p2 / person
                                          :ARG0-of (p3 / parole-01)
                                          :mod (p4 / prospective))
                                    :mod (a3 / again)))
                        :ARG2 (p5 / person
                              :ARG0-of (j / judge-01)))
                  :location (c2 / country
                        :name (n2 / name
                              :op1 "US")))
            :domain t3))


# ::id 57
# ::snt Do no harm   Regulatory and legal  compliance   Privacy protection    Fairness   Transparency  and  explainability   Figure 2.
(a / and
      :op1 (d / do-02
            :polarity -
            :mode imperative
            :ARG0 (y / you)
            :ARG1 (h / harm-01))
      :op2 (a2 / and
            :op1 (c / comply-01
                  :ARG1 (r / regulate-01))
            :op2 (c2 / comply-01
                  :ARG1 r
                  :ARG1-of (l / legal-02)))
      :op3 (p / protect-01
            :ARG1 (p2 / privacy))
      :op4 (f / fairness)
      :op5 (t / transparency)
      :op6 (a3 / and
            :op1 (p3 / possible-01
                  :ARG1 (e / explain-01))
            :op2 (p4 / possible-01
                  :ARG1 e))
      :ARG1-of (d2 / describe-01
            :ARG0 (f2 / figure
                  :mod 2)))


# ::id 57
# ::snt Fairness.
(f / fairness)


# ::id 57
# ::snt Privacy protection   Fairness   Automated  decisions  Houston teachers fired by automated system   An AI was used by the Houston school district to assess teacher performance  and  in some cases fire them.
(a / and
      :op1 (p / protect-01
            :ARG1 (p2 / privacy))
      :op2 (f / fairness)
      :op3 (d / decide-01
            :ARG1-of (a2 / automate-01))
      :op4 (f2 / fire-02
            :ARG1 (p3 / person
                  :ARG0-of (t / teach-01)
                  :location (c / city
                        :name (n / name
                              :op1 "Houston")))
            :ARG1-of (c2 / cause-01
                  :ARG0 (s / system
                        :ARG1-of (a3 / automate-01))))
      :op5 (u / use-01
            :ARG0 (d2 / district
                  :mod (s2 / school)
                  :location c)
            :ARG1 (ii / intelligent-01
                  :mod (a4 / artificial))
            :ARG2 (a5 / and
                  :op1 (a6 / assess-01
                        :ARG0 d2
                        :ARG1 (p4 / perform-02
                              :ARG0 (p5 / person
                                    :ARG0-of (t2 / teach-01))))
                  :op2 (f3 / fire-02
                        :ARG0 d2
                        :ARG1 p5
                        :time (c3 / case-04
                              :mod (s3 / some))))))


# ::id 62
# ::snt More precisely, which fairness metrics enable the identification of bi ases, for example those with a  discriminatory nature ?
(e / enable-01
      :ARG0 (m / metric
            :mod (f / fairness)
            :mod (a / amr-unknown))
      :ARG1 (ii / identify-01
            :ARG1 (a2 / affirmative-action
                  :example (o / organization
                        :ARG0-of (d / discriminate-02
                              :mod (n / nature)))))
      :ARG1-of (h / have-degree-91
            :ARG2 (p / precise)
            :ARG3 (m2 / more)))


# ::id 62
# ::snt In  Proceedings of the Conference on Fairness, Accountability, and Transparency (FAT* ’19) (2019).
(b / be-located-at-91
      :ARG2 (p / proceeding-02
            :ARG2 (c / conference
                  :name (n / name
                        :op1 "Conference"
                        :op2 "on"
                        :op3 "Fairness"
                        :op4 "Accountability"
                        :op5 "and"
                        :op6 "Transparency")
                  :time (d / date-entity
                        :year 2019)))
      :polarity (a / amr-unknown))


# ::id 62
# ::snt Principles to Promote Fairness, Ethics, Accountability and  Transparency in the Use of Artificial Intelligence and Data Analytics in Singapore ’s Financial Sector.
(p / principle
      :purpose (p2 / promote-02
            :ARG1 (a / and
                  :op1 (f / fairness)
                  :op2 (e / ethics)
                  :op3 (a2 / accountable-02)
                  :op4 (t / transparency))
            :manner (u / use-01
                  :ARG1 (a3 / and
                        :op1 (ii / intelligent-01
                              :mod (a4 / artificial))
                        :op2 (a5 / analyze-01
                              :ARG1 (d / data)))
                  :ARG2 (s / sector
                        :mod (f2 / finance)
                        :poss (c / country
                              :name (n / name
                                    :op1 "Singapore"))))))


# ::id 62
# ::snt Fairness -Aware Classifier with  Prejudice Remover Regularizer.
(a / and
      :op1 (t / thing
            :ARG0-of (c / classify-01)
            :ARG0-of (r / realize-01))
      :op2 (t2 / thing
            :ARG0-of (r2 / remove-01
                  :ARG1 (p / prejudice-01))
            :ARG0-of (r3 / regularize-01)))


# ::id 62
# ::snt ZestFinance Using AI To Bring Fairness To Mortgage Lending  (2019).
(p / publication-91
      :ARG0 (c / company
            :name (n / name
                  :op1 "ZestFinance"))
      :ARG1 (u / use-01
            :ARG0 c
            :ARG1 (a / artificial-intelligence)
            :ARG2 (b / bring-01
                  :ARG0 c
                  :ARG1 (f / fairness)
                  :ARG2 (l / lend-01
                        :ARG1 (m / mortgage-01))))
      :time (d / date-entity
            :year 2019))


# ::id 62
# ::snt Rejecting the understanding of interpretability as a monolithic concept, Lipton  introduces a continuum based on a number of logical c riteria: trust in the algorithm’s results, causality,  transferability of knowledge, information contained in the decision, fairness of the decision.
(ii / introduce-02
      :ARG0 (p / person
            :name (n / name
                  :op1 "Lipton")
            :ARG0-of (r / reject-01
                  :ARG1 (u / understand-01
                        :ARG1 (p2 / possible-01
                              :ARG1 (ii2 / interpret-01))
                        :ARG3 (c / concept
                              :mod (m / monolith)))))
      :ARG1 (c2 / continuum
            :ARG1-of (b / base-02
                  :ARG2 (n2 / number
                        :quant-of (f / formula
                              :mod (l / logic)
                              :ARG1-of (m2 / mean-01
                                    :ARG2 (a / and
                                          :op1 (t / trust-02
                                                :ARG1 (r2 / result-01
                                                      :ARG1 (a2 / algorithm)))
                                          :op2 (c3 / causality)
                                          :op3 (p3 / possible-01
                                                :ARG1 (t2 / transfer-01
                                                      :ARG1 (k / knowledge)))
                                          :op4 (ii3 / information
                                                :ARG1-of (c4 / contain-01
                                                      :ARG0 (d / decide-01)))
                                          :op5 (f2 / fair-01
                                                :ARG1 d))))))))


# ::id 62
# ::snt Exploratory works conducted by the ACPR, along with a broader analysis of the financial sector,  showed that bias detection and mitigation were at an early stage in the industry: as of now, the  emphasis is put on internal validation of AI systems and on the ir regulatory compliance, without  pushing the analysis of algorithmic fairness further than was the case with traditional methods – in  particular, the risk of reinforcing pre-existing  biases tends to be neglected.
(m / multi-sentence
      :snt1 (s / show-01
            :ARG0 (w / work-01
                  :ARG1 (e / explore-01)
                  :ARG1-of (c / conduct-01
                        :ARG0 (o / organization
                              :name (n / name
                                    :op1 "ACPR"))
                        :accompanier (a / analyze-01
                              :ARG1 (s2 / sector
                                    :mod (f / finance))
                              :ARG1-of (h / have-degree-91
                                    :ARG2 (b / broad-02
                                          :ARG1 a)
                                    :ARG3 (m2 / more)))))
            :ARG1 (b2 / be-located-at-91
                  :ARG1 (a2 / and
                        :op1 (d / detect-01
                              :ARG1 (b3 / bias-01))
                        :op2 (m3 / mitigate-01
                              :ARG1 b3))
                  :ARG2 (s3 / stage
                        :mod (e2 / early)
                        :part-of (ii / industry))))
      :snt2 (e3 / emphasize-01
            :ARG1 (a3 / and
                  :op1 (v / validate-01
                        :ARG1 (s4 / system
                              :mod (ii2 / intelligent-01
                                    :mod (a4 / artificial)))
                        :ARG1-of (ii3 / internal-02))
                  :op2 (c2 / comply-01
                        :ARG1 (r / regulate-01)))
            :time (a5 / as-of
                  :op1 (n2 / now))
            :manner (p / push-05
                  :polarity -
                  :ARG1 (a6 / analyze-01
                        :ARG1 (f2 / fairness
                              :mod (a7 / algorithm)))
                  :ARG2 (f3 / further-01
                        :ARG1 a6)
                  :ARG1-of (r2 / resemble-01
                        :polarity -
                        :ARG2 (c3 / case-04
                              :ARG1 (b4 / bias-01
                                    :ARG1-of (p2 / preexist-01))
                              :mod (m4 / method
                                    :mod (t / traditional)))))
            :example (t2 / tend-02
                  :ARG1 (r3 / risk-01
                        :ARG2 (r4 / reinforce-01
                              :ARG1 (b5 / bias-01
                                    :ARG1-of (e4 / exist-01
                                          :time (b6 / before)))))
                  :ARG2 (n3 / neglect-01
                        :ARG1 r3))))


# ::id 62
# ::snt 10   into the less -critical business processes (and those which bear little ethics and fairness risks): it can  thus be anticipated that the progressive industrialization of a dditional AI use cases in the sector will  benefit the currently very active research on those topics.
(a / and
      :li 10
      :op1 (c / concern-02
            :ARG1 (a2 / and
                  :op1 (p / process-02
                        :ARG1 (b / business)
                        :ARG1-of (h / have-degree-91
                              :ARG2 (c2 / critical-02
                                    :ARG1 p)
                              :ARG3 (l / less)))
                  :op2 (p2 / process-02
                        :ARG0-of (b2 / bear-01
                              :ARG1 (r / risk-01
                                    :ARG2 (a3 / and
                                          :op1 (e / ethics)
                                          :op2 (f / fairness))
                                    :quant (l2 / little))))))
      :op2 (p3 / possible-01
            :ARG1 (a4 / anticipate-01
                  :ARG1 (b3 / benefit-01
                        :ARG0 (ii / industrialize-01
                              :ARG1 (c3 / case-04
                                    :ARG1 (u / use-01
                                          :ARG1 (ii2 / intelligent-01
                                                :mod (a5 / artificial)))
                                    :mod (d / diverse))
                              :ARG1-of (p4 / progress-01)
                              :location (s / sector))
                        :ARG1 (r2 / research-01
                              :ARG1 a2
                              :ARG1-of (a6 / activity-06
                                    :degree (v / very)
                                    :time (c4 / current))))
                  :mod (t / thus))))


# ::id 62
# ::snt Diversity, non -discrimination and fairness   6.
(a / and
      :li 6
      :op1 (d / diversity)
      :op2 (d2 / discriminate-01
            :polarity -)
      :op3 (f / fair-01))


# ::id 62
# ::snt Those issues include social and ethical concerns in the broadest sense, and particularly  questions of fairness raised by any automated or computer -aided decision process.
(ii / include-01
      :ARG1 (a / and
            :op1 (c / concern-01
                  :ARG0 (a2 / and
                        :op1 (s / society)
                        :op2 (e / ethics))
                  :manner (s2 / sense
                        :ARG1-of (h / have-degree-91
                              :ARG2 (b / broad-02
                                    :ARG1 s2)
                              :ARG3 (m / most))))
            :op2 (q / question-01
                  :ARG1 (f / fair-01)
                  :mod (p / particular)
                  :ARG1-of (r / raise-01
                        :ARG0 (o / or
                              :op1 (p2 / process-02
                                    :ARG1 (d / decide-01)
                                    :ARG1-of (a3 / automate-01))
                              :op2 (a4 / assist-01
                                    :ARG0 (c2 / computer))
                              :mod (a5 / any)))))
      :ARG2 (ii2 / issue-02
            :mod (t / that)))


# ::id 62
# ::snt Ethics and fairness   Besides constraints stemming from sector -specific and cross -cutting regulations, ethical issues lie at  the core of the ever -increasing usage of AI in business p rocesses which impact individuals and groups  of people.
(l / lie-07
      :ARG1 (ii / issue-02
            :ARG0 (e / ethics))
      :ARG2 (c / core
            :poss (u / use-01
                  :ARG1 (ii2 / intelligent-01
                        :mod (a / artificial))
                  :ARG2 (p / problem
                        :mod (b / business)
                        :ARG0-of (ii3 / impact-01
                              :ARG1 (a2 / and
                                    :op1 (ii4 / individual)
                                    :op2 (g / group
                                          :consist-of (p2 / person)))))
                  :ARG1-of (ii5 / increase-01
                        :time (e2 / ever))))
      :ARG1-of (ii6 / instead-of-91
            :ARG2 (s / stem-01
                  :ARG1 (c2 / constrain-01)
                  :ARG2 (r / regulate-01
                        :ARG1-of (s2 / specific-02
                              :ARG2 (s3 / sector))
                        :ARG0-of (c3 / cut-01
                              :ARG0-of (c4 / cross-02)))))
      :example (a3 / and
            :op1 e
            :op2 (f / fairness)))


# ::id 62
# ::snt The presence of a statistical bias may lead to a fairness bias, but this  is neither a sufficient nor a necessary condition.
(c / contrast-01
      :ARG1 (p / possible-01
            :ARG1 (l / lead-03
                  :ARG0 (p2 / present-02
                        :ARG1 (b / bias-01
                              :ARG1 (s / statistics)))
                  :ARG2 (b2 / bias-01
                        :ARG3 (f / fair-01))))
      :ARG2 (a / and
            :op1 (s2 / suffice-01
                  :polarity -
                  :ARG0 p2)
            :op2 (n / need-01
                  :polarity -
                  :ARG1 p2)
            :domain (c2 / condition)))


# ::id 62
# ::snt Ethical considerations, such  as fairness  of processing and the  absence of discriminatory bias, have to be taken into account  in this regard.
(o / obligate-01
      :ARG2 (c / consider-02
            :ARG1 (a / and
                  :op1 (f / fairness
                        :domain (p / process-01))
                  :op2 (a2 / absent-01
                        :ARG1 (b / bias-01
                              :ARG0-of (d / discriminate-01))))
            :mod (e / ethics))
      :topic (t / this))


# ::id 62
# ::snt Besides,  counterfactual explanations constitute a particularly interesting case of post -modelling  explanatory  method insofar as they can  contribute to assessing that the appropriate data management principle  described in this document has been followed, both in terms of regulatory compliance (specifically  with respect to GDPR) and in terms of ethics and fairness.
(c / constitute-01
      :ARG0 (e / explain-01
            :mod (c2 / counterfactual))
      :ARG1 (c3 / case-04
            :ARG1 (m / method
                  :ARG0-of (e2 / explain-01)
                  :time (a / after
                        :op1 (m2 / model-01)))
            :ARG0-of (ii / interest-01
                  :mod (p / particular)
                  :ARG1-of (c4 / cause-01
                        :ARG0 (p2 / possible-01
                              :ARG1 (c5 / contribute-01
                                    :ARG0 m
                                    :ARG2 (a2 / assess-01
                                          :ARG1 (f / follow-02
                                                :ARG1 (p3 / principle
                                                      :ARG1-of (a3 / appropriate-02)
                                                      :topic (m3 / manage-01
                                                            :ARG1 (d / data))
                                                      :ARG1-of (d2 / describe-01
                                                            :ARG0 (d3 / document
                                                                  :mod (t / this))))
                                                :topic (a4 / and
                                                      :op1 (c6 / comply-01
                                                            :ARG1 (r / regulate-01)
                                                            :ARG1-of (s / specific-02
                                                                  :ARG2 (l / law
                                                                        :name (n / name
                                                                              :op1 "GDPR"))))
                                                      :op2 (a5 / and
                                                            :op1 (e3 / ethics)
                                                            :op2 (f2 / fairness))))))))))
      :mod (b / besides))


# ::id 64
# ::snt AI will improve the quality of  services and the efficiency of processes while guarantying fairness,  wellbeing, and quality of life.AI technologies  should be easily  available to promote  the efficiency  and quality of all  activities, including  SMEs, public services  and every citizen.
(m / multi-sentence
      :snt1 (ii / improve-01
            :ARG0 (ii2 / intelligent-01
                  :mod (a / artificial))
            :ARG1 (a2 / and
                  :op1 (q / quality
                        :poss (s / serve-01))
                  :op2 (e / efficient-01
                        :ARG1 (p / process-01)))
            :time (g / guarantee-01
                  :ARG0 ii2
                  :ARG1 (a3 / and
                        :op1 (f / fairness)
                        :op2 (w / well-09)
                        :op3 (q2 / quality
                              :poss (l / live-01)))))
      :snt2 (r / recommend-01
            :ARG1 (a4 / available-02
                  :ARG2 (t / technology
                        :mod (ii3 / intelligent-01))
                  :ARG1-of (e2 / easy-05)
                  :purpose (p2 / promote-02
                        :ARG0 t
                        :ARG1 (a5 / and
                              :op1 (e3 / efficient-01
                                    :ARG1 (a6 / activity-06
                                          :mod (a7 / all)
                                          :ARG2-of (ii4 / include-01
                                                :ARG1 (a8 / and
                                                      :op1 (s2 / small-molecule
                                                            :ARG1-of (s3 / special-02))
                                                      :op2 (s4 / serve-01
                                                            :ARG1-of (p3 / public-02))
                                                      :op3 (c / citizen
                                                            :mod (e4 / every))))))
                              :op2 (q3 / quality
                                    :poss a6))))))


# ::id 64
# ::snt But it should be clear that the growing usage of AI must also strengthen societal robustness by building a ADDED ECONOMIC GROWTH,  SCIENTIFIC EXCELLENCE AND  HUMAN DEVELOPMENT clear vision of the impacts of AI in democracy, privacy, security, fairness,  the labour market, governmental  and commercial transparency,  and equity.
(c / contrast-01
      :ARG2 (r / recommend-01
            :ARG1 (c2 / clear-06
                  :ARG1 (o / obligate-01
                        :ARG1 (u / use-01
                              :ARG1 (ii / intelligent-01
                                    :mod (a / artificial))
                              :ARG1-of (g / grow-01))
                        :ARG2 (s / strengthen-01
                              :ARG0 u
                              :ARG1 (r2 / robustness
                                    :mod (s2 / society))
                              :mod (a2 / also)
                              :manner (b / build-01
                                    :ARG0 u
                                    :ARG1 (a3 / and
                                          :op1 (g2 / grow-01
                                                :ARG1 (e / economy)
                                                :ARG1-of (a4 / add-02))
                                          :op2 (e2 / excel-01
                                                :mod (s3 / science))
                                          :op3 (e3 / envision-01
                                                :ARG1 (ii2 / impact-01
                                                      :ARG0 ii
                                                      :ARG1 (a5 / and
                                                            :op1 (d / democracy)
                                                            :op2 (p / privacy)
                                                            :op3 (s4 / security)
                                                            :op4 (f / fairness)
                                                            :op5 (m / market
                                                                  :mod (l / labor))
                                                            :op6 (t / transparency
                                                                  :mod (g3 / government-organization
                                                                        :ARG0-of (g4 / govern-01))
                                                                  :mod (c3 / commerce))
                                                            :op7 (e4 / equity)))
                                                :ARG1-of (c4 / clear-06)
                                                :mod (d2 / develop-02
                                                      :ARG1 (h / human))))))))))


# ::id 64
# ::snt Privacy protection,  safety, transparency, fairness and  inclusion must be ensured in the European space.
(o / obligate-01
      :ARG2 (e / ensure-01
            :ARG1 (a / and
                  :op1 (p / protect-01
                        :ARG1 (p2 / privacy))
                  :op2 (s / safe-01)
                  :op3 (t / transparency)
                  :op4 (f / fairness)
                  :op5 (ii / include-01))
            :location (s2 / space
                  :mod (c / continent
                        :name (n / name
                              :op1 "Europe")))))


# ::id 64
# ::snt With  this in mind, Portugal’s AI Strategy  will be based on the fundamental principle of not compromising  the dignity of citizens, strongly anchored by the promotion of wellbeing, fairness and quality of life.
(b / base-02
      :ARG1 (s / strategy
            :mod (ii / intelligent-01
                  :mod (a / artificial))
            :poss (c / country
                  :name (n / name
                        :op1 "Portugal")))
      :ARG2 (p / principle
            :mod (f / fundamental)
            :topic (c2 / compromise-02
                  :polarity -
                  :ARG1 (d / dignity
                        :poss (c3 / citizen)))
            :ARG1-of (a2 / anchor-01
                  :ARG2 (p2 / promote-02
                        :ARG1 (a3 / and
                              :op1 (w / well-09)
                              :op2 (f2 / fairness)
                              :op3 (q / quality
                                    :poss (l / live-01))))
                  :ARG1-of (s2 / strong-02)))
      :manner (k / keep-in-mind-08
            :ARG2 (t / this)))


# ::id 64
# ::snt AI will improve the quality of services and the efficiency of processes  while guarantying fairness, wellbeing and quality of life.Detailed Vision Promoting a better society: AI will improve the quality of services and efficiency of processes while guarantying human dignity as well as wellbeing and quality of life.
(m / multi-sentence
      :snt1 (ii / improve-01
            :ARG0 (ii2 / intelligent-01
                  :mod (a / artificial))
            :ARG1 (a2 / and
                  :op1 (q / quality
                        :poss (s / serve-01))
                  :op2 (e / efficient-01
                        :ARG1 s))
            :time (g / guarantee-01
                  :ARG0 ii2
                  :ARG1 (a3 / and
                        :op1 (f / fairness)
                        :op2 (w / well-09)
                        :op3 (q2 / quality
                              :poss (l / live-01)))))
      :snt2 (p / promote-02
            :ARG1 (s2 / society
                  :ARG1-of (h / have-degree-91
                        :ARG2 (g2 / good-02
                              :ARG1 s2)
                        :ARG3 (m2 / more)))
            :manner (ii3 / improve-01
                  :ARG0 (ii4 / intelligent-01
                        :mod a)
                  :ARG1 (a4 / and
                        :op1 (q3 / quality
                              :poss s)
                        :op2 (e2 / efficient-01
                              :ARG1 (p2 / process-02)))
                  :time (g3 / guarantee-01
                        :ARG0 ii4
                        :ARG1 (a5 / and
                              :op1 (d / dignity
                                    :mod (h2 / human))
                              :op2 w
                              :op3 q2))))
      :snt3 (e3 / envision-01
            :ARG1-of (d2 / detail-01)))


# ::id 64
# ::snt At  the same time, public policies and  decision-making processes will be  increasingly supported by evidence  and not by intuition making use of  the vast amount of administrative  data already collected for operational purposes.AI WILL IMPROVE  THE QUALITY OF  SERVICES AND THE  EFFICIENCY OF  PROCESSES, WHILE  GUARANTYING THE  HUMAN DIGNITY AS  WELL AS WELLBEING  AND QUALITY OF LIFE
24 AI PORTUGAL 2030The growing application of AI must  also strengthen societal robustness, by  building a clear vision of the impacts of AI  in democracy, privacy, security, fairness,  the labour market, governmental and  commercial transparency and equityOur main general objectives to  reach by 2030 are the following.
(m / multi-sentence
      :snt1 (a / and
            :op1 (e / enhance-01
                  :ARG0 (ii / intelligent-01
                        :mod (a2 / artificial))
                  :ARG1 (a3 / and
                        :op1 (q / quality
                              :poss (s / service))
                        :op2 (e2 / efficient-01
                              :ARG1 (p / process-02))))
            :op2 (g / guarantee-01
                  :ARG0 ii
                  :ARG1 (a4 / and
                        :op1 (q2 / quality
                              :poss (l / life))
                        :op2 (w / well-09
                              :ARG2-of (h / have-degree-91
                                    :ARG1 q2
                                    :ARG3 (e3 / equal))))))
      :snt2 (o / obligate-01
            :ARG2 (s2 / strengthen-01
                  :ARG0 (a5 / apply-02
                        :ARG1 (ii2 / intelligent-01
                              :mod (a6 / artificial))
                        :ARG1-of (g2 / grow-01))
                  :ARG1 (r / robust
                        :domain (s3 / society))
                  :manner (b / build-01
                        :ARG1 (e4 / envision-01
                              :ARG1 (ii3 / impact-01
                                    :ARG0 ii2
                                    :ARG1 (a7 / and
                                          :op1 (d / democracy)
                                          :op2 (p2 / privacy)
                                          :op3 (s4 / security)
                                          :op4 (f / fairness)
                                          :op5 (m2 / market
                                                :mod (l2 / labor))
                                          :op6 (t / transparency
                                                :mod (g3 / government-organization
                                                      :ARG0-of (g4 / govern-01)))
                                          :op7 (e5 / equity)))
                              :ARG1-of (c / clear-06)))))
      :snt3 (o2 / objective
            :ARG1-of (g5 / general-02)
            :ARG1-of (r2 / reach-01
                  :ARG0 (w2 / we)
                  :time (b2 / by
                        :op1 (d2 / date-entity
                              :year 2030)))
            :ARG1-of (f2 / follow-04)
            :time (t2 / time
                  :ARG1-of (s5 / same-01)))
      :snt4 (c2 / contrast-01
            :ARG1 (s6 / support-01
                  :ARG0 (e6 / evidence-01)
                  :ARG1 (a8 / and
                        :op1 (p3 / policy-01
                              :ARG2 (p4 / public-02))
                        :op2 (p5 / process-02
                              :ARG1 (d3 / decide-01)))
                  :manner (ii4 / increase-01))
            :ARG2 (s7 / support-01
                  :polarity -
                  :ARG0 (ii5 / intuition)
                  :ARG1 (u / use-01
                        :ARG1 (a9 / amount
                              :mod (v / vast)
                              :quant-of (d4 / data
                                    :mod (a10 / administrate-01)
                                    :ARG1-of (c3 / collect-01
                                          :time (a11 / already)
                                          :purpose (o3 / operate-01)))))))
      :time (d5 / date-entity
            :year 2030
            :month 9
            :day 24))


# ::id 64
# ::snt The growing application of AI must also strengthen societal robustness by  building a clear vision of the impacts of AI on democracy, privacy, security,  fairness, the labour market, governmental and commercial transparency  and equity.
(o / obligate-01
      :ARG2 (s / strengthen-01
            :ARG0 (a / apply-02
                  :ARG1 (ii / intelligent-01
                        :mod (a2 / artificial))
                  :ARG1-of (g / grow-01))
            :ARG1 (r / robustness
                  :mod (s2 / society))
            :manner (b / build-01
                  :ARG0 a
                  :ARG1 (e / envision-01
                        :ARG1 (ii2 / impact-01
                              :ARG0 ii
                              :ARG1 (a3 / and
                                    :op1 (d / democracy)
                                    :op2 (p / privacy)
                                    :op3 (s3 / security)
                                    :op4 (f / fairness)
                                    :op5 (m / market
                                          :mod (l / labor))
                                    :op6 (t / transparency
                                          :mod (g2 / government-organization
                                                :ARG0-of (g3 / govern-01))
                                          :mod (c / commerce))
                                    :op7 (e2 / equity)))
                        :ARG1-of (c2 / clear-06)))
            :mod (a4 / also)))


# ::id 64
# ::snt employment, democracy,  and fairness) through focused  observations and by promoting  research in the respective scientific  areas.
(a / and
      :op1 (e / employ-01)
      :op2 (d / democracy)
      :op3 (f / fairness)
      :manner (a2 / and
            :op1 (o / observe-01
                  :ARG1-of (f2 / focus-01))
            :op2 (p / promote-01
                  :ARG1 (r / research-01
                        :ARG1 (a3 / area
                              :mod (s / science)
                              :mod (r2 / respective))))))


# ::id 64
# ::snt Important research lines are emerging, such as: Transparent AI: giving algorithms the ability to explain their own decisions and provide a high level and  adaptive account of their workings to promote fairness and accountability;  Emotional AI: algorithms will utilise emotions to achieve better decisions; Autonomous AI: important not only in the automotive sector but also in information systems, cybersecurity, smart cities, industry, etc.
(e / emerge-02
      :ARG0 (l / line
            :ARG1-of (ii / important-01)
            :mod (r / research-01)
            :example (a / and
                  :op1 (a2 / artificial-physical-instrument
                        :name (n / name
                              :op1 "Transparent"
                              :op2 "AI")
                        :ARG1-of (m / mean-01
                              :ARG2 (g / give-01
                                    :ARG1 (c / capable-01
                                          :ARG1 (a3 / algorithm)
                                          :ARG2 (a4 / and
                                                :op1 (e2 / explain-01
                                                      :ARG0 a3
                                                      :ARG1 (d / decide-01
                                                            :ARG0 a3))
                                                :op2 (p / provide-01
                                                      :ARG0 a3
                                                      :ARG1 (a5 / account-01
                                                            :ARG1 (w / work-01
                                                                  :ARG0 a3)
                                                            :mod (l2 / level
                                                                  :ARG1-of (h / high-02))
                                                            :ARG1-of (a6 / adapt-01)))
                                                :purpose (p2 / promote-02
                                                      :ARG0 a3
                                                      :ARG1 (a7 / and
                                                            :op1 (f / fairness)
                                                            :op2 (a8 / accountable-02)))))
                                    :ARG2 a3)))
                  :op2 (a9 / artificial-physical-instrument
                        :name (n2 / name
                              :op1 "Empheric"
                              :op2 "AI")
                        :ARG1-of (m2 / mean-01
                              :ARG2 (u / utilize-01
                                    :ARG0 (a10 / algorithm)
                                    :ARG1 (e3 / emotion)
                                    :ARG2 (a11 / achieve-01
                                          :ARG0 a10
                                          :ARG1 (h2 / have-degree-91
                                                :ARG1 (d2 / decide-01)
                                                :ARG2 (g2 / good-02
                                                      :ARG1 d2)
                                                :ARG3 (m3 / more))))))
                  :op3 (a12 / artificial-physical-instrument
                        :name (n3 / name
                              :op1 "autonomous"
                              :op2 "AI")
                        :ARG1-of (ii2 / important-01
                              :ARG2 (a13 / and
                                    :op1 (s / sector
                                          :mod (a14 / automotive))
                                    :op2 (a15 / and
                                          :op1 (s2 / system
                                                :mod (ii3 / information))
                                          :op2 (c2 / cyber)
                                          :op3 (c3 / city
                                                :ARG1-of (s3 / smart-06))
                                          :op4 (ii4 / industry)
                                          :op5 (e4 / et-cetera))))))))


# ::id 64
# ::snt Society will demand  transparency (the ability to explain the decisions) and auditability (the ability to trace the flow  of decisions and actions from  humans to algorithm) in order to  promote safety and ethical principles, including privacy protection  and fairness.
(d / demand-01
      :ARG0 (s / society)
      :ARG1 (a / and
            :op1 (t / transparency
                  :ARG1-of (m / mean-01
                        :ARG2 (p / possible-01
                              :ARG1 (e / explain-01
                                    :ARG1 (t2 / thing
                                          :ARG1-of (d2 / decide-01))))))
            :op2 (a2 / audit-01
                  :ARG1-of (m2 / mean-01
                        :ARG2 (p2 / possible-01
                              :ARG1 (t3 / trace-02
                                    :ARG1 (f / flow-01
                                          :ARG1 (a3 / and
                                                :op1 t2
                                                :op2 (a4 / act-02))
                                          :source (h / human)
                                          :destination (a5 / algorithm)))))))
      :purpose (p3 / promote-02
            :ARG0 s
            :ARG1 (a6 / and
                  :op1 (s2 / safe-01)
                  :op2 (p4 / principle
                        :mod (e2 / ethics)
                        :ARG2-of (ii / include-01
                              :ARG1 (a7 / and
                                    :op1 (p5 / protect-01
                                          :ARG1 (p6 / privacy))
                                    :op2 (f2 / fair-01)))))))


# ::id 65
# ::snt This effort uncovered a growing consensus around eight key thematic trends: privacy, accountability, safety and security, transparency and explainability, fairness and non-discrimination, human control of technology, professional responsibility, and promotion of human values.
(u / uncover-01
      :ARG0 (e / effort-01
            :mod (t / this))
      :ARG1 (c / consensus
            :ARG1-of (g / grow-01)
            :topic (t2 / trend
                  :mod (t3 / theme)
                  :ARG1-of (k / key-02)
                  :consist-of (a / and
                        :op1 (p / private-02)
                        :op2 (a2 / accountable-02)
                        :op3 (s / safe-01)
                        :op4 (s2 / security)
                        :op5 (a3 / and
                              :op1 (t4 / transparency)
                              :op2 (e2 / explain-01
                                    :ARG1-of (p2 / possible-01)))
                        :op6 (a4 / and
                              :op1 (f / fairness)
                              :op2 (d / discriminate-01
                                    :polarity -))
                        :op7 (c2 / control-01
                              :ARG0 (h / human)
                              :ARG1 (t5 / technology))
                        :op8 (r / responsible-03
                              :mod (p3 / professional))
                        :op9 (p4 / promote-02
                              :ARG1 (v / value
                                    :mod h))))))


# ::id 66
# ::snt This effort uncovered a growing consensus around eight key thematic trends: privacy, accountability, safety and security, transparency and explainability, fairness and non-discrimination, human control of technology, professional responsibility, and promotion of human values.
(u / uncover-01
      :ARG0 (e / effort-01
            :mod (t / this))
      :ARG1 (c / consensus
            :ARG1-of (g / grow-01)
            :topic (t2 / trend
                  :mod (t3 / theme)
                  :ARG1-of (k / key-02)
                  :consist-of (a / and
                        :op1 (p / private-02)
                        :op2 (a2 / accountable-02)
                        :op3 (s / safe-01)
                        :op4 (s2 / security)
                        :op5 (a3 / and
                              :op1 (t4 / transparency)
                              :op2 (e2 / explain-01
                                    :ARG1-of (p2 / possible-01)))
                        :op6 (a4 / and
                              :op1 (f / fairness)
                              :op2 (d / discriminate-01
                                    :polarity -))
                        :op7 (c2 / control-01
                              :ARG0 (h / human)
                              :ARG1 (t5 / technology))
                        :op8 (r / responsible-03
                              :mod (p3 / professional))
                        :op9 (p4 / promote-02
                              :ARG1 (v / value
                                    :mod h))))))


# ::id 67
# ::snt To  ensure this, the CEPEJ has underli ned the importance of securing the quality and security of judicial decisions  and data, as well as the transparency, impartiality and fairness of data processing methods.
(e / ensure-01
      :ARG0 (o / organization
            :name (n / name
                  :op1 "CEPEJ"))
      :ARG1 (ii / important-01
            :ARG1 (s / secure-01
                  :ARG1 (a / and
                        :op1 (a2 / and
                              :op1 (q / quality
                                    :poss (t / thing
                                          :ARG1-of (d / decide-01
                                                :ARG0 (j / judiciary))))
                              :op2 (s2 / security
                                    :poss t))
                        :op2 (a3 / and
                              :op1 (t2 / transparency)
                              :op2 (ii2 / impartiality)
                              :op3 (f / fairness)
                              :domain (m / method
                                    :instrument-of (p / process-01
                                          :ARG1 (d2 / data))))))))


# ::id 67
# ::snt To this end, they should  pay due regard to the need to ensure the quality and security of j udicial decisions and data,  as well as the transparency, impartiality and fairness of data processing methods.
(r / recommend-01
      :ARG1 (p / pay-01
            :ARG0 (t / they)
            :ARG1 (r2 / regard
                  :mod (d / due))
            :ARG2 (n / need-01
                  :ARG1 (e / ensure-01
                        :ARG0 t
                        :ARG1 (a / and
                              :op1 (q / quality
                                    :poss (a2 / and
                                          :op1 (t2 / thing
                                                :ARG1-of (d2 / decide-01)
                                                :mod (j / judiciary))
                                          :op2 (d3 / data)))
                              :op2 (s / security
                                    :poss a2)
                              :op3 (a3 / and
                                    :op1 (t3 / transparency
                                          :poss (m / method
                                                :instrument-of (p2 / process-01
                                                      :ARG1 (d4 / data))))
                                    :op2 (ii / impartiality
                                          :poss m)
                                    :op3 (f / fairness
                                          :poss m))))))
      :purpose (e2 / end
            :mod (t4 / this)))


# ::id 67
# ::snt 41 o Member States should subject the public procurement of AI systems to adequate oversight  mechanisms:   ▪ Member States should subject their public procurement processes to legally binding  requirements that ensure the responsible use of AI in the public sector by safeguarding  compliance with the above -mentioned principles, including transparency, fairness,  responsibility and accountability.
(m / multi-sentence
      :snt1 (r / recommend-01
            :li 41
            :ARG1 (s / subject-01
                  :ARG0 (s2 / state
                        :ARG0-of (h / have-org-role-91
                              :ARG2 (m2 / member)))
                  :ARG1 (p / procure-01
                        :ARG1 (s3 / system
                              :mod (ii / intelligent-01
                                    :mod (a / artificial)))
                        :ARG1-of (p2 / public-02))
                  :ARG2 (m3 / mechanism
                        :ARG0-of (o / oversee-01)
                        :mod (a2 / adequate))))
      :snt2 (r2 / recommend-01
            :ARG1 (s4 / subject-01
                  :ARG0 (s5 / state
                        :ARG0-of h
                        :ARG2 (m4 / member)))
            :ARG1 (p3 / process-02
                  :ARG0 s5
                  :ARG1 (p4 / procure-01
                        :ARG0 s5
                        :ARG1-of (p5 / public-02)))
            :ARG2 (r3 / require-01
                  :ARG0-of (e / ensure-01
                        :ARG1 (u / use-01
                              :ARG1 (ii2 / intelligent-01
                                    :mod a))
                        :ARG2 (s6 / sector
                              :ARG1-of (p6 / public-02))
                        :ARG1-of (r4 / responsible-02))
                  :manner (s7 / safeguard-01
                        :ARG0 s5
                        :ARG1 (c / comply-01
                              :ARG1 (p7 / principle
                                    :ARG1-of (m5 / mention-01
                                          :location (a3 / above))
                                    :ARG2-of (ii3 / include-01
                                          :ARG1 (a4 / and
                                                :op1 (t / transparency)
                                                :op2 (f / fairness)
                                                :op3 r4)
                                          :op4 (a5 / accountable-02)))))))
      :ARG1-of (b / bind-01
            :ARG0 (l / law)))


# ::id 67
# ::snt The processing of personal data at any stage, including data sets, of an AI sys tem’s  lifecycle must be based on the principles set out under the Convention 108+ (including  fairness and transparency, proportionality, lawfulness of the processing, quality of data, right  not to be subject to purely automated decisions and other rights o f the data subject, data  security, accountability, impact assessments and privacy by design).
(o / obligate-01
      :ARG2 (b / base-02
            :ARG1 (p / process-01
                  :ARG1 (d / data
                        :ARG1-of (p2 / personal-02))
                  :time (s / stage
                        :mod (a / any)
                        :ARG2-of (ii / include-01
                              :ARG1 (s2 / set
                                    :consist-of (d2 / data)))
                        :part-of (l / lifecycle
                              :poss (s3 / system
                                    :ARG0-of (a2 / automate-01)
                                    :mod (a3 / artificial)))))
            :ARG2 (p3 / principle
                  :ARG1-of (s4 / set-out-06
                        :ARG0 (t / treaty
                              :name (n / name
                                    :op1 "Convention"
                                    :op2 108
                                    :op3 "+")))
                  :ARG2-of (ii2 / include-01
                        :ARG1 (a4 / and
                              :op1 (a5 / and
                                    :op1 (f / fairness)
                                    :op2 (t2 / transparency))
                              :op2 (p4 / proportionality)
                              :op3 (l2 / lawfulness
                                    :domain (p5 / process-01))
                              :op4 (q / quality
                                    :poss (d3 / data))
                              :op5 (r / right-05
                                    :ARG2 (s5 / subject
                                          :mod d3)
                                    :ARG1-of a2
                                    :ARG1-of (p6 / pure-02)))
                        :op6 (r2 / right-05
                              :ARG2 (s6 / security
                                    :mod d3)
                              :mod (o2 / other))
                        :op7 (a6 / accountable-02)
                        :op8 (a7 / assess-01
                              :ARG1 (ii3 / impact-01))
                        :op9 (p7 / privacy
                              :ARG1-of (d4 / design-01))))))


# ::id 67
# ::snt the CoE study by F. Zuiderveen Borgesius, Discrimination, artificial intelligence, and algorithmic decision -making,  2018, at: https://rm.coe.int/discrimination -artificial -intelligence -and-algorithmic -decision -making/1680925d73 ; Joy  Buolamwini, Timnit Gebru; Proceedings of the 1st Conference on Fairness, Accountability and Transparency, PMLR 81:77 91, 2018.
(b / be-located-at-91
      :ARG1 (p / publication-91
            :ARG0 (a / and
                  :op1 (p2 / person
                        :name (n / name
                              :op1 "F."
                              :op2 "Zuiderveen"
                              :op3 "Borgesius"))
                  :op2 (p3 / person
                        :name (n2 / name
                              :op1 "Joy"
                              :op2 "Buolamwini"))
                  :op3 (p4 / person
                        :name (n3 / name
                              :op1 "Timnit"
                              :op2 "Gebru")))
            :ARG1 (s / study-01
                  :ARG1 (a2 / and
                        :op1 (d / discriminate-02)
                        :op2 (ii / intelligent-01
                              :mod (a3 / artificial))
                        :op3 (d2 / decide-01
                              :mod (a4 / algorithm)))
                  :ARG4 (o / organization
                        :name (n4 / name
                              :op1 "Church"
                              :op2 "of"
                              :op3 "England"))
                  :time (d3 / date-entity
                        :year 2018))
            :ARG4 (u / url-entity
                  :value "https://rm.coe.int/discrimination -artificial-intelligence -and-algorithmic-decision-making/1680925d73")
            :ARG7 (p5 / publication
                  :name (n5 / name
                        :op1 "Proceeds"
                        :op2 "of"
                        :op3 "the"
                        :op4 "1st"
                        :op5 "Conference"
                        :op6 "on"
                        :op7 "Fairness,"
                        :op8 "Accountability"
                        :op9 "and"
                        :op10 "Transparency"))
            :ARG4 (j / journal
                  :name (n6 / name
                        :op1 "MPLR"))
            :ARG7 (a5 / and
                  :op1 81
                  :op2 77
                  :op11 91)
            :time (d4 / date-entity
                  :year 2018)))


# ::id 67
# ::snt Including criteria such as equality, fairness, accountabil ity and transparency in AI -related public procurement  processes is key168, and introducing legal safeguards to this end can serve two purposes.
(a / and
      :li 168
      :op1 (k / key-02
            :ARG1 (ii / include-01
                  :ARG1 (c / criteria
                        :example (a2 / and
                              :op1 (e / equal-01)
                              :op2 (f / fairness)
                              :op3 (a3 / accountable-02)
                              :op4 (t / transparency
                                    :topic (p / process-02
                                          :ARG1 (p2 / procure-01
                                                :ARG1-of (p3 / public-02))
                                          :ARG1-of (r / relate-01
                                                :ARG2 (ii2 / intelligent-01
                                                      :mod (a4 / artificial)))))))))
      :op2 (p4 / possible-01
            :ARG1 (s / serve-01
                  :ARG0 (ii3 / introduce-02
                        :ARG1 (s2 / safeguard-01
                              :ARG1-of (l / legal-02))
                        :ARG2 (e2 / end
                              :mod (t2 / this)))
                  :ARG1 (p5 / purpose
                        :quant 2))))


# ::id 67
# ::snt However, it also points to their limitations, as AI also paves the way for  new  types of unfair differentiation that escape current laws, suggesting the need for additional (sectoral) regulation to  protect fairness and human rights in the context of AI.
(c / contrast-01
      :ARG2 (p / point-01
            :ARG0 (ii / it)
            :ARG2 (l / limit-01
                  :ARG1 (t / they))
            :mod (a / also)
            :ARG1-of (c2 / cause-01
                  :ARG0 (p2 / pave-02
                        :ARG0 (ii2 / intelligent-01
                              :mod (a2 / artificial))
                        :ARG1 (w / way)
                        :ARG2 (t2 / type
                              :ARG1-of (n / new-01)
                              :mod (d / differentiate-01
                                    :ARG1-of (f / fair-01
                                          :polarity -))
                              :ARG0-of (e / escape-01
                                    :ARG1 (l2 / law
                                          :time (c3 / current))))
                        :mod (a3 / also)
                        :ARG0-of (s / suggest-01
                              :ARG1 (n2 / need-01
                                    :ARG1 (r / regulate-01
                                          :mod (s2 / sector)
                                          :ARG1-of (a4 / add-02)
                                          :purpose (p3 / protect-01
                                                :ARG1 (a5 / and
                                                      :op1 (f2 / fairness)
                                                      :op2 (r2 / right-05
                                                            :ARG1 (h / human)))
                                                :topic ii2))))))))


# ::id 67
# ::snt The principles of  privacy, justice and fairness showed the least variation across Council of Europe’s member States, observers  and the rest of the world, and hence the highest degree of cross -geographical and cross -cultural stability.
(s / show-01
      :ARG0 (p / principle
            :example (a / and
                  :op1 (p2 / private-02)
                  :op2 (j / justice)
                  :op3 (f / fairness)))
      :ARG1 (h / have-degree-91
            :ARG1 p
            :ARG2 (v / vary-01
                  :ARG1 p
                  :location (a2 / and
                        :op1 (s2 / state
                              :ARG0-of (h2 / have-org-role-91
                                    :ARG1 (o / organization
                                          :name (n / name
                                                :op1 "Council"
                                                :op2 "of"
                                                :op3 "Europe"))
                                    :ARG2 (m / member)))
                        :op2 (p3 / person
                              :ARG0-of (o2 / observe-01))
                        :op3 (r / rest
                              :part-of (w / world))))
            :ARG3 (l / least))
      :ARG0-of (c / cause-01
            :ARG1 (h3 / have-degree-91
                  :ARG1 (s3 / stability
                        :mod (c2 / cross-02
                              :ARG1 (a3 / and
                                    :op1 (g / geography)
                                    :op2 (c3 / culture))))
                  :ARG2 (h4 / high-02
                        :ARG1 s3)
                  :ARG3 (m2 / most))))


# ::id 67
# ::snt Non -Discrimination, Gender Equality151, Fairness and Diversity   105.
(a / and
      :op1 (d / discriminate-02
            :polarity -)
      :op2 (e / equal-01
            :ARG3 (g / gender))
      :op3 (f / fairness)
      :op4 (d2 / diversity)
      :li 105)


# ::id 67
# ::snt 24  The problematic use of AI systems (such as the COMPAS system used in the US) was demonstrated by several studies,  including the Dartmouth study on the accuracy, fairness, and limits of predicting recidivism by Julia Dressel and Hany Farid,   Science Adva nces  17 Jan 2018, Vol.
(d / demonstrate-01
      :li 24
      :ARG0 (s / study-01
            :quant (s2 / several)
            :ARG2-of (ii / include-91
                  :ARG1 (s3 / study-01
                        :ARG0 (u / university
                              :name (n / name
                                    :op1 "Dartmouth"))
                        :ARG1 (p / predict-01
                              :ARG1 (a / and
                                    :op1 (a2 / accurate)
                                    :op2 (f / fair-01)
                                    :op3 (l / limit-01))))))
      :ARG1 (u2 / use-01
            :ARG1 (s4 / system
                  :mod (ii2 / intelligent-01
                        :mod (a3 / artificial))
                  :example (s5 / system
                        :name (n2 / name
                              :op1 "Compass")
                        :ARG1-of (u3 / use-01
                              :location (c / country
                                    :name (n3 / name
                                          :op1 "US")))))
            :ARG1-of (p2 / problematic-02))
      :ARG1-of (d2 / describe-01
            :ARG0 (p3 / publication-91
                  :ARG0 (a4 / and
                        :op1 (p4 / person
                              :name (n4 / name
                                    :op1 "Julia"
                                    :op2 "Dressel"))
                        :op2 (p5 / person
                              :name (n5 / name
                                    :op1 "Hany"
                                    :op2 "Farid")))
                  :ARG4 (j / journal
                        :name (n6 / name
                              :op1 "Science"
                              :op2 "Adva"
                              :op3 "Nces"))
                  :time (d3 / date-entity
                        :day 17
                        :month 1
                        :year 2018))))


# ::id 67
# ::snt The European Commission for the Efficiency of Justice (CEPEJ) adopted in December 2018 the European Ethical  Charter for the use of artificial intelligence in judicial systems75 which sets five key principles (respect of  fundamental rights, non -discriminatio n, quality and security, transparency, impartiality and fairness, "under  the control" of the user) for the use of AI systems in this field.
(a / adopt-01
      :ARG0 (o / organization
            :name (n / name
                  :op1 "European"
                  :op2 "Commission"
                  :op3 "for"
                  :op4 "the"
                  :op5 "Efficiency"
                  :op6 "of"
                  :op7 "Justice"))
      :ARG1 (c / charter
            :mod (e / ethics)
            :mod (u / use-01
                  :ARG1 (ii / intelligent-01
                        :mod (a2 / artificial))
                  :ARG2 (s / system
                        :mod (j / judiciary)))
            :ARG0-of (s2 / set-02
                  :ARG1 (p / principle
                        :quant 5
                        :ARG1-of (k / key-02)
                        :ARG1-of (m / mean-01
                              :ARG2 (a3 / and
                                    :op1 (r / respect-01
                                          :ARG1 (r2 / right-05
                                                :mod (f / fundamental)))
                                    :op2 (d / discriminate-01
                                          :polarity -)
                                    :op3 (a4 / and
                                          :op1 (q / quality)
                                          :op2 (s3 / security))
                                    :op4 (t / transparency)
                                    :op5 (a5 / and
                                          :op1 (ii2 / impartiality)
                                          :op2 (f2 / fairness))
                                    :ARG1-of (c2 / control-01
                                          :ARG0 (p2 / person
                                                :ARG0-of (u2 / use-01))))))
                  :purpose u2
                  :ARG1 (s4 / system
                        :mod ii)
                  :ARG2 (f3 / field
                        :mod (t2 / this))))
      :ARG1-of (d2 / describe-01
            :ARG0 (p3 / publication
                  :ARG1-of (c3 / cite-01
                        :ARG2 75)))
      :time (d3 / date-entity
            :month 12
            :year 2018))


# ::id 72
# ::snt 4: transparency, impartiality and fairness  ................................ ...............  14  II.6 Indicators for Principle no.
(a / and
      :op1 (a2 / and
            :op1 (t / transparency)
            :op2 (ii / impartiality)
            :op3 (f / fairness)
            :li 4)
      :op2 (t2 / thing
            :li 14
            :ARG0-of (ii2 / indicate-01
                  :ARG1 (p / principle
                        :mod (n / no.6))))
      :op3 (t3 / thing
            :li 14
            :ARG0-of ii2))


# ::id 72
# ::snt However, there  may be a possibility of demanding that key subsets of information about the algorithms be provided to the public, for  example which variables are in use, which goals the algorithms are being optimised for, the training data and average  values and standard deviations of the results produced, or the amount and type of data being processed by the algorithm.’”  Or even the sugge stions appearing on page 117 of the aforementioned “AI for Humanity” report drafted by Mr Cédric  Villani, a member of the French National Assembly, as part of a mission assigned to him by the Prime Minister of the  French Republic: “The auditors may be sati sfied with simply checking the fairness and equity of a programme (doing only  what is required of them), by submitting a variety of false input data, for example, or by creating a large quantity of syste m  user profiles according to precise guidelines.” In addition, there are also the statements in the report by the House of Lords,  “AI in the UK: ready, willing and able?”, paragraphs 92, 96 -99.
(m / multi-sentence
      :snt1 (c / contrast-01
            :ARG2 (p / possible-01
                  :ARG1 (d / demand-01
                        :ARG1 (p2 / provide-01
                              :ARG1 (s / subset
                                    :ARG1-of (k / key-02)
                                    :consist-of (ii / information
                                          :topic (a / algorithm)
                                          :example (a2 / and
                                                :op1 (u / use-01
                                                      :ARG1 (v / variable))
                                                :op2 (o / optimize-01
                                                      :ARG1 (a3 / algorithm))
                                                :op3 (a4 / and
                                                      :op1 (d2 / data
                                                            :purpose (t / train-01))
                                                      :op2 (a5 / and
                                                            :op1 (v2 / value
                                                                  :ARG1-of (a6 / average-01))
                                                            :op2 (s2 / standard-deviation))
                                                      :op3 (a7 / and
                                                            :op1 (a8 / amount
                                                                  :quant-of (d3 / data
                                                                        :ARG1-of (p3 / process-01
                                                                              :ARG0 a3)))
                                                            :op2 (t2 / type
                                                                  :mod d3))))))
                              :ARG2 (p4 / public)))))
      :snt2 (a9 / and
            :op2 (s3 / state-01
                  :ARG0 (r / report-01
                        :ARG0 (g / government-organization
                              :name (n / name
                                    :op1 "House"
                                    :op2 "of"
                                    :op3 "Lords"))
                        :ARG1 (q / quote-01
                              :ARG1 (a10 / and
                                    :op1 (r2 / ready-02
                                          :ARG1 (ii2 / intelligent-01
                                                :mod (a11 / artificial)
                                                :location (c2 / country
                                                      :name (n2 / name
                                                            :op1 "UK"))))
                                    :op2 (w / will-02
                                          :ARG1 ii2)
                                    :op3 (p5 / possible-01
                                          :ARG1 ii2)
                                    :polarity (a12 / amr-unknown))))
                  :location (p6 / paragraph
                        :mod (v3 / value-interval
                              :op1 92
                              :op2 96)
                        :part-of (r3 / report-01
                              :ARG1 (b / benefit-01
                                    :ARG0 ii2
                                    :ARG1 (h / humanity))
                              :ARG1-of (d4 / draft-01
                                    :ARG0 (p7 / person
                                          :name (n3 / name
                                                :op1 "Mr"
                                                :op2 "Cédric"
                                                :op3 "Villani")
                                          :ARG0-of (h2 / have-org-role-91
                                                :ARG1 g
                                                :name (n4 / name
                                                      :op1 "French"
                                                      :op2 "National"
                                                      :op3 "Assembly"))
                                          :ARG2 (m2 / member))))))))


# ::id 72
# ::snt - Fairness   The issue of the fairness of artificial intelligence intersects with that of non -discrimination referred to in Principle   no.
(m / multi-sentence
      :snt1 (f / fairness)
      :snt2 (ii / intersect-01
            :ARG0 (ii2 / issue-02
                  :ARG0 (f2 / fairness
                        :poss (ii3 / intelligent-01
                              :mod (a / artificial))))
            :ARG1 (d / discriminate-02
                  :polarity -
                  :ARG1-of (r / refer-01
                        :mod (ii4 / in-principle)))))


# ::id 72
# ::snt The “by design” approach is made necessary by the place occupied in our societie s by  digital devices and algorithms which apply  programming  rules to the letter , unlike judges, for example,  who at least implicitly take considerations of fairness  into account.
(m / make-02
      :ARG0 (p / place
            :ARG1-of (o / occupy-01
                  :ARG0 (a / and
                        :op1 (d / device
                              :mod (d2 / digit))
                        :op2 (a2 / algorithm)
                        :ARG0-of (a3 / apply-02
                              :ARG1 (r / rule
                                    :ARG1-of (p2 / program-01))
                              :ARG2 (l / letter))
                        :ARG1-of (r2 / resemble-01
                              :polarity -
                              :ARG2 (p3 / person
                                    :ARG0-of (j / judge-01)
                                    :ARG0-of (c / consider-02
                                          :ARG1 (f / fairness)
                                          :ARG2-of (a4 / account-01)
                                          :manner (ii / implicit
                                                :mod (a5 / at-least)))
                                    :ARG0-of (e / exemplify-01))))
                  :location (s / society
                        :poss (w / we))))
      :ARG1 (n / need-01
            :ARG1 (a6 / approach-02
                  :manner (d3 / design-01))))


# ::id 72
# ::snt 4 of the Charter: “Principle of transparency, impartiality and fairness: make data  processing methods accessible and understandable, authorise external audits.”       50Semantic alteration  does not include, fo r example , the exclusion of sensi tive data, such as those whose removal has  previously been  envisag ed (see.
(m / multi-sentence
      :snt1 (p / principle
            :topic (a / and
                  :op1 (t / transparency)
                  :op2 (ii / impartiality)
                  :op3 (f / fairness))
            :ARG1-of (ii2 / include-91
                  :ARG2 (a2 / and
                        :op1 (m2 / make-02
                              :ARG1 (a3 / and
                                    :op1 (p2 / possible-01
                                          :ARG1 (a4 / access-01
                                                :ARG1 (m3 / method
                                                      :instrument-of (p3 / process-01
                                                            :ARG1 (d / data)))))
                                    :op2 (p4 / possible-01
                                          :ARG1 (u / understand-01
                                                :ARG1 m3))))
                        :op2 (a5 / authorize-01
                              :ARG1 (a6 / audit-01
                                    :mod (e / external)))))
            :ARG1-of (ii3 / include-91
                  :polarity -
                  :ARG2 (a7 / alter-01
                        :ARG1 (s / semantics))
                  :ARG0-of (e2 / exemplify-01
                        :ARG1 (e3 / exclude-01
                              :ARG1 (d2 / data
                                    :mod (s2 / sensitive)
                                    :example (d3 / data
                                          :ARG1-of (r / remove-01
                                                :ARG1-of (e4 / envisage-01
                                                      :time (p5 / previous)))))))))
      :snt2 (s3 / see-01
            :mode imperative
            :ARG0 (y / you)))


# ::id 72
# ::snt 4: transparency, impartiality and fairness     Principle  no.
(a / and
      :li 4
      :op1 (t / transparency)
      :op2 (ii / impartiality)
      :op3 (f / fairness)
      :op4 (p / principle)
      :op5 (n / no))


# ::id 72
# ::snt - Principle of transpa rency, impartiality and fairness: make data processing methods accessible and  understandable, authorise external audits.
(p / principle
      :topic (a / and
            :op1 (r / renounce-01
                  :ARG0 (c / company
                        :name (n / name
                              :op1 "Transpa")))
            :op2 (ii / impartiality)
            :op3 (f / fairness))
      :ARG1-of (m / mean-01
            :ARG2 (a2 / and
                  :op1 (m2 / make-02
                        :ARG1 (a3 / and
                              :op1 (p2 / possible-01
                                    :ARG1 (a4 / access-01
                                          :ARG1 (m3 / method
                                                :instrument-of (p3 / process-01
                                                      :ARG1 (d / data)))))
                              :op2 (p4 / possible-01
                                    :ARG1 (u / understand-01
                                          :ARG1 m3))))
                  :op2 (a5 / authorize-01
                        :ARG1 (a6 / audit-01
                              :mod (e / external))))))


# ::id 72
# ::snt p.6 "Furthermore, depending on the specific application various aspects (safety, fairness, privacy, security) have  different relevance which must be considered by such a labelling scheme ".
(a / and
      :op2 (r / relevant-01
            :ARG1 (a2 / aspect
                  :mod (v / various)
                  :example (a3 / and
                        :op1 (s / safe-01)
                        :op2 (f / fairness)
                        :op3 (p / privacy)
                        :op4 (s2 / security)))
            :ARG1-of (d / differ-02)
            :ARG1-of (c / consider-02
                  :ARG0 (s3 / scheme-01
                        :ARG1 (l / label-01
                              :mod (s4 / such)))
                  :ARG2-of (o / obligate-01))
            :ARG0-of (d2 / depend-01
                  :ARG1 (a4 / apply-02
                        :ARG1-of (s5 / specific-02))))
      :mod (f2 / furthermore)
      :ARG1-of (c2 / cite-01
            :ARG2 6))


# ::id 72
# ::snt - FAT-ML: artificial intelligence that is fair, accountable and transparent      In the study of the behaviour of algorithms, there is some consensus for focusing on the fair, responsible and  transparent character of artificial intelligence; this approach is called FAT -ML (Fairness, Accountability and  Transparency in Machine Learning)54.
(m / multi-sentence
      :snt1 (l / law
            :name (n / name
                  :op1 "FAT-ML")
            :ARG1-of (m2 / mean-01
                  :ARG2 (ii / intelligent-01
                        :mod (a / artificial)
                        :ARG1-of (f / fair-01)
                        :ARG0-of (a2 / accountable-02)
                        :mod (t / transparent))))
      :snt2 (c / consensus
            :mod (s / some)
            :topic (f2 / focus-01
                  :ARG1 (c2 / character
                        :poss (ii2 / intelligent-01
                              :mod a)
                        :ARG1-of (f3 / fair-01)
                        :ARG0-of (r / responsible-02)
                        :mod (t2 / transparency)))
            :time (s2 / study-01
                  :ARG1 (b / behave-01
                        :ARG0 (a3 / algorithm)))
            :ARG1-of (m3 / mean-01
                  :ARG2 (a4 / approach-02
                        :ARG1-of (c3 / call-01
                              :ARG2 (l2 / law
                                    :name (n2 / name
                                          :op1 "FAT-ML")
                                    :ARG1-of (m4 / mean-01
                                          :ARG2 (a5 / and
                                                :op1 (f4 / fairness-01)
                                                :op2 a2)
                                          :op3 (t3 / transparency)
                                          :topic (l3 / learn-01
                                                :mod (m5 / machine))))))))
      :li 54)


# ::id 74
# ::snt AI activities in the 7 administrations participating in the FCAI  AI ethics Frameworks Existing AI  regulation AI standards Public  Investment Australia Australia’s AI Ethics Framework Review of existing  regulations per  the AI Action Plan Standards Australia focuses on  by-design and standards testing;  AI Standards Roadmap AUD 124.1 million  (USD 90.9 million)  2021-2022  Canada CIFAR Pan-Canadian AI  Strategy 2017; Digital  Charter 2017/2021; Montreal  Declaration for Responsible  Development of AI Directive on  Automated  Decision Making;  Algorithmic  Impact  Assessment CIO Strategy Council develops  AI Standards and is accredited  by Standards Council of Canada,  focusing on ethical design and  ADM audits; $8.6 million over  five years, starting in 2021–22,  to advance the development and  adoption of AI standards CAD 125 million  (USD 100 million)  2017-2022  EU Ethics Guidelines for  Trustworthy AI; White Paper on  AI; Proposal for a regulation on  AI; National ethics guidelines Coordinated Plan  on AI; Proposal  for a regulation on  AI; Digital Decade  package CEN-CENELC Joint Technical  Committee 21 ‘Artificial  Intelligence’; national standards  focus on EU interoperability,  ethics, fundamental rights, and  safety EUR 20 billion  (USD 23.3 billion)  per year until 2030,  national funding Japan R&D Guidelines 2018; Social  Principles of Human-Centric AI  2019; AI Utilization Guidelines  2019; Society 5.0 framework Draft AI Utilization  Principles  Guidelines 2019;  AI Technology  Strategy 2017 Ministry of Economy, Trade  and Industry (METI), Japanese  Industrial Standards Committee  and Information Technology  Standards Commission focus  on developing sector-specific  standards in transportation,  safety, and patents Yen 77 billion  (USD 70 billion)  2018  Singapore Model AI Governance  Framework, 2nd Edition,  2020; Implementation and  Self-Assessment Guide for  Organizations; Principles  to Promote Fairness,  Ethics, Accountability and  Transparency National AI  Strategy Voluntary Horizontal Model  Framework contributes to global  standards for AI-related policies  and guidelines Up to SG$150  million  (USD 110.8  million)  2017-2022   U.K.
(m / multi-sentence
      :snt1 (a / and
            :op1 (f / focus-01
                  :ARG0 (g / government-organization
                        :quant 7
                        :ARG0-of (a2 / administrate-01)
                        :ARG0-of (p / participate-01
                              :ARG1 (c / conference
                                    :name (n / name
                                          :op1 "FCAI"))))
                  :ARG1 (a3 / and
                        :op1 (s / standard
                              :mod (e / ethics)
                              :mod (a4 / artificial))
                        :op2 (s2 / standard
                              :mod (e2 / ethics)
                              :ARG1-of (e3 / exist-01))))
            :op2 (p2 / publication
                  :name (n2 / name
                        :op1 "Public"
                        :op2 "Investment"
                        :op3 "Board")
                  :mod (c2 / country
                        :name (n3 / name
                              :op1 "Australia")))
            :op3 (r / review-01
                  :ARG0 (g2 / government-organization
                        :name (n4 / name
                              :op1 "Standards"
                              :op2 "Council"
                              :op3 "of"
                              :op4 "Canada"))
                  :ARG1 (r2 / regulate-01
                        :ARG1 (ii / intelligent-01
                              :mod (a5 / artificial)))
                  :time (d / date-entity
                        :year 2017))
            :op4 (p3 / publication
                  :name (n5 / name
                        :op1 "Model"
                        :op2 "AI"
                        :op3 "Governance"
                        :op4 "Framework")
                  :time (d2 / date-entity
                        :year 2020)
                  :mod (c3 / country
                        :name (n6 / name
                              :op1 "Singapore")))
            :op5 (p4 / publication
                  :name (n7 / name
                        :op1 "Social"
                        :op2 "Principles"
                        :op3 "of"
                        :op4 "Human-Centric"
                        :op5 "AI")
                  :time (d3 / date-entity
                        :year 2019))
            :op6 (p5 / publication
                  :name (n8 / name
                        :op1 "Draft"
                        :op2 "AI"
                        :op3 "Utilization"
                        :op4 "Principles")
                  :time (d4 / date-entity
                        :year 2019))
            :op7 (p6 / publication
                  :name (n9 / name
                        :op1 "CEN-CENELC"
                        :op2 "Joint"
                        :op3 "Technical"
                        :op4 "Committee")
                  :time (d5 / date-entity
                        :year 2017))
            :op8 (p7 / publication
                  :name (n10 / name
                        :op1 "Public"
                        :op2 "Accountability"
                        :op3 "and"
                        :op4 "Transparency"
                        :op5 "Guide"
                        :op6 "for"
                        :op7 "Organization"))
            :op9 (p8 / publication
                  :name (n11 / name
                        :op1 "Information"
                        :op2 "Technology"
                        :op3 "Standards"
                        :op4 "Commission")
                  :time (d6 / date-entity
                        :year 2017))
            :op10 (p9 / publication
                  :name (n12 / name
                        :op1 "Yuan"
                        :op2 "Billion")
                  :ARG1-of (r3 / rate-entity-91
                        :ARG2 (t / temporal-quantity
                              :quant 1
                              :unit (y / year))))
            :op11 (m2 / monetary-quantity
                  :quant 125000000
                  :unit (d7 / dollar)))
      :op12 (m3 / monetary-quantity
            :quant 100000000
            :unit (d8 / dollar))
      :op13 (m4 / monetary-quantity
            :quant 1550000000
            :unit (d9 / dollar))
      :purpose (a6 / advance-01
            :ARG1 (a7 / and
                  :op1 (d10 / develop-02
                        :ARG1 s)
                  :op2 (a8 / adopt-01
                        :ARG1 s))))


# ::id 74
# ::snt In sectors like finance, key  criteria such as fairness, discrimination, and transparency have long been  subject to extensive regulatory intervention, and sectoral regulation  must ensure continuity while accounting for the increasing use of AI.
(a / and
      :op1 (s / subject-01
            :ARG1 (c / criteria
                  :ARG1-of (k / key-02)
                  :example (a2 / and
                        :op1 (f / fairness)
                        :op2 (d / discriminate-02)
                        :op3 (t / transparency)))
            :ARG2 (ii / intervene-01
                  :ARG0 (r / regulate-01)
                  :ARG1-of (e / extensive-03))
            :ARG1-of (l / long-03)
            :location (s2 / sector
                  :example (f2 / finance)))
      :op2 (o / obligate-01
            :ARG1 (r2 / regulate-01
                  :mod (s3 / sector))
            :ARG2 (e2 / ensure-01
                  :ARG0 r2
                  :ARG1 (c2 / continue-01)
                  :time (a3 / account-01
                        :ARG0 r2
                        :ARG1 (u / use-01
                              :ARG1 (ii2 / intelligent-01
                                    :mod (a4 / artificial))
                              :ARG1-of (ii3 / increase-01))))))


# ::id 74
# ::snt An analysis of 22 AI ethics principles found that the  values of accountability, privacy, fairness, transparency, and cybersecurity  appeared in over 70 percent of the documents.
(f / find-01
      :ARG0 (a / analyze-01
            :ARG1 (p / principle
                  :quant 22
                  :topic (e / ethics)
                  :mod (a2 / artificial)))
      :ARG1 (a3 / appear-01
            :ARG1 (v / value
                  :example (a4 / and
                        :op1 (a5 / accountable-02)
                        :op2 (p2 / privacy)
                        :op3 (f2 / fairness)
                        :op4 (t / transparency)
                        :op5 (c / cybersecurity)))
            :location (d / document
                  :quant (o / over
                        :op1 (p3 / percentage-entity
                              :value 70)))))


# ::id 74
# ::snt Other common principles  include human oversight, explainability or interpretability, legal status of  AI systems, and the equitable economic effect of AI.31 A separate analysis  of 84 AI ethics documents done in 2019 found that there has been a global  convergence around “transparency, justice and fairness, non-maleficence,  responsibility and privacy.”32 While much progress has been made aligning on responsible AI, there remain  differences—even among FCAI participants.
(m / multi-sentence
      :li 31
      :snt1 (ii / include-01
            :ARG1 (a / and
                  :op1 (o / oversight
                        :mod (h / human))
                  :op2 (o2 / or
                        :op1 (p / possible-01
                              :ARG1 (e / explain-01))
                        :op2 (p2 / possible-01
                              :ARG1 (ii2 / interpret-01)))
                  :op3 (s / status
                        :ARG1-of (l / legal-02)
                        :poss (s2 / system
                              :mod (a2 / artificial)))
                  :op4 (a3 / affect-01
                        :ARG0 (ii3 / intelligent-01
                              :mod (a4 / artificial))
                        :ARG1 (e2 / economy)
                        :mod (e3 / equitable)))
            :ARG2 (p3 / principle
                  :ARG1-of (s3 / share-01)
                  :mod (o3 / other)))
      :snt2 (f / find-01
            :ARG0 (a5 / analyze-01
                  :ARG1 (d / document
                        :quant 84
                        :topic (e4 / ethics)
                        :mod (ii4 / intelligent-01
                              :mod (a6 / artificial)))
                  :ARG1-of (s4 / separate-02)
                  :time (d2 / date-entity
                        :year 2019))
            :ARG1 (c / converge-01
                  :ARG1 (a7 / and
                        :op1 (t / transparency)
                        :op2 (j / justice)
                        :op3 (f2 / fairness)
                        :op4 (m2 / malice
                              :polarity -)
                        :op5 (r / responsible-02)
                        :op6 (p4 / privacy))
                  :mod (g / globe)))
      :snt3 (c2 / contrast-01
            :ARG1 (p5 / progress-01
                  :ARG1 (a8 / align-01
                        :ARG2 ii4)
                  :quant (m3 / much))
            :ARG2 (r2 / remain-01
                  :ARG1 (d3 / differ-02
                        :ARG1 (p6 / person
                              :ARG0-of (p7 / participate-01
                                    :ARG1 (e5 / event
                                          :name (n / name
                                                :op1 "FCAI")))))
                  :mod (e6 / even))))


# ::id 74
# ::snt In addition, Canada has issued regulations to address certain risks related to  artificial intelligence and the processing of personal information in the federal  government; the Directive on Automated Decision-Making came into effect  April 19, 2019, requiring federal government bodies to complete algorithmic  impact assessments prior to utilizing automated decisionmaking tools, notify  affected parties both before and after automated decisions, and analyze  all results for potential bias.57 In the private sector, the federal Personal  Information Protection and Electronic Documents Act (PIPEDA) regulates  how businesses handle personal information, setting out ten fair information  principles that include safeguards to maintain privacy, accuracy, and fairness  in data processing and minimize potential harms or discrimination to 
STRENGTHENING INTERNATIONAL COOPERATION  ON AI29individuals.58 These regulations—together with Canada’s Digital Charter,  a government initiative to build public trust in emerging technologies— contribute to the government’s objectives to maximize the economic and  social benefits of AI while minimizing any potential pitfalls or risks.59 Canada has made working with the international community on collective  ways to harness the benefits of AI a feature of its AI strategy.
(m / multi-sentence
      :snt1 (a / and
            :li 58
            :op2 (r / regulate-01
                  :ARG0 (c / country
                        :name (n / name
                              :op1 "Canada"))
                  :ARG0-of (a2 / address-02
                        :ARG1 (r2 / risk-01
                              :ARG1-of (r3 / relate-01
                                    :ARG2 (a3 / and
                                          :op1 (ii / intelligent-01
                                                :mod (a4 / artificial))
                                          :op2 (p / process-01
                                                :ARG0 (b / business)
                                                :ARG1 (ii2 / information
                                                      :ARG1-of (p2 / personal-02)))))
                              :mod (c2 / certain)))))
      :snt2 (r4 / regulate-01
            :li 57
            :ARG0 (l / law
                  :name (n2 / name
                        :op1 "Directed"
                        :op2 "on"
                        :op3 "Automated"
                        :op4 "Decision-Making")
                  :ARG0-of (r5 / require-01
                        :ARG1 (a5 / and
                              :op1 (c3 / complete-01
                                    :ARG0 (b2 / body
                                          :mod (g / government-organization
                                                :ARG0-of (g2 / govern-01)
                                                :mod (f / federal)))
                                    :ARG1 (a6 / assess-01
                                          :ARG1 (ii3 / impact-01
                                                :mod (a7 / algorithm)))
                                    :time (p3 / prior
                                          :op1 (u / utilize-01
                                                :ARG0 b2
                                                :ARG1 (t / tool
                                                      :instrument-of (m2 / make-01
                                                            :ARG1 (d / decide-01))
                                                      :ARG1-of (a8 / automate-01)))))
                              :op2 (n3 / notify-01
                                    :ARG0 b2
                                    :ARG1 (p4 / party
                                          :ARG1-of (a9 / affect-01))
                                    :time (a10 / and
                                          :op1 (b3 / before
                                                :op1 d)
                                          :op2 (a11 / after
                                                :op1 d)))
                              :op3 (a12 / analyze-01
                                    :ARG0 b2
                                    :ARG1 (r6 / result-01
                                          :mod (a13 / all))
                                    :ARG2 (b4 / bias-01
                                          :mod (p5 / potential)))))
                  :ARG0-of (s / set-out-06
                        :ARG1 (p6 / principle
                              :quant 10
                              :topic (ii4 / information
                                    :ARG1-of (f2 / fair-01))
                              :ARG2-of (ii5 / include-01
                                    :ARG1 (a14 / and
                                          :op1 (m3 / maintain-01
                                                :ARG1 (a15 / and
                                                      :op1 (p7 / private-03)
                                                      :op2 (a16 / accurate))))
                                    :op2 (m4 / minimize-01
                                          :ARG1 (o / or
                                                :op1 (h / harm-01)
                                                :op2 (d2 / discriminate-02))))))))
      :ARG0-of (c4 / contribute-01
            :ARG2 (o2 / objective
                  :poss g
                  :topic (m5 / maximize-01
                        :ARG0 g
                        :ARG1 (b5 / benefit-01
                              :ARG0 ii
                              :ARG1 (a17 / and
                                    :op1 (e / economy)
                                    :op2 (s2 / society)))))))


# ::id 74
# ::snt An integral element of its AI governance is its Society  5.0, a conceptual vision document guiding actions in science, technology,  and innovation aimed at synergies for a prosperous future.79 The Society 5.0  framework frames Japan’s AI principles (human-centricity, education/literacy,  privacy, security, fair competition, fairness, accountability and transparency,  and innovation) mainly in relation to cultural and social aspects of its society.80  The Cabinet Office Council on Industrial Competitiveness81 has targeted selfdriving cars, drones, and production management, including smart factories,  all powered by AI, as key opportunities to increase Japan’s productivity.
(m / multi-sentence
      :snt2 (t / target-01
            :li 80
            :ARG0 (g / government-organization
                  :name (n / name
                        :op1 "Cabinet"
                        :op2 "Office"
                        :op3 "Council"
                        :op4 "on"
                        :op5 "Industrial"
                        :op6 "Competitive"
                        :op7 "Compitiveness"))
            :ARG1 (o / opportunity
                  :ARG1-of (k / key-02)
                  :purpose (ii / increase-01
                        :ARG0 g
                        :ARG1 (p / productive-03
                              :ARG0 (c / country
                                    :name (n2 / name
                                          :op1 "Japan"))))
                  :domain (a / and
                        :op1 (c2 / car
                              :ARG0-of (d / drive-01
                                    :ARG1 (s / self)))
                        :op2 (d2 / drone)
                        :op3 (m2 / manage-01
                              :ARG1 (p2 / produce-01)
                              :ARG2-of (ii2 / include-01
                                    :ARG1 (f / factory
                                          :ARG1-of (s2 / smart-06)
                                          :ARG1-of (p3 / power-01
                                                :ARG0 (a2 / artificial-03)))))))
            :ARG2 (s3 / synergize-01
                  :ARG1-of (a3 / aim-02
                        :ARG2 (f2 / future
                              :ARG0-of (p4 / prosper-01)))))
      :snt1 (f3 / frame-06
            :ARG0 (e / element
                  :mod (ii3 / integral)
                  :part-of (g2 / govern-01
                        :ARG0 (ii4 / it)
                        :ARG1 (a4 / artificial-03))
                  :domain (d3 / document
                        :name (n3 / name
                              :op1 "Society"
                              :op2 "5.0")
                        :mod (e2 / envision-01)))
            :ARG1 (p5 / principle
                  :mod (a5 / artificial-03)
                  :poss c
                  :ARG1-of (r / relate-01
                        :ARG2 (a6 / aspect
                              :mod (c3 / culture)
                              :mod (s4 / society))
                        :mod (m3 / main))
                  :example (a7 / and
                        :op1 (f4 / focus-01
                              :ARG2 (h / human))
                        :op2 (e3 / educate-01)
                        :op3 (l / literacy)
                        :op4 (s5 / security)
                        :op5 (c4 / compete-01
                              :ARG1-of (f5 / fair-01))
                        :op6 (a8 / and
                              :op1 (a9 / accountable-02)
                              :op2 (t2 / transparency))
                        :op7 (ii5 / innovate-01)))))


# ::id 74
# ::snt In addition, Singapore has released nonbinding guidance to help organizations  navigate data ethics and governance principles, such as transparency, fairness,  and explainability.
(a / and
      :op2 (r / release-01
            :ARG0 (c / country
                  :name (n / name
                        :op1 "Singapore"))
            :ARG1 (g / guide-01
                  :ARG1-of (b / binding-07
                        :polarity -)
                  :ARG0-of (h / help-01
                        :ARG1 (n2 / navigate-01
                              :ARG0 (o / organization)
                              :ARG1 (a2 / and
                                    :op1 (e / ethics
                                          :mod (d / data))
                                    :op2 (p / principle
                                          :topic (g2 / govern-01)
                                          :example (a3 / and
                                                :op1 (t / transparency)
                                                :op2 (f / fairness)
                                                :op3 (e2 / explain-01
                                                      :ARG1-of (p2 / possible-01))))))
                        :ARG2 o))))


# ::id 74
# ::snt It names the Alan Turing Institute as  the national AI research center to “remain a globally leading player in AI,”  “promote the U.K.’s interests through collaborations with international  partners,” and “attract world-leading talent to the U.K..”98Singapore has  released nonbinding  guidance to help  organizations  navigate data ethics  and governance  principles, such  as transparency,  fairness, and  explainability.
(a / and
      :li 98
      :op1 (n / name-01
            :ARG0 (ii / it)
            :ARG1 (r / research-institute
                  :name (n2 / name
                        :op1 "Alan"
                        :op2 "Turing"
                        :op3 "Institute"))
            :ARG2 (c / center
                  :mod (n3 / nation)
                  :purpose (a2 / and
                        :op1 (r2 / remain-01
                              :ARG1 (c2 / country
                                    :name (n4 / name
                                          :op1 "U.K."))
                              :ARG3 (p / play-02
                                    :ARG0 c2
                                    :ARG1 (ii2 / intelligent-01
                                          :mod (a3 / artificial))
                                    :mod (g / globe)))
                        :op2 (p2 / promote-02
                              :ARG0 c2
                              :ARG1 (ii3 / interest-01
                                    :ARG1 c2)
                              :manner (c3 / collaborate-01
                                    :ARG0 c2
                                    :ARG1 (c4 / country
                                          :ARG0-of (p3 / partner-01
                                                :ARG1 c2)
                                          :mod (ii4 / international))))
                        :op3 (a4 / attract-01
                              :ARG0 c2
                              :ARG1 (t / talent
                                    :ARG0-of (l / lead-02
                                          :ARG1 (w / world)))
                              :ARG2 c2))))
      :op2 (r3 / release-01
            :ARG0 (c5 / country
                  :name (n5 / name
                        :op1 "Singapore"))
            :ARG1 (g2 / guide-01
                  :ARG1 c5
                  :ARG2 (h / help-01
                        :ARG0 c5
                        :ARG1 (n6 / navigate-01
                              :ARG0 (o / organization)
                              :ARG1 (p4 / principle
                                    :example (a5 / and
                                          :op1 (e / ethics
                                                :mod (d / data))
                                          :op2 (g3 / govern-01))))
                        :ARG2 o)
                  :ARG1-of (b / binding-07
                        :polarity -))))


# ::id 74
# ::snt In addition to identifying a  need for review of regulations, the report also recognized a need to address  the threat of unintended consequences by ensuring an ethical governance  approach that emphasizes fairness and safety.
(a / and
      :op1 (r / recognize-01
            :ARG0 (r2 / report)
            :ARG1 (n / need-01
                  :ARG1 (a2 / address-02
                        :ARG1 (t / threaten-01
                              :ARG0 (c / consequence-03
                                    :ARG1-of (ii / intend-01
                                          :polarity -)))
                        :ARG2 (e / ensure-01
                              :ARG1 (a3 / approach-02
                                    :ARG1 (g / govern-01
                                          :manner (e2 / ethics))
                                    :ARG0-of (e3 / emphasize-01
                                          :ARG1 (a4 / and
                                                :op1 (f / fairness)
                                                :op2 (s / safe-01)))))))
            :mod (a5 / also))
      :op2 (ii2 / identify-01
            :ARG0 r2
            :ARG1 (n2 / need-01
                  :ARG1 (r3 / review-01
                        :ARG1 (r4 / regulate-01)))))


# ::id 74
# ::snt The Asilomar AI  Principles, developed in 2017, were signed by nearly 6,000 AI experts and  adopted as informal guiding principles by the state of California.28 The IEEE’s  Ethically Aligned Design is a comprehensive exploration of AI developed 
STRENGTHENING INTERNATIONAL COOPERATION  ON AI21over a three-year period, also involving several thousand experts.29 AI Now  was an early civil society mover in propounding recommendations for  government policies.30 There is considerable overlap among these various sets of principles, including  on the importance of fairness, privacy preservation, and respect for human  rights and autonomy.
(m / multi-sentence
      :snt1 (a / and
            :li 28
            :op1 (s / sign-01
                  :ARG0 (p / person
                        :ARG1-of (e / expert-01
                              :ARG2 (ii / intelligent-01
                                    :mod (a2 / artificial)))
                        :quant (n / nearly
                              :op1 6000))
                  :ARG1 (p2 / principle
                        :name (n2 / name
                              :op1 "Asilomar"
                              :op2 "AI"
                              :op3 "Principles")
                        :ARG1-of (d / develop-02
                              :time (d2 / date-entity
                                    :year 2017))))
            :op2 (a3 / adopt-01
                  :ARG0 (s2 / state
                        :name (n3 / name
                              :op1 "California"))
                  :ARG1 p2
                  :ARG3 (p3 / principle
                        :ARG0-of (g / guide-01)
                        :mod (f / formal
                              :polarity -))))
      :snt2 (o / overlap-01
            :li 30
            :ARG0 (s3 / set
                  :consist-of (p4 / principle)
                  :mod (v / various)
                  :ARG2-of (ii2 / include-01
                        :ARG1 (a4 / and
                              :op1 (ii3 / important-01
                                    :ARG1 (f2 / fairness))
                              :op2 (p5 / preserve-01
                                    :ARG1 (p6 / privacy))
                              :op3 (r / respect-01
                                    :ARG1 (a5 / and
                                          :op1 (r2 / right-05
                                                :ARG1 (h / human))
                                          :op2 (a6 / autonomy
                                                :poss h))))))
            :degree (c / considerable))
      :snt3 (e2 / explore-01
            :ARG0 (p7 / publication
                  :name (n4 / name
                        :op1 "IAI's"
                        :op2 "Ethical"
                        :op3 "Aligned"
                        :op4 "Design"))
            :ARG1 (ii4 / intelligent-01
                  :mod (a7 / artificial)
                  :ARG1-of (d3 / develop-02)
                  :ARG0-of (s4 / strengthen-01
                        :ARG1 (c2 / cooperate-01
                              :ARG2 ii4
                              :mod (ii5 / international))
                        :duration (t / temporal-quantity
                              :quant 3
                              :unit (y / year))
                        :ARG2-of (ii6 / involve-01
                              :ARG1 (p8 / person
                                    :ARG1-of (e3 / expert-01)
                                    :quant (s5 / several
                                          :op1 1000))
                              :mod (a8 / also)))))
      :snt4 (m2 / move-02
            :ARG0 (p9 / publication
                  :name (n5 / name
                        :op1 "AI"
                        :op2 "Now"))
            :ARG1 (p10 / propound-01
                  :ARG1 (r3 / recommend-01
                        :ARG1 (p11 / policy-01
                              :ARG0 (g2 / government-organization
                                    :ARG0-of (g3 / govern-01)))))
            :time (e4 / early)
            :mod (s6 / society
                  :mod (c3 / civil))))


# ::id 74
# ::snt It covers best practices for compliance with  data protection laws in development and deployment of AI systems  and focuses on accountability and governance; data protection impact  assessment; lawfulness, fairness, and transparency; security and data  minimization; and individual rights in AI systems.184 • The Partnership on AI hosted important discussions for the development  of an end-to-end approach to internal algorithmic auditing, including an  analysis of how to learn across industries.185
STRENGTHENING INTERNATIONAL COOPERATION  ON AI55• At the sectoral level, work on algorithmic auditing is intensifying  with several sector-specific frameworks being developed in finance,  health care, and intelligence.
(m / multi-sentence
      :li 185
      :snt1 (a / and
            :op1 (c / cover-01
                  :ARG0 (ii / it)
                  :ARG1 (a2 / and
                        :op1 (p / practice-01
                              :ARG1 (c2 / comply-01
                                    :ARG1 (l / law
                                          :topic (p2 / protect-01
                                                :ARG1 (d / data)))))
                        :op2 (d2 / deploy-01
                              :ARG1 (s / system
                                    :mod (a3 / artificial)))))
            :op2 (f / focus-01
                  :ARG0 ii
                  :ARG2 (a4 / and
                        :op1 (a5 / and
                              :op1 (a6 / accountable-02)
                              :op2 (g / govern-01))
                        :op2 (a7 / assess-01
                              :ARG1 (ii2 / impact-01
                                    :ARG0 (l2 / law
                                          :topic (p3 / protect-01
                                                :ARG1 (d3 / data)))))
                        :op3 (f2 / fairness)
                        :op4 (t / transparency)
                        :op5 (a8 / and
                              :op1 (s2 / security)
                              :op2 (m2 / minimize-01
                                    :ARG1 (d4 / data)))
                        :op6 (r / right-05
                              :ARG1 (ii3 / individual)
                              :location (s3 / system
                                    :mod (a9 / artificial))))))
      :snt2 (ii4 / intensify-01
            :ARG1 (w / work-01
                  :ARG1 (a10 / audit-01
                        :mod (a11 / algorithm))
                  :ARG1-of (ii5 / internal-02))
            :ARG1-of (c3 / cause-01
                  :ARG0 (d5 / develop-02
                        :ARG1 (f3 / framework
                              :ARG1-of (s4 / specific-02
                                    :ARG2 (s5 / sector))
                              :quant (s6 / several))))
            :location (l3 / level
                  :mod (s7 / sector)))
      :snt3 (h / host-01
            :ARG0 (o / organization
                  :name (n / name
                        :op1 "Partnership"
                        :op2 "on"
                        :op3 "AI"))
            :ARG1 (d6 / discuss-01
                  :ARG1 (d7 / develop-02
                        :ARG1 (a12 / approach-02
                              :ARG1 (a13 / audit-01
                                    :mod (e / end-to-end)))
                        :ARG2-of (ii6 / include-01
                              :ARG1 (a14 / analyze-01
                                    :ARG1 (t2 / thing
                                          :manner-of (l4 / learn-01
                                                :location (a15 / across
                                                      :op1 (ii7 / industry))))))))
            :ARG1-of (ii8 / important-01)))


# ::id 74
# ::snt Because of the importance to these governments  and others of fairness, due process, nondiscrimination, and humancentered AI, they face a heightened need to practice what they preach by  ensuring the AI that they deploy is ethical and trustworthy.
(c / cause-01
      :ARG0 (ii / important-01
            :ARG1 (a / and
                  :op1 (f / fairness)
                  :op2 (d / due-process)
                  :op3 (d2 / discriminate-02
                        :polarity -)
                  :op4 (ii2 / intelligent-01
                        :mod (a2 / artificial)
                        :ARG1-of (c2 / center-01
                              :ARG2 (h / human))))
            :ARG2 (a3 / and
                  :op1 (g / government-organization
                        :ARG0-of (g2 / govern-01)
                        :mod (t / this))
                  :op2 (o / other)))
      :ARG1 (f2 / face-01
            :ARG0 (t2 / they)
            :ARG1 (n / need-01
                  :ARG1 (p / practice-01
                        :ARG0 t2
                        :ARG1 (t3 / thing
                              :ARG1-of (p2 / preach-01
                                    :ARG0 t2))
                        :manner (e / ensure-01
                              :ARG0 t2
                              :ARG1 (a4 / and
                                    :op1 (e2 / ethics
                                          :domain (ii3 / intelligent-01
                                                :mod a2))
                                    :op2 (t4 / trustworthy
                                          :domain ii3))))
                  :ARG1-of (h2 / heighten-01))))


# ::id 74
# ::snt STRENGTHENING INTERNATIONAL COOPERATION  ON AI45Table 2. International frameworks for the development of responsible AI Values Definitions EU Australia Japan Singapore OECD Human  centeredAI systems should be designed to be  inclusive, accommodating the needs of the  individuals that interact with it, and used in  a manner that is aligned with the values of  the community in which it is deployed.✔ ✔ ✔ ✔ ✔ Mitigate risks  and promote  benefitsAI systems should be designed and  deployed for the benefit of end users and  avoid unintended negative impacts on third  parties.✔ ✔ ✔ ✔ ✔ Fairness Governance and technical safeguards are  important to identify and mitigate risks of  unfair biases, particularly in circumstances  where an AI system could have a  consequential impact on people.✔ ✔ ✔ ✔ ✔ Explainability AI systems should be understandable;  context will dictate the appropriate  mechanisms for providing transparency  about a particular system’s decisionmaking  processes.✔ ✔ ✔ ✔ ✔ Privacy and  securityAI systems should be secure and enable  users to make informed choices regarding  use of personal information.✔ ◑ ✔ ○ ✔ Safety and  reliabilityAI systems should be designed to mitigate  foreseeable safety risks and adequately  tested to ensure that they operate as  intended.✔ ✔ ◑ ✔ ✔ Accountability A lifecycle approach to AI accountability,  including appropriate governance  structures for the design phase and redress  mechanisms following deployment is  important.✔ ✔ ◑ ✔ ✔ Riskbased and  proportionateRisks are context-specific and encourage  stakeholders to deploy risk management  techniques that are tailored to specific use  cases.✔ ✔ ○ ✔ ✔ Multiple  stakeholdersMultiple stakeholders have important roles  to play in mitigating risks involved in the  development, deployment, and use of AI.◑ ◑ ✔ ✔ ✔ Promotes  innovationGovernment is a key enabler of AI  innovation, and promotes a policy  environment that is conducive to crossborder data flows, value-added data  services, access to non-sensitive  government data, R&D, and workforce  development initiatives.○ ◑ ✔ ○ ✔ ✔  Satisfactory       ◑ Partial       ○ Unaddressed Source: BSA/The Software Alliance153
3.
(m / multi-sentence
      :snt1 (a / and
            :op1 (s / strengthen-01
                  :ARG1 (c / cooperate-01
                        :mod (ii / international)
                        :topic (p / product
                              :name (n / name
                                    :op1 "AI45"))))
            :op2 (d / dictate-01
                  :ARG0 (c2 / context)
                  :ARG1 (a2 / and
                        :op1 (a3 / approach-02
                              :ARG1 (a4 / accountable-02
                                    :ARG1 p)
                              :mod (l / lifecycle)
                              :ARG2-of (ii2 / include-01
                                    :ARG1 (a5 / and
                                          :op1 (p2 / phase
                                                :mod (d2 / design-01))
                                          :op2 (m2 / mechanism
                                                :ARG0-of (r / redress-01
                                                      :ARG1 p2))))))))
      :snt2 (a6 / and
            :op1 (r2 / recommend-01
                  :ARG1 (a7 / and
                        :op1 (s2 / secure-02
                              :ARG1 (s3 / system
                                    :mod (ii3 / intelligent-01
                                          :mod (a8 / artificial))))
                        :op2 (e / enable-01
                              :ARG0 s3
                              :ARG1 (c3 / choose-01
                                    :ARG0 (p3 / person
                                          :ARG0-of (u / use-01))
                                    :ARG1 (u2 / use-01
                                          :ARG1 (ii4 / information
                                                :ARG1-of (p4 / personal-02))))
                              :ARG2 p3)))
            :op2 (r3 / recommend-01
                  :ARG1 (a9 / and
                        :op1 (d3 / design-01
                              :ARG1 s3
                              :ARG3 (a10 / and
                                    :op1 (ii5 / inclusive)
                                    :op2 (a11 / accommodate-01
                                          :ARG0 s3
                                          :ARG1 (n2 / need-01
                                                :ARG0 (p5 / person
                                                      :ARG0-of (ii6 / interact-01
                                                            :ARG1 s3)))))
                              :op2 (u3 / use-01
                                    :ARG1 s3
                                    :manner (a12 / align-01
                                          :ARG1 s3
                                          :ARG2 (v / value
                                                :poss (c4 / community
                                                      :location-of (d4 / deploy-01
                                                            :ARG1 s3))))))
                        :op3 (t / test-01
                              :ARG1 s3
                              :ARG2 (e2 / ensure-01
                                    :ARG1 (o / operate-01
                                          :ARG1 s3
                                          :ARG1-of (ii7 / intend-01)))))))
      :snt3 (a13 / and
            :op1 (e3 / enforce-01
                  :ARG0 (g / government-organization
                        :ARG0-of (g2 / govern-01)))
            :op2 (e4 / enforce-01
                  :ARG0 (g3 / government-organization
                        :ARG0-of (g4 / govern-01))))
      :op3 (p6 / promote-02
            :ARG0 g
            :ARG1 (ii8 / innovate-01))
      :snt4 (a14 / and
            :op1 (e5 / enforce-01
                  :ARG0 (g5 / government-organization
                        :name (n3 / name
                              :op1 "BSA")))
            :op2 (o2 / organization
                  :name (n4 / name
                        :op1 "The"
                        :op2 "Software"
                        :op3 "Alliance")))
      :snt5 (ii9 / important-01))


# ::id 74
# ::snt pdf; “Launch of AI: Accelerated initiative for artificial intelligence—an accelerated application-to-grant service for patent applications in  artificial intelligence,” Intellectual Property Office of Singapore, April 26, 2019, https://www.ipos.gov.sg/docs/default-source/resourceslibrary/patents/circulars/(2019)-circular-no-2---ai2-initiative_final.pdf?sfvrsn=2; “Singapore’s approach to AI governance,” Personal Data  Protection Commissioner Singapore, accessed September 1, 2021, https://www.pdpc.gov.sg/Help-and-Resources/2020/01/Model-AIGovernance-Framework; “Principles to promote fairness, ethics, accountability, and transparency (FEAT) in the use of artificial intelligence  and data analytics in Singapore’s financial sector,” Monetary Authority of Singapore, http://www.mas.gov.sg/~/media/MAS/News%20 and%20Publications/Monographs%20and%20Information%20Papers/FEAT%20Principles%20Final.pdf; Singapore researchers plug in to  world’s fastest supercomputer,” HPC Wire, November 30, 2020, https://www.hpcwire.com/off-the-wire/singapore-researchers-plug-in-toworlds-fastest-supercomputer/;  316.
(a / and
      :op1 (p / publication-91
            :ARG1 (p2 / publication
                  :name (n / name
                        :op1 "Launch"
                        :op2 "of"
                        :op3 "Accelerated"
                        :op4 "Initiative"
                        :op5 "for"
                        :op6 "Artificial"
                        :op7 "Intelligence")
                  :ARG1-of (m / mean-01
                        :ARG2 (p3 / publication
                              :name (n2 / name
                                    :op1 "Public"
                                    :op2 "Data"
                                    :op3 "Protection"
                                    :op4 "Commission")
                              :mod (c / country
                                    :name (n3 / name
                                          :op1 "Singapore"))
                              :ARG1-of (a2 / access-01
                                    :time (d / date-entity
                                          :month 9
                                          :day 1
                                          :year 2021)))))
            :ARG4 (u / url-entity
                  :value "https://www.hpcwire.gov.sg/off-the-wire/singapore-researchers-plug-in-toworlds-fastest-supercomputer/"))
      :op2 (p4 / publication
            :name (n4 / name
                  :op1 "Model-AIGovernance-Framework"))
      :op3 (p5 / publication
            :name (n5 / name
                  :op1 "Principles"
                  :op2 "to"
                  :op3 "Promoting"
                  :op4 "for"
                  :op5 "Artificial"
                  :op6 "Intelligence"
                  :op7 "and"
                  :op8 "Ethics"
                  :op9 "and"
                  :op10 "Data"
                  :op11 "Accountability"
                  :op12 "and"
                  :op13 "Transparency"))
      :time (d2 / date-entity
            :month 11
            :day 30
            :year 2020))


# ::id 74
# ::snt “Model artificial intelligence governance framework: second edition,” Personal Data Protection Commission Singapore, January 2020,  https://www.pdpc.gov.sg/help-and-resources/2020/01/second-edition-of-model-artificial-intelligence-governance-framework ; “Singapore’s  approach to AI governance,” Personal Data Protection Commissioner Singapore, accessed September 1, 2021, https://www.pdpc.gov.sg/ Help-and-Resources/2020/01/Model-AI-Governance-Framework; “Principles to promote fairness, ethics, accountability, and transparency  (FEAT) in the use of artificial intelligence and data analytics in Singapore’s financial sector,” Monetary Authority of Singapore, http://www.
(a / and
      :op1 (p / publication
            :name (n / name
                  :op1 "Model"
                  :op2 "Artificial"
                  :op3 "Intelligence"
                  :op4 "Governance"
                  :op5 "Framework")
            :mod (e / edition
                  :ord (o / ordinal-entity
                        :value 2))
            :medium (p2 / publication
                  :name (n2 / name
                        :op1 "Personal"
                        :op2 "Data"
                        :op3 "Protection"
                        :op4 "Commission"
                        :op5 "Singapore"))
            :time (d / date-entity
                  :month 1
                  :year 2020))
      :op2 (p3 / publication
            :name (n3 / name
                  :op1 "Monetary"
                  :op2 "Authority"
                  :op3 "of"
                  :op4 "Singapore")
            :ARG1-of (a2 / access-01
                  :time (d2 / date-entity
                        :month 9
                        :day 1
                        :year 2021))
            :medium (u / url-entity
                  :value "https://www.pdpc.gov.sg/help-and-resources/2020/01/Model-AI-Governance-Framework"))
      :op3 (p4 / publication
            :name (n4 / name
                  :op1 "FEAT")
            :ARG0-of (p5 / promote-02
                  :ARG1 (a3 / and
                        :op1 (f / fairness)
                        :op2 (e2 / ethics)
                        :op3 (a4 / accountable-02)
                        :op4 (t / transparency))
                  :topic (u2 / use-01
                        :ARG1 (a5 / and
                              :op1 (ii / intelligent-01
                                    :mod (a6 / artificial))
                              :op2 (a7 / analyze-01
                                    :ARG1 (d3 / data)))
                        :ARG2 (s / sector
                              :mod (f2 / finance)
                              :location (c / country
                                    :name (n5 / name
                                          :op1 "Singapore")))))))


# ::id 74
# ::snt “AI Fairness 360,” IBM Research Trusted AI, accessed August 27, 2021, https://aif360.mybluemix.net/; “AI Explainability 360,” IBM Research  Trusted AI, accessed August 27, 2021, https://aix360.mybluemix.net/.
(a / and
      :op1 (p / publication
            :name (n / name
                  :op1 "AI"
                  :op2 "Fairness"
                  :op3 360)
            :medium (p2 / publication
                  :name (n2 / name
                        :op1 "IBM"
                        :op2 "Research"
                        :op3 "Trusted"
                        :op4 "AI"))
            :ARG1-of (a2 / access-01
                  :time (d / date-entity
                        :day 27
                        :month 8
                        :year 2021))
            :mod (u / url-entity
                  :value "https://aif360.mybluemix.net/"))
      :op2 (p3 / publication
            :name (n3 / name
                  :op1 "AI"
                  :op2 "Explainedability"
                  :op3 360)
            :medium p2
            :ARG1-of (a3 / access-01
                  :time d)))


# ::id 74
# ::snt Inioluwa Debora Raji, et al., “Closing the AI accountability gap: Defining an end-to-end framework for internal algorithmic auditing,”  Conference on Fairness, Accountability, and Transparency (FAT* ’20), January 27-30, 2020, https://arxiv.org/pdf/2001.00973.pdf.
(p / publication-91
      :ARG0 (a / and
            :op1 (p2 / person
                  :name (n / name
                        :op1 "Inioluwa"
                        :op2 "Debora"
                        :op3 "Raji"))
            :op2 (p3 / person
                  :mod (o / other)))
      :ARG1 (p4 / publication
            :name (n2 / name
                  :op1 "Closing"
                  :op2 "the"
                  :op3 "Accountability"
                  :op4 "Gap")
            :ARG1-of (m / mean-01
                  :ARG2 (d / define-01
                        :ARG1 (f / framework
                              :mod (e / end-to-end)
                              :purpose (a2 / audit-01
                                    :mod (a3 / algorithm)
                                    :ARG1-of (ii / internal-02))))))
      :ARG4 (c / conference
            :name (n3 / name
                  :op1 "Conference"
                  :op2 "on"
                  :op3 "Fairness,"
                  :op4 "Accountability"
                  :op5 "and"
                  :op6 "Transparency")
            :time (d2 / date-interval
                  :op1 (d3 / date-entity
                        :month 1
                        :day 27
                        :year 2020)
                  :op2 (d4 / date-entity
                        :month 1
                        :day 30
                        :year 2020)))
      :ARG4 (u / url-entity
            :value "https://arxiv.org/pdf/2001.00973.pdf"))


# ::id 74
# ::snt AI policies and investment by FCAI participants   | ⮌  contents AI ethical  framework307Existing AI  regulation308Data governance309 AI Standards310Computing power Privacy IP Cyber SI315Model AI Governance  Framework, 2nd Edition,  2020;  Implementation and SelfAssessment Guide for  Organisations (ISAGO);  Principles to Promote  Fairness, Ethics,  Accountability and  Transparency (FEAT) National AI Strategy Personal Data  Protection Act 2012  (PDPA) (amended in  2020);  Trusted Data  Sharing Framework  (voluntary)Patents Act;  Copyright Act;  AI2 Scheme  for fast-track  examinationCybersecurity  Act 2018;  Computer  Misuse ActSpring SG: Voluntary  Horizontal Model  Framework also  contributes to global  standards for AI-related  policies and guidelinesNSCC cooperates with  Japan’s RIKEN and RIST  to access Fugaku;  National Research  Foundation builds second  national supercomputer  system (SG$200 million) U.K.316Guidance on Ethics,  Transparency  Accountability for ADMNational AI Strategy Data Protection Act  2018 (U.K. GDPR);  U.K. eIDAS RegulationCopyright, Designs  and Patents Act  1988Cyber Security  Information  Sharing  Partnership, UK  GDPRBritish Standard Institute  focuses on international  cooperation and  healthcare standardsGBP 20 million funding  for DiRAC (academic);  High Performance  Computing facility US317Principles in Executive  Order 13859 and  Executive Order 13960;  Agency specific  frameworks, statespecific guidelines  Government  agencies assessing  where AI regulation  is needed, where  existing regulation  applies, and roles  for self assessment,  codes, etc.
(m / multi-sentence
      :snt1 (a / and
            :op1 (a2 / and
                  :op1 (p / policy-01
                        :ARG0 (o / organization
                              :ARG0-of (p2 / participate-01
                                    :ARG1 (e / event
                                          :name (n / name
                                                :op1 "FCAI")))))
                  :op2 (ii / invest-01
                        :ARG0 o))
            :op2 (f / focus-01
                  :ARG0 o
                  :ARG2 (a3 / and
                        :op1 (c / cooperate-01
                              :mod (ii2 / international))
                        :op2 (s / standard
                              :mod (h / healthcare)))))
      :snt2 (a4 / and
            :op1 (r / regulate-01
                  :ARG1 (ii3 / intelligent-01
                        :mod (a5 / artificial))
                  :ARG1-of (e2 / exist-01)
                  :ARG1-of (c2 / cite-01
                        :ARG2 308))
            :op2 (r2 / regulate-01
                  :ARG1 (p3 / power
                        :mod (c3 / computer))
                  :ARG1-of (m2 / mean-01
                        :ARG2 (l / law
                              :name (n2 / name
                                    :op1 "Cybersecurity"
                                    :op2 "Act")
                              :time (d / date-entity
                                    :year 2018))))
            :op3 (o2 / organization
                  :name (n3 / name
                        :op1 "European"
                        :op2 "Digital"
                        :op3 "Standard"
                        :op4 "Institute"))
            :op4 (f2 / fund-01
                  :ARG1 (f3 / facility
                        :name (n4 / name
                              :op1 "DiRAC")
                        :mod (a6 / academia))
                  :ARG3 (m3 / monetary-quantity
                        :quant 200000000
                        :unit (p4 / pound)))
            :op5 (a7 / and
                  :op1 (p5 / publication
                        :name (n5 / name
                              :op1 "Executive"
                              :op2 "Order"
                              :op3 "13859"))
                  :op2 (p6 / publication
                        :name (n6 / name
                              :op1 "Executive"
                              :op2 "Order"
                              :op3 "13960"))
                  :op6 (f4 / framework
                        :ARG1-of (s2 / specific-02
                              :ARG2 (a8 / agency
                                    :mod (g / government-organization
                                          :ARG0-of (g2 / govern-01
                                                :ARG1 (c4 / country
                                                      :name (n7 / name
                                                            :op1 "U.K."))))))))
            :op7 (g3 / guide
                  :name (n8 / name
                        :op1 "ISAGO")))
      :op8 (p7 / principle
            :ARG0-of (p8 / promote-02
                  :ARG1 (a9 / and
                        :op1 (f5 / fairness)
                        :op2 (e3 / ethics
                              :op3 (a10 / accountable-02)
                              :op4 (a11 / accountable-02))))
            :ARG1-of (m4 / mean-01
                  :ARG2 (l2 / law
                        :name (n9 / name
                              :op1 "FEAT"))))
      :snt3 (a12 / and
            :op1 (l3 / law
                  :name (n10 / name
                        :op1 "Cyber"
                        :op2 "Security"
                        :op3 "Information"
                        :op4 "Sharing"
                        :op5 "Agreement"))
            :op2 (l4 / law
                  :name (n11 / name
                        :op1 "Computer"
                        :op2 "Misuse"
                        :op3 "Act"))
            :time (d2 / date-entity
                  :year 1988)))


# ::id 74
# ::snt Some examples  are discussed below:196 • In sectors like finance, key criteria such as fairness, discrimination, and  transparency have been subject to extensive regulatory intervention  in the past, and sectoral regulation must ensure continuity while at  the same time accounting for the increasing use of AI.
(d / discuss-01
      :li 196
      :ARG1 (e / example
            :quant (s / some))
      :location (b / below)
      :example-of (a / and
            :op1 (s2 / subject-01
                  :ARG1 (c / criteria
                        :ARG1-of (k / key-02)
                        :example (a2 / and
                              :op1 (f / fairness)
                              :op2 (d2 / discriminate-02)
                              :op3 (t / transparency)))
                  :ARG2 (ii / intervene-01
                        :ARG0 (r / regulate-01)
                        :ARG1-of (e2 / extensive-03))
                  :time (p / past)
                  :location (s3 / sector
                        :example (f2 / finance)))
            :op2 (o / obligate-01
                  :ARG1 (r2 / regulate-01
                        :mod (s4 / sector))
                  :ARG2 (e3 / ensure-01
                        :ARG0 r2
                        :ARG1 (c2 / continue-01)
                        :time (t2 / time
                              :ARG1-of (s5 / same-01
                                    :ARG2 (a3 / account-01
                                          :ARG1 (u / use-01
                                                :ARG1 (ii2 / intelligent-01
                                                      :mod (a4 / artificial))
                                                :ARG1-of (ii3 / increase-01)))))))))


# ::id 74
# ::snt The key themes of the Guidance are accountability and governance; data protection; lawfulness, fairness, and transparency;  security and data minimisation; and ensuring that individuals can effectively exercise their rights relating to their data.
(t / theme
      :ARG1-of (k / key-02)
      :poss (g / government-organization
            :name (n / name
                  :op1 "Guarding"))
      :domain (a / and
            :op1 (a2 / and
                  :op1 (a3 / accountable-02)
                  :op2 (g2 / govern-01))
            :op2 (p / protect-01
                  :ARG1 (d / data))
            :op3 (l / lawfulness)
            :op4 (f / fairness)
            :op5 (t2 / transparency)
            :op6 (a4 / and
                  :op1 (s / security)
                  :op2 (m / minimize-01
                        :ARG1 (d2 / data)))
            :op7 (e / ensure-01
                  :ARG1 (p2 / possible-01
                        :ARG1 (e2 / exercise-01
                              :ARG0 (ii / individual)
                              :ARG1 (r / right-05
                                    :ARG1 ii
                                    :ARG1-of (r2 / relate-01
                                          :ARG2 d2))
                              :ARG1-of (e3 / effective-04))))))


# ::id 80
# ::snt Civil rights and privacy advocates have expressed concern that this erodes accountability and fairness.
(e / express-01
      :ARG0 (p / person
            :ARG0-of (a / advocate-01
                  :ARG1 (a2 / and
                        :op1 (r / right-05
                              :mod (c / civil))
                        :op2 (p2 / privacy))))
      :ARG1 (c2 / concern-01
            :ARG0 (e2 / erode-01
                  :ARG0 (t / this)
                  :ARG1 (a3 / and
                        :op1 (a4 / accountable-02)
                        :op2 (f / fair-01)))
            :ARG1 p))


# ::id 80
# ::snt The question of how to ensure fairness in automated systems is complicated and will require thoughtful and candid  debate.
(a / and
      :op1 (c / complicate-01
            :ARG1 (q / question-01
                  :ARG1 (e / ensure-01
                        :ARG1 (f / fairness)
                        :ARG2 (s / system
                              :ARG1-of (a2 / automate-01)))))
      :op2 (r / require-01
            :ARG0 q
            :ARG1 (d / debate-01
                  :manner (t / thoughtful)
                  :manner (c2 / candid))))


# ::id 82
# ::snt (2020), “Implications of AI (un-)fairness in higher education admissions:  the effects of perceived AI (un-)fairness on exit, voice and organiza-tional  reputation” , Proceedings of the 2020 Conference on Fairness, Accountability, and T ransparency, pp.
(p / publication-91
      :ARG1 (p2 / publication
            :name (n / name
                  :op1 "Implications"
                  :op2 "of"
                  :op3 "AI"
                  :op4 "Fairness")
            :topic (a / admit-02
                  :ARG2 (e / educate-01
                        :ARG1-of (h / have-degree-91
                              :ARG2 (h2 / high-02)
                              :ARG3 (m / more))))
            :ARG1-of (m2 / mean-01
                  :ARG2 (a2 / affect-01
                        :ARG0 (f / fairness
                              :polarity -
                              :ARG1-of (p3 / perceive-01)
                              :mod (a3 / artificial))
                        :ARG1 (a4 / and
                              :op1 (e2 / exit-01)
                              :op2 (v / voice)
                              :op3 (r / reputation
                                    :mod (c / collective))))))
      :ARG4 (c2 / conference
            :name n
            :op1 "Proceeds"
            :op2 "of"
            :op3 "the"
            :op4 "2020"
            :op5 "Conference"
            :op6 "on"
            :op7 "Fairness,"
            :op8 "Accountability"
            :op9 "and"
            :op10 "Tranparency")
      :ARG7 (v2 / value-interval
            :op1 (p4 / pp)
            :op2 (p5 / pp))
      :time (d / date-entity
            :year 2020))


# ::id 82
# ::snt (2017), Jobs lost, jobs gained: workforce transi-tions  in a time of automation , McKinsey Global Institute, available at h ttps://www.mck insey.com/~/media/mckinsey/industries/public%20and%20social%20sector/our%20insigh ts/what%20the%20future%20of%20work%20will%20mean%20for%20
Page 80  A rtificial intelligence and educationjobs%20skills%20and%20wages/mgi-jobs-lost-jobs-gained-executive-summary-dec ember-6-2017.pdf, ac cessed 23 June 2022.Buolam wini J. and Gebru T. (2018), “Gender shades: intersectional accuracy dispar-ities  in commercial gender classification” , Conference on Fairness, Accountability, a nd Transparency, pp.
(p / publication-91
      :ARG0 (a / and
            :op1 (p2 / person
                  :name (n / name
                        :op1 "Buolam"
                        :op2 "W."
                        :op3 "J."))
            :op2 (p3 / person
                  :name (n2 / name
                        :op1 "Gebru"
                        :op2 "T."
                        :op3 "H.")))
      :ARG1 (p4 / publication
            :name (n3 / name
                  :op1 "Gender"
                  :op2 "Shadows"
                  :op3 ":"
                  :op4 ":"
                  :op5 "Interval"
                  :op6 "of"
                  :op7 "Workers"
                  :op8 "Transformation")
            :time (a2 / automate-01))
      :ARG4 (c / conference
            :name (n4 / name
                  :op1 "Conference"
                  :op2 "on"
                  :op3 "Fairness"
                  :op4 "and"
                  :op5 "Accountability"))
      :ARG7 (p5 / page
            :mod 80)
      :ARG2-of (a3 / available-02
            :location (u / url-entity
                  :value "http://www.mckinsey.com/~/media/mckinsey/industries/public%20and%20social%20sector/what%20the%20future%20of%20work%20will%20mean%20for%20.pdf"))
      :time (d / date-entity
            :year 2022
            :month 6
            :day 23))


# ::id 82
# ::snt References  P age 79Bietti E. (2020), “From ethics washing to ethics bashing: a view on tech ethics fr om within moral philosophy” , Proceedings of the 2020 Conference on Fairness, A ccountability, and Transparency , pp.
(c / cite-01
      :ARG2 (p / publication-91
            :ARG0 (p2 / person
                  :name (n / name
                        :op1 "Bietti"
                        :op2 "E."))
            :ARG1 (p3 / publication
                  :name (n2 / name
                        :op1 "From"
                        :op2 "Ethics"
                        :op3 "Wash"
                        :op4 "to"
                        :op5 "Ethics"
                        :op6 "Bash"
                        :op7 "A"
                        :op8 "View"
                        :op9 "of"
                        :op10 "Ethics"
                        :op11 "Fr om"
                        :op12 "within"
                        :op13 "moral"
                        :op14 " philosophy"))
            :ARG4 (c2 / conference
                  :name (n3 / name
                        :op1 "Proceeds"
                        :op2 "of"
                        :op3 "the"
                        :op4 "2020"
                        :op5 "Conference"
                        :op6 "on"
                        :op7 "Fairness,"
                        :op8 "A"
                        :op9 "Countable"
                        :op10 "and"
                        :op11 "Transparency"))
            :ARG7 (v / value-interval
                  :op1 79
                  :op2 89)
            :time (d / date-entity
                  :year 2020)))


# ::id 82
# ::snt How do we ensure that we move beyond focusing exclusively on  the technological dimension of AI to instead give equal attention to the human dimension  of AI – issues such as the impact of AI on human rights, autonomy and agenc y, alongside questions of transparency, fairness, trustworthiness and ethics.Giv en the novelty of our perspective on such a fast-growing domain of AI, we do not claim  that this report provides definitive answers.
(e / ensure-01
      :ARG0 (w / we)
      :ARG1 (m / move-01
            :ARG1 w
            :ARG2 (a / attend-02
                  :ARG0 w
                  :ARG1 (d / dimension
                        :mod (h / human)
                        :poss (ii / intelligent-01
                              :mod (a2 / artificial))
                        :example (ii2 / impact-01
                              :ARG0 ii
                              :ARG1 (a3 / and
                                    :op1 (r / right-05
                                          :ARG1 (h2 / human))
                                    :op2 (a4 / autonomy)
                                    :op3 (a5 / agency)))
                        :prep-along-with (q / question-01
                              :ARG1 (a6 / and
                                    :op1 (t / transparency)
                                    :op2 (f / fairness)
                                    :op3 (d2 / deserve-01
                                          :ARG1 (t2 / trust-01))
                                    :op4 (e2 / ethics))))
                  :ARG1-of (e3 / equal-01)
                  :ARG1-of (ii3 / instead-of-91
                        :ARG2 (f2 / focus-01
                              :ARG1 w
                              :ARG2 (d3 / dimension
                                    :mod (t3 / technology)
                                    :poss ii)
                              :ARG1-of (e4 / exclusive-02)))))
      :manner (a7 / amr-unknown)
      :ARG1-of (c / cause-01
            :ARG0 (n / novelty
                  :poss (p / perspective
                        :poss w
                        :topic (d4 / domain
                              :ARG1-of (g / grow-01
                                    :ARG1-of (f3 / fast-02))
                              :mod (s / such)
                              :mod ii))))
      :ARG1-of (c2 / claim-01
            :polarity -
            :ARG0 w
            :ARG1 (a8 / answer-01
                  :ARG0 (r2 / report
                        :mod (t4 / this))
                  :mod (d5 / definitive))))


# ::id 82
# ::snt They found that students rate algorithmic decision making higher than  human decision making with respect to procedural and distributive fairness; they  are perceived as more objective and fairer.
(m / multi-sentence
      :snt1 (f / find-01
            :ARG0 (t / they)
            :ARG1 (r / rate-01
                  :ARG0 (p / person
                        :ARG0-of (s / study-01))
                  :ARG1 (m2 / make-01
                        :ARG1 (d / decide-01)
                        :manner (a / algorithm))
                  :ARG2 (h / high-02
                        :ARG2-of (h2 / have-degree-91
                              :ARG1 m2
                              :ARG3 (m3 / more)
                              :ARG4 (m4 / make-01
                                    :ARG1 (d2 / decide-01)
                                    :mod (h3 / human)))
                        :topic (f2 / fairness
                              :mod (p2 / procedure)
                              :mod (d3 / distribute-01)))))
      :snt2 (p3 / perceive-01
            :ARG1 (t2 / they)
            :ARG2 (a2 / and
                  :op1 (o / objective
                        :ARG2-of (h4 / have-degree-91
                              :ARG1 t2
                              :ARG3 (m5 / more)))
                  :op2 (f3 / fair-01
                        :ARG2-of (h5 / have-degree-91
                              :ARG1 t2
                              :ARG3 (m6 / more))))))


# ::id 82
# ::snt In other words, it is the tech-n ology that de facto exercises rights, on behalf of or rather instead of the child.n ology that de facto exercises rights, on behalf of or rather instead of the child.f Right not to suffer from discrimination (fairness and bias)I n addition to the UNCRC, the obligation of states to combat and eliminate I n addition to the UNCRC, the obligation of states to combat and eliminate d iscrimination is set out in other international agreements: the International d iscrimination is set out in other international agreements: the International C onvention on the Elimination of All Forms of Racial Discrimination, the C onvention on the Elimination of All Forms of Racial Discrimination, the C onvention on the Elimination of All Forms of Discrimination against Women, C onvention on the Elimination of All Forms of Discrimination against Women, 
Page 54  A rtificial intelligence and educationand the International Convention on the Protection of the Rights of All Migrant W orkers and Members of Their Families.W orkers and Members of Their Families.B uilding on these various conventions, it is clear that wherever AI is imple-B uilding on these various conventions, it is clear that wherever AI is imple-m ented, by design it must be non-discriminatory, fair and inclusive throughout m ented, by design it must be non-discriminatory, fair and inclusive throughout i ts entire lifecycle (from design to use) (Leslie et al.
(m / multi-sentence
      :snt1 (c / clear-06
            :ARG1 (o / obligate-01
                  :ARG1 (a / and
                        :op1 (d / discriminate-02
                              :polarity -)
                        :op2 (f / fair-01)
                        :op3 (ii / include-91
                              :ARG2 (d2 / discriminate-02)))
                  :ARG2 (a2 / and
                        :op1 (t / technology)
                        :op2 (o2 / operate-01)
                        :ARG0-of (e / exercise-01
                              :ARG1 (r / right-05)
                              :mod (d3 / de-facto)
                              :ARG1-of (ii2 / instead-of-91
                                    :ARG2 (r2 / right-05
                                          :ARG1 (c2 / child)))))
                  :location (a3 / agree-01
                        :mod (ii3 / international)
                        :example (a4 / and
                              :op1 (t2 / treaty
                                    :name (n / name
                                          :op1 "C"
                                          :op2 "on"
                                          :op3 "the"
                                          :op4 "Eliminate"
                                          :op5 "of"
                                          :op6 "All"
                                          :op7 "Formations"
                                          :op8 "of"
                                          :op9 "Racial"
                                          :op10 "Discrimination"))
                              :op2 (t3 / treaty
                                    :name (n2 / name
                                          :op1 "C"
                                          :op2 "on"
                                          :op3 "the"
                                          :op4 "Eliminate"
                                          :op5 "of"
                                          :op6 "All"
                                          :op7 "Formations"
                                          :op8 "of"
                                          :op9 "Racial"
                                          :op10 "Discrimination"))
                              :op3 (t4 / treaty
                                    :name (n3 / name
                                          :op1 "International"
                                          :op2 "Convention"
                                          :op3 "on"
                                          :op4 "the"
                                          :op5 "Protection"
                                          :op6 "of"
                                          :op7 "the"
                                          :op8 "Rights"
                                          :op9 "of"
                                          :op10 "All"
                                          :op11 "Migrant"
                                          :op12 "and"
                                          :op13 "Members"
                                          :op14 "of"
                                          :op15 "Their"
                                          :op16 "Family"))))
                  :location (r3 / regardless-91
                        :ARG2 (ii4 / impact-01
                              :ARG0 (ii5 / intelligent-01
                                    :mod (a5 / artificial))
                              :ARG1 (c3 / convention
                                    :mod (v / various)
                                    :mod (t5 / this))))))
      :mod (w / word
            :mod (o3 / other))
      :mod (p / page
            :mod 54)
      :snt2 (o4 / obligate-01
            :ARG1 (s / state)
            :ARG2 (a6 / and
                  :op1 (c4 / combat-01
                        :ARG0 s
                        :ARG1 (d4 / discriminate-02))
                  :op2 (e2 / eliminate-01
                        :ARG0 s
                        :ARG1 d4))
            :duration (l / lifetime
                  :mod (e3 / entire)
                  :source (d5 / design-01)
                  :destination (u / use-01))))


# ::id 82
# ::snt With an  increasing use of algorithmic decision-making systems in general, “fairness concerns ar e gaining momentum in academic and public discourses” (Marcinkowski et al.
(c / concern-01
      :ARG0 (f / fairness)
      :ARG1 (g / gain-02
            :ARG0 f
            :ARG1 (m / momentum)
            :location (d / discourse-01
                  :ARG1 (a / and
                        :op1 (a2 / academia)
                        :op2 (p / public))))
      :ARG1-of (c2 / cause-01
            :ARG0 (u / use-01
                  :ARG1 (s / system
                        :instrument-of (m2 / make-01
                              :ARG1 (d2 / decide-01))
                        :mod (a3 / algorithm))
                  :ARG1-of (ii / increase-01)
                  :ARG1-of (g2 / general-02)))
      :ARG1-of (d3 / describe-01
            :ARG0 a
            :op1 (p2 / person
                  :name (n / name
                        :op1 "Marcinkowski"))
            :op2 (p3 / person
                  :mod (o / other))))


# ::id 82
# ::snt These basic  rights are based on shared values like dignity, fairness, equality, respect and independenc e. These values are defined and protected by law.
(m / multi-sentence
      :snt1 (b / base-02
            :ARG1 (r / right-05
                  :mod (b2 / basic)
                  :mod (t / this))
            :ARG2 (v / value
                  :ARG1-of (s / share-01)
                  :example (a / and
                        :op1 (d / dignity)
                        :op2 (f / fairness)
                        :op3 (e / equal-01)
                        :op4 (r2 / respect-01)
                        :op5 (d2 / depend-01
                              :polarity -))))
      :snt2 (a2 / and
            :op1 (d3 / define-01
                  :ARG0 (l / law)
                  :ARG1 (v2 / value
                        :mod (t2 / this)))
            :op2 (p / protect-01
                  :ARG0 l
                  :ARG1 v2)))


# ::id 82
# ::snt Page 28  A rtificial intelligence and educationincludes 20 competences, grouped into values, attitudes, skills and knowledge and cr itical understanding:f values: valuing human dignity and human rights, cultural diversity, democracy, justic e, fairness, equality and the rule of law;f attitudes: openness to cultural otherness and to other beliefs, world views and pr actices, respect, civic-mindedness, responsibility, self-efficacy, tolerance of ambiguit y;f skills: autonomous learning skills, analytical and critical thinking skills, skills of  listening and observing, empathy, flexibility and adaptability, linguistic, c ommunicative and plurilingual skills, co-operation skills, conflict-resolution sk ills;f knowledge and critical understanding: knowledge and critical understanding of the  self, of language and communication and of the world: politics, law, human r ights, culture, cultures, religions, history, media, economies, environment, sustainabilit y.F inally here, the 2020 United Nations Development Programme report37 reiterated tha t education has more than an instrumental role – its purpose is transformative thr ough exposure to broad human values and the promotion of critical thinking, to f oster politically aware and active people.
(r / reiterate-01
      :ARG0 (p / publication
            :mod 37
            :ARG1-of (r2 / report-01
                  :ARG0 (o / organization
                        :name (n / name
                              :op1 "United"
                              :op2 "Nations"
                              :op3 "Development"
                              :op4 "Programme")))
            :time (d / date-entity
                  :year 2020))
      :ARG1 (h / have-purpose-91
            :ARG1 (e / educate-01)
            :ARG2 (a / and
                  :op1 (v / value)
                  :op2 (a2 / attitude)
                  :op3 (s / skill)
                  :op4 (k / know-01)
                  :op5 (u / understand-01
                        :mod (c / circuit))
                  :ARG1-of (g / group-01
                        :ARG2 (a3 / and
                              :op1 v
                              :op2 a2
                              :op3 (s2 / skill
                                    :mod (l / learn-01)
                                    :mod (a4 / autonomy))
                              :op4 (s3 / skill
                                    :mod (a5 / analyze-01)
                                    :mod (t / think-01))
                              :op5 (s4 / skill
                                    :topic (a6 / and
                                          :op1 (l2 / listen-01)
                                          :op2 (o2 / observe-01)))
                              :op6 (e2 / empathy)
                              :op7 (f / flexibility)
                              :op8 (a7 / adapt-01)
                              :op9 (s5 / skill
                                    :mod (l3 / linguistics))
                              :op10 (s6 / skill
                                    :mod (c2 / cooperate-01))
                              :op11 (r3 / resolve-01
                                    :ARG1 (c3 / conflict-01))
                              :op12 (a8 / and
                                    :op1 (b / believe-01
                                          :mod (o3 / other))
                                    :op2 (v2 / view-02
                                          :mod (w / world))
                                    :op3 (a9 / act-02
                                          :mod (o4 / other))
                                    :op4 (r4 / respect-01)
                                    :op5 (r5 / responsible-01)
                                    :op6 (t2 / tolerate-01
                                          :ARG1 (a10 / ambiguity))
                                    :op7 (e3 / economy)
                                    :op8 (e4 / environment)
                                    :op9 (r6 / rule-01
                                          :ARG1 (l4 / law)))))))
      :mod (m / more-than
            :op1 (r7 / role
                  :mod (ii / instrumental)))
      :medium (p2 / page
            :mod 28))


# ::id 82
# ::snt However, by 2020, GRADE had been dropped because of its v arious biases.30 Nonetheless, AI is increasingly being used to support admissions, with  a focus on fairness and the institutions’ reputations (Dennis 2018; Marcinkowski et al .
(m / multi-sentence
      :snt1 (c / contrast-01
            :ARG2 (d / drop-05
                  :ARG1 (g / grade)
                  :time (b / by
                        :op1 (d2 / date-entity
                              :year 2020))
                  :ARG1-of (c2 / cause-01
                        :ARG0 (b2 / bias-01
                              :ARG1 g
                              :mod (v / versatile)))))
      :snt2 (h / have-concession-91
            :li 30
            :ARG1 (u / use-01
                  :ARG1 (ii / intelligent-01
                        :mod (a / artificial))
                  :ARG2 (s / support-01
                        :ARG0 ii
                        :ARG1 (a2 / admit-02)
                        :manner (f / focus-01
                              :ARG1 (a3 / and
                                    :op1 (f2 / fairness)
                                    :op2 (r / reputation
                                          :poss (ii2 / institution)))))
                  :manner (ii3 / increase-01)))
      :snt3 (p / publication-91
            :ARG0 (p2 / person
                  :name (n / name
                        :op1 "Dennis"))
            :time (d3 / date-entity
                  :year 2018))
      :snt4 a3
      :op1 (p3 / person
            :name (n2 / name
                  :op1 "Marcinkowski"))
      :op2 (p4 / person
            :mod (o / other)))


# ::id 82
# ::snt Interested in perceptions of (un)fairness of the  application of algorithmic decision making for admissions in higher education, M arcinkowski and colleagues (2020) carried out a survey of 304 students at a German univ ersity to assess their attitudes and perceptions of algorithmic versus human decision  making.
(c / carry-out-03
      :ARG0 (a / and
            :op1 (p / person
                  :name (n / name
                        :op1 "M"
                        :op2 "Arcinkowski"))
            :op2 (p2 / person
                  :ARG0-of (h / have-rel-role-91
                        :ARG1 p
                        :ARG2 (c2 / colleague)))
            :ARG1-of (ii / interest-01
                  :ARG2 (t / thing
                        :ARG1-of (p3 / perceive-01
                              :ARG2 (f / fair-01
                                    :polarity -
                                    :ARG1 (a2 / apply-02
                                          :ARG1 (m / make-01
                                                :ARG1 (d / decide-01
                                                      :manner (a3 / algorithm)))
                                          :ARG2 (a4 / admit-02
                                                :ARG2 (e / educate-01
                                                      :ARG1-of (h2 / have-degree-91
                                                            :ARG2 (h3 / high-02)
                                                            :ARG3 (m2 / more))))))))))
      :ARG1 (s / survey-01
            :ARG1 (p4 / person
                  :quant 304
                  :ARG0-of (s2 / study-01)
                  :location (u / university
                        :location (c3 / country
                              :name (n2 / name
                                    :op1 "Germany"))))
            :ARG2 (a5 / assess-01
                  :ARG0 a
                  :ARG1 (a6 / and
                        :op1 (a7 / attitude
                              :poss p4)
                        :op2 (t2 / thing
                              :ARG1-of (p5 / perceive-01
                                    :ARG0 p4)
                              :topic (m3 / make-01
                                    :ARG1 (d2 / decide-01)
                                    :manner a3
                                    :ARG1-of (c4 / compare-01
                                          :ARG2 (m4 / make-01
                                                :ARG1 (d3 / decide-01
                                                      :ARG0 (h4 / human)))))))))
      :time (d4 / date-entity
            :year 2020))


# ::id 82
# ::snt In the context of AI in education, this translates as  children not having the same capacity as adults to understand issues such as bias and  fairness, to give genuinely informed consent, or to understand or contest the eff ects of AI-based recommendations and predictions on their lives.I n any case, there is little evidence of the widespread adoption of learner-centric appr oaches in AI in education, despite claims to the contrary by some commercial pla yers.
(h / have-concession-91
      :ARG1 (e / evidence-01
            :ARG1 (a / adopt-01
                  :ARG1 (a2 / application
                        :ARG1-of (c / center-01
                              :ARG2 (p / person
                                    :ARG0-of (l / learn-01)))
                        :mod (ii / intelligent-01
                              :mod (a3 / artificial)))
                  :ARG1-of (s / spread-02
                        :ARG1-of (w / wide-02))
                  :location (c2 / context
                        :topic (e2 / educate-01)))
            :quant (l2 / little))
      :ARG2 (c3 / claim-01
            :ARG0 (p2 / person
                  :mod (c4 / commerce)
                  :quant (s2 / some))
            :ARG1 (c5 / capable-01
                  :polarity -
                  :ARG1 (c6 / child)
                  :ARG2 (o / or
                        :op1 (u / understand-01
                              :ARG0 c6
                              :ARG1 (ii2 / issue-02
                                    :example (a4 / and
                                          :op1 (b / bias-01)
                                          :op2 (f / fairness))))
                        :op2 (c7 / consent-01
                              :ARG0 c6
                              :ARG1-of (ii3 / inform-01
                                    :manner (g / genuine)))
                        :op3 (o2 / or
                              :op1 (u2 / understand-01
                                    :ARG0 c6
                                    :ARG1 (a5 / affect-01
                                          :ARG0 (a6 / and
                                                :op1 (r / recommend-01)
                                                :op2 (p3 / predict-01)
                                                :ARG1-of (b2 / base-02
                                                      :ARG2 ii))
                                          :ARG1 (l3 / life
                                                :poss c6)))
                              :op2 (c8 / contest-01
                                    :ARG0 c6
                                    :ARG1 a6))))))


# ::id 82
# ::snt In 2019, Jobin and colleagues (2019) identified 84 published sets of ethical pr inciples for AI, which they concluded converged on five areas: transparency, justice and  fairness, non-maleficence, responsibility and privacy.
(ii / identify-01
      :ARG0 (a / and
            :op1 (p / person
                  :name (n / name
                        :op1 "Jobin"))
            :op2 (p2 / person
                  :ARG0-of (h / have-rel-role-91
                        :ARG1 p
                        :ARG2 (c / colleague))))
      :ARG1 (s / set
            :quant 84
            :ARG1-of (p3 / publish-01)
            :consist-of (p4 / principle
                  :mod (e / ethics))
            :purpose (ii2 / intelligent-01
                  :mod (a2 / artificial))
            :ARG0-of (c2 / converge-01
                  :ARG1 (a3 / area
                        :quant 5
                        :ARG2-of (ii3 / include-91
                              :ARG1 (a4 / and
                                    :op1 (t / transparency)
                                    :op2 (j / justice)
                                    :op3 (f / fairness)
                                    :op4 (m / maleficence
                                          :polarity -)
                                    :op5 (r / responsible-02)
                                    :op6 (p5 / privacy))))
                  :ARG1-of (c3 / conclude-01
                        :ARG0 a)))
      :time (d / date-entity
            :year 2019))


# ::id 83
# ::snt Evaluation techniques you might use include holding retrospective roundtables at the end of the project; inviting an external expert or a ‘critical friend’ from a different team to observe and evaluate the project; request external consultations or audits

5.2 Repeatedly revisit the user need and public benefit throughout the project (fairness)

How has the user need changed?
(m / multi-sentence
      :snt1 (ii / include-01
            :ARG1 (a / and
                  :op1 (h / hold-04
                        :ARG1 (r / roundtable
                              :mod (r2 / retrospective))
                        :time (e / end-01
                              :ARG1 (p / project)))
                  :op2 (ii2 / invite-01
                        :ARG1 (o / or
                              :op1 (p2 / person
                                    :ARG1-of (e2 / expert-01)
                                    :mod (e3 / external))
                              :op2 (p3 / person
                                    :ARG0-of (h2 / have-rel-role-91
                                          :ARG1 p2
                                          :ARG2 (f / friend
                                                :ARG1-of (c / critical-02)))
                                    :source (t / team
                                          :ARG1-of (d / differ-02))))
                        :ARG2 (a2 / and
                              :op1 (o2 / observe-01
                                    :ARG0 o
                                    :ARG1 (p4 / project))
                              :op2 (e4 / evaluate-01
                                    :ARG0 o
                                    :ARG1 p4)))
                  :op3 (r3 / request-01
                        :ARG1 (o3 / or
                              :op1 (c2 / consult-01
                                    :ARG0 (e5 / external))
                              :op2 (a3 / audit-01))))
            :ARG2 (t2 / technique
                  :instrument-of (e6 / evaluate-01)
                  :ARG1-of (u / use-01
                        :ARG0 (y / you)
                        :ARG1-of (p5 / possible-01))))
      :snt2 (r4 / revisit-01
            :li 5.2
            :ARG1 (a4 / and
                  :op1 (n / need-01
                        :ARG0 (p6 / person
                              :ARG0-of (u2 / use-01)))
                  :op2 (b / benefit-01
                        :ARG1 (p7 / public)))
            :ARG1-of (r5 / repeat-01)
            :time (t3 / throughout
                  :op1 (p8 / project))
            :snt3 (c3 / change-01
                  :ARG1 (n2 / need-01
                        :ARG0 p6)
                  :manner (a5 / amr-unknown))))


# ::id 83
# ::snt In addition, the framework provides specific actions you can take at each stage of the project to advance transparency, accountability, and fairness.
(a / and
      :op2 (p / provide-01
            :ARG0 (f / framework)
            :ARG1 (a2 / act-02
                  :ARG1-of (s / specific-02)
                  :ARG1-of (t / take-01
                        :ARG0 (y / you)
                        :ARG1-of (p2 / possible-01)
                        :time (s2 / stage
                              :mod (e / each)
                              :subevent-of (p3 / project))
                        :purpose (a3 / advance-01
                              :ARG0 y
                              :ARG1 (a4 / and
                                    :op1 (t2 / transparency)
                                    :op2 (a5 / accountable-02)
                                    :op3 (f2 / fairness)))))))


# ::id 83
# ::snt Score the accountability of your project from 0 to 5 where:

0 means mechanisms for scrutiny, governance, or peer review for the project haven’t been established
5 means long-term oversight and public scrutiny mechanisms are built into the project cycle

Fairness
It is crucial to eliminate your project’s potential to have unintended discriminatory effects on individuals and social groups.
(m / multi-sentence
      :snt1 (s / score-01
            :ARG0 (y / you)
            :ARG1 (a / accountable-02
                  :ARG0 (p / project
                        :poss y))
            :ARG2 (s2 / score-on-scale-91
                  :ARG1 0
                  :ARG3 5)
            :location (a2 / amr-unknown))
      :snt2 (m2 / mean-01
            :ARG1 (p2 / publication
                  :name (n / name
                        :op1 "Global"
                        :op2 "Health"
                        :op3 "Reporting"
                        :op4 "Center"))
            :ARG2 (e / establish-01
                  :polarity -
                  :ARG1 (m3 / mechanism
                        :purpose (a3 / and
                              :op1 (s3 / scrutinize-01)
                              :op2 (g / govern-01)
                              :op3 (r / review-01
                                    :ARG0 (p3 / peer)
                                    :ARG1 (p4 / project))))))
      :snt3 (m4 / mean-01
            :ARG1 (p5 / publication
                  :name (n2 / name
                        :op1 "Global"
                        :op2 "Health"
                        :op3 "Reporting"
                        :op4 "Center"))
            :ARG2 (b / build-02
                  :ARG1 (a4 / and
                        :op1 (o / oversight
                              :ARG1-of (l / long-03))
                        :op2 (m5 / mechanism
                              :purpose (s4 / scrutinize-01
                                    :ARG1 (p6 / public))))
                  :ARG2 (c / cycle-02
                        :ARG1 (p7 / project))))
      :snt4 (c2 / crucial
            :domain (e2 / eliminate-01
                  :ARG1 (p8 / potential
                        :mod (a5 / affect-01
                              :ARG0 (p9 / project
                                    :poss y)
                              :ARG1 (a6 / and
                                    :op1 (ii / individual)
                                    :op2 (g2 / group
                                          :mod (s5 / society)))
                              :ARG2 (d / discriminate-02)
                              :ARG1-of (ii2 / intend-01
                                    :polarity -))))))


# ::id 83
# ::snt You can read more about fairness and its different types in the ‘Understanding artificial intelligence ethics and safety’ guide developed by the Government Digital Service and the Office for Artificial Intelligence.
(p / possible-01
      :ARG1 (r / read-01
            :ARG0 (y / you)
            :ARG1 (g / guide
                  :ARG1-of (d / develop-02
                        :ARG0 (a / and
                              :op1 (g2 / government-organization
                                    :name (n / name
                                          :op1 "Digital"
                                          :op2 "Service")
                                    :mod (g3 / government-organization
                                          :ARG0-of (g4 / govern-01)))
                              :op2 (g5 / government-organization
                                    :name (n2 / name
                                          :op1 "Office"
                                          :op2 "for"
                                          :op3 "Artificial"
                                          :op4 "Intelligence"))))
                  :topic (a2 / and
                        :op1 (f / fairness)
                        :op2 (t / type
                              :poss f
                              :ARG1-of (d2 / differ-02))))
            :ARG3 (p2 / publication
                  :name (n3 / name
                        :op1 "Understanding"
                        :op2 "Artificial"
                        :op3 "Intelligence"
                        :op4 "and"
                        :op5 "Safety"))
            :mod (m / more)))


# ::id 83
# ::snt Score the fairness of your project from 0 to 5 where:

0 means there is a significant risk that the project will result in harm or detrimental and discriminatory effects for the public or certain groups
5 means the project promotes just and equitable outcomes, has negligible detrimental effects, and is aligned with human rights considerations

Specific actions

1.
(m / multi-sentence
      :snt1 (s / score-01
            :ARG1 (f / fair-01
                  :ARG1 (p / project
                        :poss (y / you)))
            :ARG2 (s2 / score-on-scale-91
                  :ARG1 0
                  :ARG3 5)
            :location (a / amr-unknown))
      :snt2 (m2 / mean-01
            :ARG1 (r / risk-01
                  :ARG2 (r2 / result-01
                        :ARG1 (p2 / project)
                        :ARG2 (o / or
                              :op1 (h / harm-01
                                    :ARG0 p2
                                    :ARG1 (o2 / or
                                          :op1 (p3 / public)
                                          :op2 (g / group
                                                :mod (c / certain))))
                              :op2 (a2 / and
                                    :op1 (h2 / harm-01
                                          :ARG0 p2
                                          :ARG1 o2)
                                    :op2 (d / discriminate-02
                                          :ARG0 p2))))
                  :ARG1-of (s3 / significant-02)))
      :snt3 (m3 / mean-01
            :ARG1 (a3 / and
                  :op1 (p4 / promote-02
                        :ARG0 (p5 / project)
                        :ARG1 (o3 / outcome
                              :mod (j / just)
                              :mod (e / equitable)))
                  :op2 (h3 / have-03
                        :ARG0 p5
                        :ARG1 (a4 / affect-01
                              :ARG0 p5
                              :ARG2 (h4 / harm-01
                                    :mod (n / negligible))))
                  :op3 (a5 / align-01
                        :ARG1 p5
                        :ARG2 (c2 / consider-02
                              :ARG1 (r3 / right-05
                                    :ARG1 (h5 / human)))))
            :ARG1-of (r4 / request-confirmation-91))
      :snt4 (m4 / mean-01
            :ARG1 (a6 / act-02
                  :ARG1-of (s4 / specific-02)
                  :ARG1-of (m5 / mean-01
                        :ARG2 (t / thing
                              :name (n2 / name
                                    :op1 "nero"
                                    :op2 "1"))))
            :ARG2 (a7 / and
                  :op1 (a8 / atrocious)
                  :op2 (a9 / atrocious))))


# ::id 83
# ::snt 1.2 Understand unintended consequences of your project (fairness)

What would be the harm in not using data?
(a / and
      :op1 (u / understand-01
            :li 2
            :ARG1 (c / consequence-03
                  :ARG1 (p / project
                        :poss (y / you))
                  :ARG2 (f / fairness)
                  :ARG1-of (ii / intend-01
                        :polarity -)))
      :op2 (h / harm-01
            :ARG0 (u2 / use-01
                  :polarity -
                  :ARG1 (d / data))
            :ARG2 (a2 / amr-unknown)))


# ::id 83
# ::snt 1.3 Human rights considerations (fairness)

How does the design and implementation of the project or algorithm respect human rights and democratic values?
(m / multi-sentence
      :snt1 (c / consider-02
            :li 1.3
            :ARG1 (r / right-05
                  :ARG1 (h / human))
            :ARG1-of (m2 / mean-01
                  :ARG2 (f / fairness)))
      :snt2 (r2 / respect-01
            :ARG0 (a / and
                  :op1 (d / design-01
                        :ARG1 (o / or
                              :op1 (p / project)
                              :op2 (a2 / algorithm)))
                  :op2 (ii / implement-01
                        :ARG1 o))
            :ARG1 (a3 / and
                  :op1 (r3 / right-05
                        :ARG1 h)
                  :op2 (v / value
                        :mod (d2 / democracy)))
            :manner (a4 / amr-unknown)))


# ::id 83
# ::snt 2.2 Ensure diversity within your team (fairness)

How have you ensured diversity in your team?
(e / ensure-01
      :li 2
      :ARG0 (y / you)
      :ARG1 (d / diversity
            :location (t / team
                  :poss y)
            :ARG1-of (m / mean-01
                  :ARG2 (f / fairness)))
      :manner (a / amr-unknown))


# ::id 83
# ::snt 3.6 Ensure the project’s compliance with the Equality Act 2010 (fairness)
Data analysis or automated decision making must not result in outcomes that lead to discrimination as defined in the Equality Act 2010.
(e / ensure-01
      :li 3.6
      :ARG1 (c / comply-01
            :ARG0 (p / project)
            :ARG1 (l / law
                  :name (n / name
                        :op1 "Equality"
                        :op2 "Act"
                        :op3 2010
                        :op4 "Fairness")))
      :ARG1-of (c2 / cause-01
            :ARG0 (o / obligate-01
                  :ARG2 (r / result-01
                        :polarity -
                        :ARG1 (o2 / or
                              :op1 (a / analyze-01
                                    :ARG1 (d / data))
                              :op2 (m / make-01
                                    :ARG1 (d2 / decide-01)
                                    :ARG1-of (a2 / automate-01)))
                        :ARG2 (o3 / outcome
                              :ARG0-of (l2 / lead-03
                                    :ARG2 (d3 / discriminate-02)))
                        :ARG1-of (d4 / define-01
                              :ARG0 l)))))


# ::id 83
# ::snt 5.3 Check how your project influences policy (fairness)

How accurately are the insights from the project used in the practical policy context?
(c / check-01
      :li 5
      :mode imperative
      :ARG0 (y / you)
      :ARG1 (t / thing
            :manner-of (ii / influence-01
                  :ARG0 (p / project
                        :poss y)
                  :ARG1 (p2 / policy-01
                        :ARG2 (t2 / thing
                              :ARG1-of (m / mean-01
                                    :ARG2 (f / fairness))))))
      :ARG2 (t3 / thing
            :manner-of (u / use-01
                  :ARG1 (ii2 / insight
                        :source p)
                  :location (c2 / context
                        :mod (p3 / policy-01
                              :ARG1-of (p4 / practical-02))))
            :mod (a / accurate
                  :degree (a2 / amr-unknown))))


# ::id 83
# ::snt 4.3 Bias in data (fairness)

How has the data being used to train a model been assessed for potential bias?
(a / assess-01
      :li 4.3
      :ARG1 (d / data
            :ARG1-of (u / use-01
                  :ARG2 (t / train-01
                        :ARG1 (m / model))))
      :ARG2 (b / bias-01
            :ARG1 d
            :mod (p / potential))
      :manner (a2 / amr-unknown)
      :ARG1-of (m2 / mean-01
            :ARG2 (b2 / bias-01
                  :ARG1 (d2 / data)
                  :ARG1-of m2
                  :ARG2 (f / fairness))))


# ::id 85
# ::snt org.uk/for-organisations/guide-to-data-protection/guide-to-the-general-data-protectionregulation-gdpr/principles/lawfulness-fairness-and-transparency/ Review into bias in algorithmic decision-making: The issue
Centre for Data Ethics and Innovation 30Notions of fairness Notions of fair decision-making (whether  human or algorithmic) are typically gathered  into two broad categories: • Procedural fairness  is concerned with ‘fair treatment’  of people, i.e.
(m / multi-sentence
      :snt1 (c / concern-02
            :ARG0 (f / fairness
                  :mod (p / procedure))
            :ARG1 (t / treat-01
                  :ARG1 (p2 / person)
                  :ARG1-of (f2 / fair-01)))
      :snt2 (g / gather-01
            :ARG1 (n / notion
                  :topic (m2 / make-01
                        :ARG1 (d / decide-01)
                        :ARG1-of (f3 / fair-01)
                        :ARG1-of (r / regardless-91
                              :ARG2 (m3 / make-01
                                    :ARG1 (d2 / decide-01)
                                    :manner (a / algorithm)))))
            :ARG3 (c2 / category
                  :quant 2
                  :ARG1-of (b / broad-02))
            :ARG1-of (t2 / typical-02))
      :snt3 (r2 / review-01
            :ARG1 (b2 / bias-01
                  :ARG1 (m4 / make-01
                        :ARG1 (d3 / decide-01)
                        :manner a)))
      :topic-of (p3 / publication
            :name (n2 / name
                  :op1 "Centre"
                  :op2 "for"
                  :op3 "Data"
                  :op4 "Ethics"
                  :op5 "and"
                  :op6 "Innovation"
                  :op7 30)))


# ::id 85
# ::snt • Outcome fairness is concerned with what decisions  are made i.e.
(c / concern-02
      :ARG0 (f / fairness
            :mod (o / outcome))
      :ARG1 (t / thing
            :ARG1-of (d / decide-01)
            :ARG1-of (m / mean-01
                  :ARG2 (e / event))))


# ::id 85
# ::snt The concept of what  a fair outcome means is of course highly subjective;  there are multiple different definitions of outcome  fairness.
(a / and
      :op1 (s / subjective-03
            :ARG1 (c / concept
                  :topic (t / thing
                        :ARG2-of (m / mean-01
                              :ARG1 (o / outcome
                                    :ARG1-of (f / fair-01)))))
            :degree (h / high-02)
            :mod (o2 / of-course))
      :op2 (d / define-01
            :ARG1 (o3 / outcome
                  :ARG1-of (f2 / fair-01))
            :ARG1-of (d2 / differ-02)
            :quant (m2 / multiple)))


# ::id 85
# ::snt Some of these definitions are complementary to each  other, and none alone can capture all notions of fairness.
(a / and
      :op1 (c / complement-01
            :ARG0 (t / thing
                  :ARG2-of (d / define-01)
                  :quant (s / some)
                  :ARG1-of (ii / include-91
                        :ARG2 (t2 / thing
                              :ARG2-of (d2 / define-01)
                              :mod (t3 / this))))
            :ARG1 t)
      :op2 (p / possible-01
            :polarity -
            :ARG1 (c2 / capture-01
                  :ARG0 (n / none
                        :mod (a2 / alone))
                  :ARG1 (n2 / notion
                        :topic (f / fair-01)
                        :mod (a3 / all)))))


# ::id 85
# ::snt This is particularly relevant for the use of protected  characteristics data to measure and mitigate  algorithmic bias, the lawful use of bias mitigation  techniques, identifying new forms of bias beyond  existing protected characteristics, and for sectorspecific measures of algorithmic fairness beyond  discrimination.
(r / relevant-01
      :ARG1 (t / this)
      :ARG2 (a / and
            :op1 (u / use-01
                  :ARG1 (d / data
                        :topic (c / characteristic-02
                              :ARG1-of (p / protect-01)))
                  :ARG2 (a2 / and
                        :op1 (m / measure-01
                              :ARG1 (b / bias-01
                                    :mod (a3 / algorithm)))
                        :op2 (m2 / mitigate-01
                              :ARG1 b)))
            :op2 (u2 / use-01
                  :ARG1 (t2 / technique
                        :ARG0-of (m3 / mitigate-01
                              :ARG1 (b2 / bias-01)))
                  :manner (l / lawful))
            :op3 (ii / identify-01
                  :ARG1 (f / form
                        :ARG1-of (n / new-01)
                        :mod (b3 / bias-01
                              :mod (b4 / beyond
                                    :op1 (c2 / characteristic-02
                                          :ARG1-of (p2 / protect-01)
                                          :ARG1-of (e / exist-01))))))
            :op4 (m4 / measure-01
                  :ARG1 (f2 / fair-01
                        :mod (a4 / algorithm))
                  :mod (s / sector)
                  :mod (s2 / specific)
                  :mod (b5 / beyond
                        :op1 (d2 / discriminate-02))))
      :mod (p3 / particular))


# ::id 85
# ::snt Even within  outcome fairness there are many mutually incompatible  definitions for a fair outcome.
(t / thing
      :ARG2-of (d / define-01
            :ARG1 (o / outcome
                  :ARG1-of (f / fair-01)))
      :quant (m / many)
      :ARG1-of (c / compatible
            :polarity -
            :mod (m2 / mutual))
      :concession (e / even-when
            :op1 (f2 / fair-01
                  :ARG1 (o2 / outcome))))


# ::id 85
# ::snt Two possible definitions of outcome  fairness in this example are: A.
(t / thing
      :quant 2
      :ARG2-of (d / define-01
            :ARG1 (f / fair-01
                  :mod (o / outcome))
            :ARG1-of (p / possible-01))
      :prep-in (e / example
            :mod (t2 / this))
      :li "A")


# ::id 85
# ::snt In the real world sex and income are not independent of  each other; the UK has a gender pay gap meaning that, on  average, men earn more than women.20 Given that gap,  it is mathematically impossible to achieve both A and B  simultaneously.This example is by no means exhaustive in highlighting  the possible conflicting definitions that can be made, with  a large collection of possible definitions identified in the  machine learning literature.21 In human decision-making we can often accept ambiguity  around this type of issue, but when determining if an  algorithmic decision-making process is fair, we have to  be able to explicitly determine what notion of fairness  we are trying to optimise for.
(m / multi-sentence
      :li 20
      :snt1 (d / depend-01
            :polarity -
            :ARG0 (a / and
                  :op1 (s / sex)
                  :op2 (ii / income))
            :ARG1 (o / other
                  :mod (e / each))
            :location (w / world
                  :ARG1-of (r / real-04)))
      :snt2 (c / contrast-01
            :li 21
            :ARG1 (p / possible-01
                  :ARG1 (a2 / accept-01
                        :ARG0 (w2 / we)
                        :ARG1 (a3 / ambiguity
                              :topic (ii2 / issue-02
                                    :mod (t / type
                                          :mod (t2 / this))))
                        :frequency (o2 / often)))
            :ARG2 (o3 / obligate-01
                  :ARG2 (p2 / possible-01
                        :ARG1 (d2 / determine-01
                              :ARG0 w2
                              :ARG1 (n / notion
                                    :topic (f / fair-01)
                                    :ARG1-of (o4 / optimize-01
                                          :ARG0 w2
                                          :ARG1-of (t3 / try-01
                                                :ARG0 w2)))
                              :ARG1-of (e2 / explicit-03)
                              :time-of o3))
                  :ARG0 w2
                  :ARG1 (f2 / fair-01
                        :ARG1 (p3 / process-02
                              :ARG1 (d3 / decide-01
                                    :manner (a4 / algorithm))))))
      :snt3 (g / give-01
            :ARG1 (g2 / gap
                  :mod (p4 / pay-01
                        :ARG3 (g3 / gender))
                  :ARG1-of (m2 / mean-01
                        :ARG2 (e3 / earn-01
                              :ARG0 (m3 / man)
                              :ARG1-of (a5 / average-04)
                              :ARG2-of (h / have-quant-91
                                    :ARG3 (m4 / more)
                                    :ARG4 (w3 / woman))
                              :ARG1-of h)))
            :ARG2 g2
            :mod (m5 / mathematics))
      :snt4 (e4 / exhaustive
            :mod (b / by-no-means)
            :domain (e5 / example
                  :mod (t4 / this))
            :prep-with (c2 / collect-01
                  :ARG1 (d4 / define-01
                        :ARG1-of (p5 / possible-01))
                  :mod (l / large)
                  :ARG1-of (ii3 / identify-01
                        :location (l2 / literature
                              :mod (m6 / machine
                                    :ARG0-of (l3 / learn-01)))))))


# ::id 85
# ::snt When determining if an algorithmic decisionmaking process is fair, we have to be able  to explicitly determine what notion of fairness  we are trying to optimise for.
(o / obligate-01
      :ARG2 (c / capable-01
            :ARG1 (w / we)
            :ARG2 (d / determine-01
                  :ARG0 w
                  :ARG1 (n / notion
                        :topic (f / fair-01)
                        :ARG2-of (o2 / optimize-01
                              :ARG0 w
                              :ARG1-of (t / try-01
                                    :ARG0 w)))
                  :ARG1-of (e / explicit-03)))
      :time (d2 / determine-01
            :ARG1 (f2 / fair-01
                  :ARG1 (p / process-02
                        :ARG1 (d3 / decide-01
                              :manner (a / algorithm))))))


# ::id 85
# ::snt 20 ONS, ‘Gender pay gap in the UK: 2019’;  https://www.ons.gov.uk/employmentandlabourmarket/peopleinwork/earningsandworkinghours/bulletins/genderpaygapintheuk/2019 21 A comprehensive review of different possibilities is given in, for example, Mehrabi, Ninareh; Morstatter, Fred; Saxena, Nripsuta;  Lerman, Kristina; Galstyan, Aram; ‘A Survey on Bias and  Fairness in Machine Learning’, 2019; https://arxiv.org/pdf/1908.09635.pdf or Chouldechova, Alexandra; Roth, Aaron; ‘The Frontiers of Fairness in Machine Learning’, 2018;  https://arxiv.org/ abs/1810.08810 Review into bias in algorithmic decision-making: The issue
Centre for Data Ethics and Innovation31Addressing fairness Even when we can agree what constitutes  fairness, it is not always clear how to  respond.
(m / multi-sentence
      :snt1 (r / review-01
            :ARG1 (t / thing
                  :ARG1-of (p / possible-01)
                  :ARG1-of (d / differ-02))
            :ARG0-of (e / exemplify-01
                  :ARG1 (a / and
                        :op1 (p2 / person
                              :name (n / name
                                    :op1 "Mehrabi")
                              :mod (p3 / person
                                    :name (n2 / name
                                          :op1 "Ninareh")))
                        :op2 (p4 / person
                              :name (n3 / name
                                    :op1 "Morstatter")
                              :mod (p5 / person
                                    :name (n4 / name
                                          :op1 "Fred")))
                        :op3 (p6 / person
                              :name (n5 / name
                                    :op1 "Saxena")
                              :mod (p7 / person
                                    :name (n6 / name
                                          :op1 "Nripsuta")))
                        :op4 (p8 / person
                              :name (n7 / name
                                    :op1 "Lerman")
                              :mod (p9 / person
                                    :name (n8 / name
                                          :op1 "Kristina")))
                        :op5 (p10 / person
                              :name (n9 / name
                                    :op1 "Galstyan")
                              :mod (p11 / person
                                    :name (n10 / name
                                          :op1 "Aram")))
                        :op6 (p12 / publication
                              :name (n11 / name
                                    :op1 "The"
                                    :op2 "Frontiers"
                                    :op3 "of"
                                    :op4 "Fairness"
                                    :op5 "in"
                                    :op6 "Machine"
                                    :op7 "Learning")
                              :time (d2 / date-entity
                                    :year 2018))))
            :snt2 (a2 / address-02
                  :ARG1 (f / fairness)
                  :concession (e2 / even-when
                        :op1 (p13 / possible-01
                              :ARG1 (a3 / agree-01
                                    :ARG0 (w / we)
                                    :ARG1 (t2 / thing
                                          :ARG0-of (c / constitute-01
                                                :ARG1 (f2 / fairness)))))))
            :time (d3 / date-entity
                  :year 2019
                  :month 7
                  :day 21)
            :medium (u / url-entity
                  :value "https://www.ons.gov.uk/employmentandlabourmarket/peopleinwork/earningsandworkinghours/bulletins/genderpaygapintheuk/1908.09635.html")))


# ::id 85
# ::snt Conflicting views about the value  of fairness definitions arise when the  application of a process intended to be fair  produces outcomes regarded as unfair.
(a / arise-02
      :ARG1 (v / view-02
            :ARG1 (v2 / value-02
                  :ARG1 (d / define-01
                        :ARG1 (f / fair-01)))
            :ARG0-of (c / conflict-01))
      :time (p / produce-01
            :ARG0 (a2 / apply-02
                  :ARG1 (p2 / process-02
                        :ARG1-of (f2 / fair-01
                              :ARG1-of (ii / intend-01))))
            :ARG1 (o / outcome
                  :ARG1-of (r / regard-01
                        :ARG2 (f3 / fair-01
                              :polarity -)))))


# ::id 85
# ::snt The first argument implies greater outcome fairness is  consistent with more accurate and fair decision-making.
(ii / imply-01
      :ARG0 (t / thing
            :ARG1-of (a / argue-01)
            :ord (o / ordinal-entity
                  :value 1))
      :ARG1 (c / consistent-01
            :ARG1 (f / fair-01
                  :ARG1 (o2 / outcome)
                  :ARG1-of (h / have-degree-91
                        :ARG2 (g / great)
                        :ARG3 (m / more)))
            :ARG2 (m2 / make-01
                  :ARG1 (d / decide-01)
                  :ARG1-of (h2 / have-degree-91
                        :ARG2 (a2 / and
                              :op1 (a3 / accurate)
                              :op2 (f2 / fair-01))
                        :ARG3 (m3 / more)))))


# ::id 85
# ::snt There are  a variety of techniques, for example steps to ensure  fairness in an interview-based recruitment process  might include: • Training interviewers to recognise and challenge their  own individual unconscious biases.
(t / technique
      :mod (v / variety)
      :example (p / possible-01
            :ARG1 (ii / include-01
                  :ARG1 (t2 / train-01
                        :ARG1 (a / and
                              :op1 (r / recognize-02
                                    :ARG0 (p2 / person
                                          :ARG0-of (ii2 / interview-01))
                                    :ARG1 (b / bias-01
                                          :ARG1 p2
                                          :mod (u / unconscious)
                                          :mod (ii3 / individual)))
                              :op2 (c / challenge-01
                                    :ARG0 p2
                                    :ARG1 b))
                        :ARG2 p2)
                  :ARG2 (s / step-01
                        :ARG2 (e / ensure-01
                              :ARG1 (f / fair-01
                                    :ARG1 (p3 / process-02
                                          :ARG1 (r2 / recruit-01)
                                          :ARG1-of (b2 / base-02
                                                :ARG2 (ii4 / interview-01)))))))))


# ::id 85
# ::snt Training interviewers to recognise and challenge their own individual unconscious biases is one technique used to ensure fairness in an interview-based recruitment process.
(t / technique
      :quant 1
      :ARG1-of (u / use-01
            :ARG2 (e / ensure-01
                  :ARG0 t
                  :ARG1 (f / fair-01)
                  :time (p / process-02
                        :ARG1 (r / recruit-01)
                        :ARG1-of (b / base-02
                              :ARG2 (ii / interview-01)))))
      :domain (t2 / train-01
            :ARG1 (a / and
                  :op1 (r2 / recognize-02
                        :ARG0 (p2 / person
                              :ARG0-of (ii2 / interview-01))
                        :ARG1 (b2 / bias-01
                              :ARG1 p2
                              :mod (u2 / unconscious)
                              :mod (ii3 / individual)))
                  :op2 (c / challenge-01
                        :ARG0 p2
                        :ARG1 b2))
            :ARG2 p2))


# ::id 85
# ::snt The need for conscious decisions about fairness:  In  data-driven systems, organisations need to address more  of these issues at the point a model is built, rather than  relying on human decision-makers to interpret guidance  appropriately (an algorithm can’t apply “common sense”  on a case-by-case basis).
(m / multi-sentence
      :snt1 (n / need-01
            :ARG1 (d / decide-01
                  :ARG3 (f / fairness)
                  :mod (c / conscious)))
      :snt2 (n2 / need-01
            :ARG0 (o / organization)
            :ARG1 (a / address-02
                  :ARG0 o
                  :ARG1 (ii / issue-02
                        :ARG0 (t / this)
                        :quant (m2 / more))
                  :time (b / build-01
                        :ARG1 (m3 / model))
                  :ARG1-of (ii2 / instead-of-91
                        :ARG2 (r / rely-01
                              :ARG0 o
                              :ARG1 (p / person
                                    :ARG0-of (d2 / decide-01)
                                    :mod (h / human))
                              :ARG2 (ii3 / interpret-01
                                    :ARG0 p
                                    :ARG1 (g / guide-01)
                                    :ARG1-of (a2 / appropriate-02)))))
            :location (s / system
                  :ARG1-of (d3 / drive-02
                        :ARG0 (d4 / data))))
      :snt3 (p2 / possible-01
            :polarity -
            :ARG1 (a3 / apply-02
                  :ARG0 (a4 / algorithm)
                  :ARG1 (s2 / sense-02
                        :ARG1-of (s3 / share-01))
                  :manner (b2 / basis
                        :mod (c2 / case-by-case)))))


# ::id 85
# ::snt Algorithmic decision-making will inevitably increase over  time; the aim should be to ensure that this happens  in a way that acts to challenge bias, increase fairness  and promote equality, rather than entrenching existing  problems.
(m / multi-sentence
      :snt1 (ii / increase-01
            :ARG1 (m2 / make-01
                  :ARG0 (a / algorithm)
                  :ARG1 (d / decide-01))
            :ARG1-of (a2 / avoid-01
                  :ARG1-of (p / possible-01
                        :polarity -))
            :mod (o / over-time))
      :snt2 (r / recommend-01
            :ARG1 (e / ensure-01
                  :ARG1 (ii2 / increase-01
                        :ARG1 (t / this)
                        :manner (a3 / act-02
                              :ARG0 t
                              :ARG1 (a4 / and
                                    :op1 (c / challenge-01
                                          :ARG0 t
                                          :ARG1 (b / bias-01))
                                    :op2 (ii3 / increase-01
                                          :ARG0 t
                                          :ARG1 (f / fairness))
                                    :op3 (p2 / promote-02
                                          :ARG0 t
                                          :ARG1 (e2 / equal-01)))
                              :ARG1-of (ii4 / instead-of-91
                                    :ARG2 (e3 / entrench-01
                                          :ARG0 t
                                          :ARG1 (p3 / problem
                                                :ARG1-of (e4 / exist-01)))))))))


# ::id 85
# ::snt “Algorithmic decision-making and the Cost of Fairness”, 2017; https://arxiv.org/pdf/1701.08230.pdf 170 Larson, Jeff; Mattu, Surya; Kirchner, Lauren; and Angwin, Julia; ‘How We Analyzed the COMPAS Recidivism Algorithm.’ 2016; https://www.propublica.org/article/how-we-analyzed-thecompas-recidivism-algorithm.
(p / publication-91
      :ARG0 (a / and
            :op1 (p2 / person
                  :name (n / name
                        :op1 "Larson")
                  :mod (p3 / person
                        :name (n2 / name
                              :op1 "Jeff")))
            :op2 (p4 / person
                  :name (n3 / name
                        :op1 "Mattu")
                  :mod (p5 / person
                        :name (n4 / name
                              :op1 "Surya")))
            :op3 (p6 / person
                  :name (n5 / name
                        :op1 "Kirchner")
                  :mod (p7 / person
                        :name (n6 / name
                              :op1 "Lauren")))
            :op4 (p8 / person
                  :name (n7 / name
                        :op1 "Angwin")
                  :mod (p9 / person
                        :name (n8 / name
                              :op1 "Julia"))))
      :ARG1 (p10 / publication
            :name (n9 / name
                  :op1 "How"
                  :op2 "We"
                  :op3 "Analyze"
                  :op4 "the"
                  :op5 "Compass"
                  :op6 "Recidivism"
                  :op7 "Algorithm"))
      :ARG4 (u / url-entity
            :value "https://www.propublica.org/article/how-we-analyzed-thecompas-recidivism-algorithm.pdf")
      :time (d / date-entity
            :year 2017))


# ::id 85
# ::snt The legislation does not elaborate further on the  meaning of fairness, but the ICO guides organisations that  “In general, fairness means that you should only handle  personal data in ways that people would reasonably expect  and not use it in ways that have unjustified adverse effects  on them.”19 Note that the discussion in this section is wider  than the notion in GDPR, and does not attempt to define  how the word fair should be interpreted in that context.2.5 Fairness Even in cases where fairness can be more  precisely defined, it can still be challenging to capture all relevant aspects of fairness in  a mathematical definition.
(m / multi-sentence
      :snt1 (c / contrast-01
            :li 19
            :ARG1 (e / elaborate-01
                  :polarity -
                  :ARG0 (l / legislate-01)
                  :ARG1 (m2 / mean-01
                        :ARG1 (f / fairness)
                        :ARG2 (t / thing)
                        :degree (f2 / further)))
            :ARG2 (g / guide-01
                  :ARG0 (g2 / government-organization
                        :name (n / name
                              :op1 "ICO"))
                  :ARG1 (o / organization)
                  :ARG2 (m3 / mean-01
                        :ARG1 (f3 / fairness)
                        :ARG2 (r / recommend-01
                              :ARG1 (h / handle-01
                                    :ARG0 (y / you)
                                    :ARG1 (d / data
                                          :ARG1-of (p / personal-02))
                                    :ARG2 (w / way
                                          :ARG1-of (e2 / expect-01
                                                :ARG0 (p2 / person)
                                                :ARG1-of (r2 / reasonable-02))))
                              :ARG2 (h2 / handle-01
                                    :polarity -
                                    :ARG0 y
                                    :ARG1 d
                                    :ARG2 (w2 / way
                                          :ARG0-of (a / affect-01
                                                :ARG1 p2
                                                :mod (a2 / adverse)
                                                :ARG1-of (j / justify-01
                                                      :polarity -)))))
                        :ARG1-of (g3 / general-02))))
      :snt2 (h3 / have-concession-91
            :ARG1 (p3 / possible-01
                  :ARG1 (c2 / challenge-01
                        :ARG2 (c3 / capture-01
                              :ARG1 (a3 / aspect
                                    :ARG1-of (r3 / relevant-01)
                                    :mod (a4 / all)
                                    :topic (f4 / fairness))
                              :manner (d2 / define-01
                                    :mod (m4 / mathematics)))
                        :mod (s / still)))
            :ARG2 (p4 / possible-01
                  :ARG1 (d3 / define-01
                        :ARG1 (f5 / fair-01)
                        :ARG1-of (h4 / have-degree-91
                              :ARG2 (p5 / precise)
                              :ARG3 (m5 / more)))))
      :snt3 (n2 / note-01
            :mode imperative
            :ARG0 y)
      :ARG1 (a5 / and
            :op1 h4
            :ARG1 (d4 / discuss-01
                  :location (s2 / section
                        :mod (t2 / this)))
            :ARG2 (w3 / wide-02
                  :ARG1 d4)
            :ARG3 m5)
      :ARG4 (n3 / notion
            :location (l2 / law
                  :name (n4 / name
                        :op1 "GDPR")))
      :op2 (a6 / attempt-01
            :polarity -
            :ARG0 d4
            :ARG1 (d5 / define-01
                  :ARG1 (t3 / thing
                        :ARG2-of (ii / interpret-01
                              :ARG1 (w4 / word
                                    :mod (f6 / fair-01))
                              :ARG1-of (r4 / recommend-01))
                        :location (c4 / context
                              :mod (t4 / that))))))


# ::id 85
# ::snt Humans  must choose which notions of fairness are appropriate for  a particular algorithm, and they need to be willing to do so  upfront when a model is built and a process is designed.
(a / and
      :op1 (o / obligate-01
            :ARG1 (h / human)
            :ARG2 (c / choose-01
                  :ARG0 h
                  :ARG1 (n / notion
                        :ARG1-of (a2 / appropriate-02
                              :ARG2 (a3 / algorithm
                                    :mod (p / particular)))
                        :topic (f / fairness))))
      :op2 (n2 / need-01
            :ARG0 h
            :ARG1 (w / will-02
                  :ARG0 h
                  :ARG1 c
                  :time (a4 / and
                        :op1 (b / build-01
                              :ARG1 (m / model))
                        :op2 (d / design-01
                              :ARG1 (p2 / process-01)))
                  :mod (u / upfront))))


# ::id 85
# ::snt Notions of fairness are neither universal  nor unambiguous, and they are often  inconsistent with one another.
(a / and
      :op1 (o / or
            :op1 (u / universal
                  :polarity -
                  :domain (n / notion
                        :topic (f / fairness)))
            :op2 (c / clear-06
                  :polarity -
                  :ARG1 n))
      :op2 (c2 / consistent-02
            :polarity -
            :ARG1 n
            :ARG2 (a2 / another
                  :mod (o2 / one))
            :frequency (o3 / often)))


# ::id 85
# ::snt Even in cases where fairness can be more precisely  defined, it can still be challenging to capture all  relevant aspects of fairness in a mathematical  definition.
(h / have-concession-91
      :ARG1 (p / possible-01
            :ARG1 (c / challenge-01
                  :ARG2 (c2 / capture-01
                        :ARG1 (a / aspect
                              :ARG1-of (r / relevant-01)
                              :mod (a2 / all)
                              :mod (f / fairness))
                        :ARG2 (d / define-01
                              :mod (m / mathematics)))
                  :mod (s / still)))
      :ARG2 (c3 / case-04
            :ARG1 (p2 / possible-01
                  :ARG1 (d2 / define-01
                        :ARG1 (f2 / fairness)
                        :ARG1-of (h2 / have-degree-91
                              :ARG2 (p3 / precise)
                              :ARG3 (m2 / more))))))


# ::id 85
# ::snt We want a system that is fair and accountable; one that  preserves, protects or improves fairness in decisions being  made with the use of algorithms.
(w / want-01
      :ARG0 (w2 / we)
      :ARG1 (s / system
            :ARG0-of (f / fair-01)
            :ARG0-of (a / accountable-02)
            :ARG0-of (o / or
                  :op1 (p / preserve-01
                        :ARG1 (f2 / fair-01
                              :ARG1 (d / decide-01
                                    :manner (u / use-01
                                          :ARG1 (a2 / algorithm)))))
                  :op2 (p2 / protect-01
                        :ARG1 f2)
                  :op3 (ii / improve-01
                        :ARG1 f2))))


# ::id 85
# ::snt Organisations need to identify what  fairness objectives they want to achieve and how they plan  to do this.
(n / need-01
      :ARG0 (o / organization)
      :ARG1 (ii / identify-01
            :ARG0 o
            :ARG1 (a / and
                  :op1 (o2 / objective
                        :topic (f / fair-01)
                        :ARG1-of (a2 / achieve-01
                              :ARG0 o
                              :ARG1-of (w / want-01
                                    :ARG0 o)))
                  :op2 (p / plan-01
                        :ARG0 o
                        :ARG1 (d / do-02
                              :ARG0 o
                              :ARG1 o2)))))


# ::id 85
# ::snt • Putting structures in place to gather data and monitor outcomes for fairness.
(p / put-03
      :ARG1 (s / structure)
      :ARG2 (ii / in-place)
      :purpose (a / and
            :op1 (g / gather-01
                  :ARG0 s
                  :ARG1 (d / data))
            :op2 (m / monitor-01
                  :ARG0 s
                  :ARG1 (o / outcome)
                  :ARG3 (f / fairness))))


# ::id 85
# ::snt • Recommendation 17: Cabinet Office  and the  Crown Commercial Service  should update model contracts  and framework agreements for public sector procurement to incorporate a set of minimum standards around  ethical use of AI, with particular focus on expected levels of transparency and explainability, and ongoing testing  for fairness.Next steps and future challenges This review has considered a complex and  rapidly evolving field.
(m / multi-sentence
      :snt1 (r / recommend-01
            :li 17
            :ARG1 r
            :ARG1 (u / update-01
                  :ARG0 (a / and
                        :op1 (g / government-organization
                              :name (n / name
                                    :op1 "Cabinet"
                                    :op2 "Office"))
                        :op2 (g2 / government-organization
                              :name (n2 / name
                                    :op1 "Crown"
                                    :op2 "Commercial"
                                    :op3 "Service")))
                  :ARG1 (a2 / and
                        :op1 (c / contract
                              :mod (m2 / model))
                        :op2 (a3 / agree-01
                              :ARG1 (p / procure-01
                                    :ARG0 (s / sector
                                          :ARG1-of (p2 / public-02)))
                              :mod (f / framework)))
                  :ARG2 (ii / incorporate-02
                        :ARG0 a
                        :ARG1 (s2 / set
                              :consist-of (s3 / standard
                                    :mod (m3 / minimum)
                                    :topic (u2 / use-01
                                          :ARG1 (ii2 / intelligent-01
                                                :mod (a4 / artificial))
                                          :mod (e / ethics)))
                              :ARG2-of (f2 / focus-01
                                    :ARG1 (a5 / and
                                          :op1 (l / level
                                                :mod (t / transparency))
                                          :op2 (l2 / level
                                                :mod (e2 / explain-01))
                                          :ARG1-of (e3 / expect-01))
                                    :op2 (t2 / test-01
                                          :ARG2 (f3 / fair-01)
                                          :ARG1-of (g3 / go-on-15))
                                    :mod (p3 / particular))))))
      :snt2 (c2 / consider-01
            :ARG0 (r2 / review-01
                  :mod (t3 / this))
            :ARG1 (f4 / field
                  :mod (c3 / complex)
                  :ARG1-of (e4 / evolve-01
                        :manner (r3 / rapid)))
            :ARG2 (a6 / and
                  :op1 (s4 / step-01
                        :mod (n3 / next))
                  :op2 (c4 / challenge-01
                        :time (f5 / future)))))


# ::id 85
# ::snt The issue   2.1 Introduction   2.2 The use of algorithms in decision-making   2.3 Bias  2.4 Discrimination and equality   2.5 Fairness   2.6 Applying ethical principles   2.7 The opportunity15  16   16  17  18   20  22  23  26  27  29  34  36
Background  and scopeChapter 1 
Centre for Data Ethics and Innovation16The adoption of data-driven technology  affects every aspect of our society and its  use is creating opportunities as well as new  ethical challenges.
(m / multi-sentence
      :snt1 (a / and
            :op1 (ii / introduce-02
                  :li 2.1)
            :op2 (u / use-01
                  :li 2.2
                  :ARG1 (a2 / algorithm)
                  :ARG2 (m2 / make-18
                        :ARG1 (d / decide-01)))
            :op3 (a3 / and
                  :li 2.3
                  :op1 (b / bias-01)
                  :op2 (e / equal-01))
            :op4 (a4 / and
                  :li 2.5
                  :op2 (f / fairness))
            :op5 (a5 / and
                  :li 2.6
                  :op1 (a6 / apply-02
                        :ARG1 (p / principle
                              :mod (e2 / ethics)))
                  :op2 (o / opportunity)
                  :op3 (o2 / opportunity
                        :li 15)
                  :op4 (o3 / opportunity
                        :li 16)
                  :op5 (o4 / opportunity
                        :li 17)
                  :op6 (o5 / opportunity
                        :li 18)
                  :op7 (o6 / opportunity
                        :li 19)
                  :op8 (o7 / opportunity
                        :li 20)
                  :op9 (o8 / opportunity
                        :li 23)
                  :op10 (o9 / organization
                        :name (n / name
                              :op1 "Centre"
                              :op2 "for"
                              :op3 "Data"
                              :op4 "Ethics"
                              :op5 "and"
                              :op6 "Innovation"
                              :op7 16))
                  :op11 o9
                  :name n
                  :op1 "European"
                  :op2 "Union"))
      :op12 o9
      :name n
      :op1 "European"
      :op2 "Union"
      :op13 o9
      :name n
      :op1 "European"
      :op3 "Union"
      :op14 o9
      :name n
      :op1 "European"
      :op2 "Union"
      :op15 o9
      :name n
      :op1 "European"
      :op3 "Union"
      :op16 o9
      :name n
      :op1 "European"
      :op2 "Union"
      :op17 o9
      :name n
      :op1 "European"
      :op3 "Union"
      :op18 o9
      :name n
      :op1 "European"
      :op2 "Union"
      :op19 o9
      :name n
      :op1 "European"
      :op3 "Union"
      :op20 o9
      :name n
      :op1 "European"
      :op2 "Union"
      :op21 "Union"
      :snt2 (a7 / and
            :li 36
            :op1 (b2 / background)
            :op2 (s / scope)))


# ::id 85
# ::snt 4 CDEI, ‘Landscape Summary: Bias in Algorithmic Decision-Making’, 2019; https://www.gov.uk/government/publications/landscape-summaries-commissioned-by-the-centre-for-data-ethicsand-innovation 5 CDEI, ‘Call for evidence summary of responses, Review into bias in algorithmic decision-making’, 2019; https://www.gov.uk/government/publications/responses-to-cdei-call-for-evidence/ cdei-bias-review-call-for-evidence-summary-of-responses 6 The Behavioural Insights Team, ‘The perceptions of fairness of algorithms and proxy information in financial services’, 2019; https://www.bi.team/publications/the-perception-of-fairnessof-algorithms-and-proxy-information-in-financial-services/ 7 Royal United Services Institute, Briefing Paper: ‘Data Analytics and Algorithmic Bias in Policing’, 2019;  https://www.rusi.org/sites/default/files/20190916_data_analytics_and_algorithmic_ bias_in_policing_web.pdf and Royal United Services Institute, Occasional Paper: ‘Data Analytics and Algorithms in Policing in England and Wales’, 2019; https://rusi.org/sites/default/files/ rusi_pub_165_2020_01_algorithmic_policing_babuta_final_web_copy.pdf 8 Main report: produced under contract https://cdeiuk.github.io/bias-mitigation-docs/Bias%20Identification%20and%20Mitigation.pdfReview into bias in algorithmic decision-making: Background and scope
The issue Chapter 2
Centre for Data Ethics and Innovation 21• Algorithms are structured processes, which have long  been used to aid human decision-making.
(m / multi-sentence
      :snt1 (p / publication-91
            :li 5
            :ARG0 (o / organization
                  :name (n / name
                        :op1 "CDEI"))
            :ARG1 (p2 / publication
                  :name (n2 / name
                        :op1 "Bias"
                        :op2 "in"
                        :op3 "Algorithmic"
                        :op4 "Decision-Making"))
            :ARG4 (p3 / publication
                  :name (n3 / name
                        :op1 "Occasional"
                        :op2 "Paper"))
            :time (d / date-entity
                  :year 2019)
            :medium (u / url-entity
                  :value "https://www.rusi.gov.uk/sites/default/files/ rusi_pub_165_2020_01_algorithmic_policing_babuta_final_web_copy.pdf"))
      :snt2 (p4 / publication
            :name (n4 / name
                  :op1 "Review"
                  :op2 "into"
                  :op3 "Bias"
                  :op4 "in"
                  :op5 "Algorithmic"
                  :op6 "Decision-Making")
            :medium (u2 / url-entity
                  :value "https://www.rusi.gov.uk/sites/default/files/20190916_data_analytics_and_algorithmic_ bias_in_policing_web.pdf"))
      :snt3 (p5 / publication
            :name (n5 / name
                  :op1 "The"
                  :op2 "Behavioural"
                  :op3 "Insights"
                  :op4 "Team")
            :ARG1 (p6 / publication
                  :name (n6 / name
                        :op1 "The"
                        :op2 "Perception"
                        :op3 "of"
                        :op4 "Fair-01"
                        :op5 "Algorithms"
                        :op6 "in"
                        :op7 "Policing"
                        :op8 "in"
                        :op9 "England"
                        :op10 "and"
                        :op11 "Wales"))
            :time (d2 / date-entity
                  :year 2019))
      :snt4 (p7 / publication
            :name (n7 / name
                  :op1 "The"
                  :op2 "Bias"
                  :op3 "Ethics"
                  :op4 "and"
                  :op5 "Innovation"
                  :op6 21)
            :medium (u3 / url-entity
                  :value "https://www.rusi.gov.uk/government/publications/responses-to-cdei-call-for-evidence/ cdei-bias-review-call-for-evidence-summary-of-responses")))


# ::id 85
# ::snt • There are multiple concepts of fairness,  some  of which are incompatible and many of which are  ambiguous.
(c / concept
      :topic (f / fair-01)
      :quant (m / multiple)
      :ARG2-of (ii / include-91
            :ARG1 (s / some
                  :ARG1-of (c2 / compatible
                        :polarity -)))
      :ARG2-of (ii2 / include-91
            :ARG1 (m2 / many
                  :ARG1-of (a / ambiguous-02))))


# ::id 85
# ::snt Fairness is about much  more than the absence of bias:  fair decisions need  to also be non-arbitrary, reasonable, consider equality  implications and respect the circumstances and  personal agency of the individuals concerned.• Despite concerns about ‘black box’ algorithms, in some  ways algorithms can be more transparent than human  decisions; unlike a human it is possible to reliably test  how an algorithm responds to changes in parts of the  input.
(m / multi-sentence
      :snt1 (c / concern-02
            :ARG0 (a / algorithm
                  :mod (b / box
                        :ARG1-of (b2 / black-04))))
      :snt2 (p / possible-01
            :ARG1 (t / transparent
                  :domain (a2 / algorithm)
                  :ARG2-of (h / have-degree-91
                        :ARG1 a2
                        :ARG3 (m2 / more)
                        :ARG4 (d / decide-01
                              :ARG0 (h2 / human))))
            :concession (r / resemble-01
                  :polarity -
                  :ARG1 (t2 / test-01
                        :ARG1 (t3 / thing
                              :manner-of (r2 / respond-01
                                    :ARG0 (a3 / algorithm)
                                    :ARG1 (t4 / thing
                                          :part-of (ii / input)
                                          :ARG1-of (c2 / change-01))))
                        :ARG1-of (r3 / rely-01
                              :ARG1-of (p2 / possible-01)))))
      :snt3 c
      :topic (f / fairness)
      :ARG1-of (m3 / mean-01
            :ARG2 (n / need-01
                  :ARG0 (d2 / decide-01
                        :ARG1-of (f2 / fair-01))
                  :ARG1 (a4 / and
                        :op1 (a5 / arbitrary-02
                              :polarity -
                              :ARG1 d2
                              :mod (a6 / also))
                        :op2 (r4 / reasonable-02
                              :ARG1 d2)
                        :op3 (c3 / consider-02
                              :ARG0 d2
                              :ARG1 (ii2 / implicate-01
                                    :ARG1 (e / equal-01)))
                        :op4 (r5 / respect-01
                              :ARG0 d2
                              :ARG1 (a7 / and
                                    :op1 (c4 / circumstance)
                                    :op2 (a8 / agency
                                          :ARG1-of (p3 / personal-02))
                                    :poss (ii3 / individual
                                          :ARG1-of c))))))
      :ARG1-of (h3 / have-quant-91
            :ARG3 m2
            :quant (m4 / much))
      :ARG4 (a9 / absent-01
            :ARG1 (b3 / bias-01)))


# ::id 85
# ::snt Human developers  and users of algorithms must decide the concepts  of fairness that apply to their context, and ensure  that algorithms deliver fair outcomes.
(o / obligate-01
      :ARG1 (a / and
            :op1 (p / person
                  :ARG0-of (d / develop-02)
                  :mod (h / human))
            :op2 (p2 / person
                  :ARG0-of (u / use-01
                        :ARG1 (a2 / algorithm))))
      :ARG2 (a3 / and
            :op1 (d2 / decide-01
                  :ARG0 a
                  :ARG1 (c / concept
                        :topic (f / fairness)
                        :ARG1-of (a4 / apply-02
                              :ARG2 (c2 / context
                                    :poss a))))
            :op2 (e / ensure-01
                  :ARG0 a
                  :ARG1 (d3 / deliver-01
                        :ARG0 a2
                        :ARG1 (o2 / outcome
                              :ARG1-of (f2 / fair-01))))))


# ::id 85
# ::snt • Fairness through unawareness is often not   enough to prevent bias:  ignoring protected  characteristics is insufficient to prevent algorithmic  bias and it can prevent organisations from identifying  and addressing bias.
(m / multi-sentence
      :snt1 (h / have-quant-91
            :polarity -
            :ARG1 (f / fairness
                  :manner (r / realize-01
                        :polarity -))
            :ARG3 (e / enough)
            :ARG6 (p / prevent-01
                  :ARG0 f
                  :ARG1 (b / bias-01))
            :frequency (o / often))
      :snt2 (a / and
            :op1 (s / suffice-01
                  :polarity -
                  :ARG0 (ii / ignore-01
                        :ARG1 (t / thing
                              :ARG2-of (c / characteristic-02)
                              :ARG1-of (p2 / protect-01)))
                  :ARG1 (p3 / prevent-01
                        :ARG0 ii
                        :ARG1 (b2 / bias-01
                              :mod (a2 / algorithm))))
            :op2 (p4 / possible-01
                  :ARG1 (p5 / prevent-01
                        :ARG0 ii
                        :ARG1 (a3 / and
                              :op1 (ii2 / identify-01
                                    :ARG0 (o2 / organization)
                                    :ARG1 (b3 / bias-01))
                              :op2 (a4 / address-02
                                    :ARG0 o2
                                    :ARG1 b3))))))


# ::id 85
# ::snt It is also critical for innovation that  algorithms are used in a way that is both fair, and   seen by the public to be fair.The issue: Summary Fairness through unawareness is often not  enough to prevent bias.
(m / multi-sentence
      :snt1 (c / critical-02
            :ARG1 (u / use-01
                  :ARG1 (a / algorithm)
                  :manner (a2 / and
                        :op1 (f / fair-01
                              :ARG1 a)
                        :op2 (s / see-01
                              :ARG0 (p / public)
                              :ARG1 f)))
            :ARG2 (ii / innovate-01)
            :mod (a3 / also))
      :snt2 (ii2 / issue-02
            :ARG0 (h / have-quant-91
                  :polarity -
                  :ARG1 (f2 / fairness
                        :manner (t / through
                              :op1 (r / realize-01
                                    :polarity -)))
                  :ARG3 (e / enough)
                  :ARG6 (p2 / prevent-01
                        :ARG0 f2
                        :ARG1 (b / bias-01))
                  :frequency (o / often))))


# ::id 85
# ::snt The growing  use of algorithms in decision-making has  raised concerns around bias and fairness.
(r / raise-01
      :ARG0 (u / use-01
            :ARG1 (a / algorithm)
            :ARG2 (m / make-18
                  :ARG1 (d / decide-01))
            :ARG1-of (g / grow-01))
      :ARG1 (c / concern-01
            :topic (a2 / and
                  :op1 (b / bias-01)
                  :op2 (f / fair-01))))


# ::id 85
# ::snt This highlights challenges  in defining what we mean by fairness,  which is a complex and long debated topic.
(h / highlight-01
      :ARG0 (t / this)
      :ARG1 (c / challenge-01
            :ARG2 (d / define-01
                  :ARG1 (t2 / thing
                        :ARG2-of (m / mean-01
                              :ARG0 (w / we)
                              :ARG1 (f / fairness))
                        :mod (t3 / topic
                              :ARG1-of (d2 / debate-01
                                    :ARG1-of (l / long-03))
                              :mod (c2 / complex))))))


# ::id 85
# ::snt Calibration is also capable of perpetuating pre-existing biases.Observational Demographic parity ('independence') Conditional demographic parity Equalised odds ('separation') Calibration ('sufficiency') Sub group fairness Individual fairnessCasual Unresolved discrimination Proxy discrimination Meritocratic fairness Counterfactual fairnessThe table below shows how the most commonly used approaches (and other examples) sit within wider definitions as  described above.
(m / multi-sentence
      :snt1 (c / capable-01
            :ARG1 (c2 / calibrate-01)
            :ARG2 (p / perpetuate-01
                  :ARG0 c2
                  :ARG1 (b / bias-01
                        :ARG1-of (e / exist-01
                              :time (b2 / before))))
            :mod (a / also))
      :snt2 (a2 / and
            :op1 (p2 / parity
                  :mod (d / demography)
                  :ARG1-of (o / observe-01)
                  :ARG1-of (d2 / depend-01
                        :polarity -))
            :op2 (p3 / parity
                  :mod (d3 / demography)
                  :ARG1-of (c3 / condition-01
                        :polarity -))
            :op3 (e2 / equalize-01
                  :ARG1 (o2 / odds)
                  :ARG1-of (m2 / mean-01
                        :ARG2 (s / separate-01)))
            :op4 (c4 / contrast-01
                  :ARG1 c2
                  :ARG1-of (m3 / mean-01
                        :ARG2 (s2 / suffice-01)))
            :ARG2 (f / fairness
                  :mod (g / group
                        :mod (s3 / sub))))
      :op5 (d4 / discriminate-02
            :ARG1-of (r / resolve-01
                  :polarity -)
            :mod (c5 / casual))
      :op6 (d5 / discriminate-02
            :mod (p4 / proxy))
      :op7 (f2 / fairness
            :mod (m4 / meritocracy))
      :op8 (f3 / fairness
            :mod (c6 / contingency))
      :ARG1-of (d6 / describe-01
            :location (a3 / above))
      :snt3 (s4 / show-01
            :ARG0 (t / table
                  :location (b3 / below))
            :ARG1 (s5 / sit-01
                  :ARG1 (a4 / and
                        :op1 (a5 / approach-02
                              :ARG1-of (u / use-01
                                    :ARG1-of (h / have-degree-91
                                          :ARG2 (c7 / common)
                                          :ARG3 (m5 / most))))
                        :op2 (e3 / example
                              :mod (o3 / other)))
                  :ARG2 (d7 / define-01
                        :ARG1-of (h2 / have-degree-91
                              :ARG2 (w / wide-02
                                    :ARG1 d7)
                              :ARG3 (m6 / more))))))


# ::id 85
# ::snt In human decision-making systems, it is possible to leave a  degree of ambiguity about how fairness is defined.
(p / possible-01
      :ARG1 (l / leave-14
            :ARG1 (a / ambiguity
                  :degree (d / degree)
                  :topic (d2 / define-01
                        :ARG1 (f / fair-01))))
      :location (s / system
            :mod (m / make-01
                  :ARG0 (h / human)
                  :ARG1 (d3 / decide-01))))


# ::id 85
# ::snt If we want a  model to comply with a definition of fairness, we must tell it  explicitly what that definition is.
(o / obligate-01
      :ARG1 (w / we)
      :ARG2 (t / tell-01
            :ARG0 w
            :ARG1 (t2 / thing
                  :ARG2-of (d / define-01
                        :ARG1 (f / fair-01)))
            :ARG2 (m / model)
            :ARG1-of (e / explicit-03))
      :condition (w2 / want-01
            :ARG0 w
            :ARG1 (c / comply-01
                  :ARG0 m
                  :ARG1 t2
                  :ARG2-of (d2 / define-01
                        :ARG1 f))))


# ::id 85
# ::snt Sometimes the meaning of  fairness is very clearly defined; to take an extreme example,  a chess playing AI achieves fairness by following the rules  of the game.
(d / define-01
      :ARG1 (t / thing
            :ARG2-of (m / mean-01
                  :ARG1 (f / fair-01)))
      :ARG1-of (c / clear-06
            :degree (v / very))
      :time (s / sometimes)
      :example (a / achieve-01
            :ARG0 (t2 / thing
                  :ARG1-of (ii / intelligent-01)
                  :ARG0-of (p / play-01
                        :ARG1 (g / game
                              :name (n / name
                                    :op1 "chess")))
                  :mod (a2 / artificial))
            :ARG1 (f2 / fair-01)
            :manner (f3 / follow-02
                  :ARG0 t2
                  :ARG1 (t3 / thing
                        :ARG3-of (r / rule-01
                              :ARG1 g))))
      :mod (e / extreme))


# ::id 85
# ::snt In fact, the trade-offs between mathematical  definitions demonstrate that a model cannot conform to  all possible fairness definitions at the same time.
(d / demonstrate-01
      :ARG0 (t / trade-off-02
            :ARG1 (d2 / define-01
                  :mod (m / mathematics)))
      :ARG1 (p / possible-01
            :polarity -
            :ARG1 (c / conform-01
                  :ARG1 (m2 / model)
                  :ARG2 (d3 / define-01
                        :ARG1 (f / fair-01)
                        :mod (a / all)
                        :ARG1-of (p2 / possible-01))
                  :time (t2 / time
                        :ARG1-of (s / same-01))))
      :mod (ii / in-fact))


# ::id 85
# ::snt Again, we found potential for  algorithms to support decision-making, but this introduces  new issues around the balance between security, privacy  and fairness, and there is a clear requirement for strong  democratic oversight.
(a / and
      :op1 (c / contrast-01
            :ARG1 (f / find-01
                  :ARG0 (w / we)
                  :ARG1 (p / potential
                        :domain (s / support-01
                              :ARG0 (a2 / algorithm)
                              :ARG1 (m / make-01
                                    :ARG1 (d / decide-01))))
                  :mod (a3 / again))
            :ARG2 (ii / introduce-02
                  :ARG0 p
                  :ARG1 (ii2 / issue-02
                        :ARG0 (b / balance-01
                              :ARG1 (s2 / security)
                              :ARG2 (p2 / privacy)
                              :ARG3 (f2 / fairness))
                        :ARG1-of (n / new-01))))
      :op2 (r / require-01
            :ARG1 (o / oversee-01
                  :manner (d2 / democratic)
                  :ARG1-of (s3 / strong-02))
            :ARG1-of (c2 / clear-06)))


# ::id 85
# ::snt At a decision level, Demographic Parity  might mean that the same proportion of men and women applying for loans are successful, but this kind of  fairness can also be applied when assigning risk scores, regardless of where a success threshold is applied.
(c / contrast-01
      :ARG1 (p / possible-01
            :ARG1 (m / mean-01
                  :ARG1 (p2 / parity
                        :mod (d / demography))
                  :ARG2 (s / succeed-01
                        :ARG0 (p3 / proportion
                              :ARG1-of (s2 / same-01)
                              :quant-of (a / and
                                    :op1 (m2 / man)
                                    :op2 (w / woman)
                                    :ARG0-of (a2 / apply-01
                                          :ARG1 (l / loan-01))))))
            :location (l2 / level
                  :mod (d2 / decide-01)))
      :ARG2 (p4 / possible-01
            :ARG1 (a3 / apply-02
                  :ARG1 (f / fairness
                        :mod (k / kind
                              :mod (t / this)))
                  :time (a4 / assign-01
                        :ARG1 (s3 / score-01
                              :ARG3 (r / risk-01)))
                  :ARG1-of (r2 / regardless-91
                        :ARG2 (l3 / location
                              :location-of (a5 / apply-02
                                    :ARG1 (t2 / threshold
                                          :mod (s4 / succeed-01)))))
                  :mod (a6 / also))))


# ::id 85
# ::snt The approaches were  changed in response to public concerns,   and significant criticism about both  individual fairness and concerns that   grades were biased.
(c / change-01
      :ARG1 (a / approach-02)
      :ARG2-of (r / respond-01
            :ARG1 (a2 / and
                  :op1 (c2 / concern-01
                        :ARG1 (p / public))
                  :op2 (c3 / criticize-01
                        :ARG1 (a3 / and
                              :op1 (f / fairness
                                    :mod (ii / individual))
                              :op2 (c4 / concern-01
                                    :ARG0 (b / bias-01
                                          :ARG1 (g / grade))))
                        :ARG1-of (s / significant-02)))))


# ::id 85
# ::snt To improve fairness in decisionmaking, it needs to be as easy as possible for organisations  to identify and address bias.
(n / need-01
      :ARG1 (h / have-degree-91
            :ARG1 (a / and
                  :op1 (ii / identify-01
                        :ARG0 (o / organization)
                        :ARG1 (b / bias-01))
                  :op2 (a2 / address-02
                        :ARG0 o
                        :ARG1 b))
            :ARG2 (e / easy-05
                  :ARG1 a)
            :ARG3 (e2 / equal)
            :ARG4 (p / possible-01
                  :ARG1 a))
      :purpose (ii2 / improve-01
            :ARG1 (f / fairness
                  :topic (m / make-01
                        :ARG1 (d / decide-01)))))


# ::id 85
# ::snt A number of factors are  required to help build algorithmic decision-making tools  and machine learning models with fairness in mind: • Sufficient diversity  in the workforce to understand  potential issues of bias and the problems they cause.
(r / require-01
      :ARG1 (f / factor
            :quant (n / number)
            :ARG2-of (ii / include-91
                  :ARG1 (d / diversity
                        :ARG1-of (s / suffice-01
                              :ARG0 (u / understand-01
                                    :ARG0 (w / workforce)
                                    :ARG1 (a / and
                                          :op1 (ii2 / issue-02
                                                :ARG0 (b / bias-01)
                                                :mod (p / potential))
                                          :op2 (p2 / problem
                                                :ARG1-of (c / cause-01
                                                      :ARG0 ii2))))))))
      :purpose (h / help-01
            :ARG0 f
            :ARG1 (b2 / build-01
                  :ARG1 (a2 / and
                        :op1 (t / tool
                              :instrument-of (m / make-01
                                    :ARG1 (d2 / decide-01)
                                    :manner (a3 / algorithm)))
                        :op2 (m2 / model
                              :instrument-of (l / learn-01
                                    :manner (m3 / machine))))
                  :manner (k / keep-in-mind-08
                        :ARG2 (f2 / fairness)))))


# ::id 85
# ::snt It is always better to acknowledge biases, understand  underlying causes, and address them as far as possible,  but the “correct“ approach for ensuring fairness in an  algorithmic decision-making tool will depend strongly on  use case and context.
(c / contrast-01
      :ARG1 (h / have-degree-91
            :ARG1 (a / and
                  :op1 (a2 / acknowledge-01
                        :ARG1 (b / bias-01))
                  :op2 (u / understand-01
                        :ARG1 (c2 / cause
                              :ARG0-of (u2 / underlie-01)))
                  :op3 (a3 / address-02
                        :ARG1 b
                        :ARG1-of (h2 / have-degree-91
                              :ARG2 (f / far)
                              :ARG3 (e / equal)
                              :ARG4 (p / possible-01
                                    :ARG1 a3))))
            :ARG2 (g / good-02
                  :ARG1 a)
            :ARG3 (m / more)
            :time (a4 / always))
      :ARG2 (d / depend-01
            :ARG0 (a5 / approach-02
                  :ARG1 (e2 / ensure-01
                        :ARG1 (f2 / fair-01
                              :ARG1 (t / tool
                                    :instrument-of (m2 / make-01
                                          :ARG1 (d2 / decide-01)
                                          :manner (a6 / algorithm)))))
                  :ARG1-of (c3 / correct-02))
            :ARG1 (a7 / and
                  :op1 (c4 / case-04
                        :ARG1 (u3 / use-01))
                  :op2 (c5 / context))
            :ARG1-of (s / strong-02)))


# ::id 85
# ::snt What good practice should  a team then follow when seeking to ensure fairness in an  algorithmic decision-making tool?
(r / recommend-01
      :ARG1 (f / follow-02
            :ARG0 (t / team)
            :ARG1 (p / practice-01
                  :ARG1 (a / amr-unknown)
                  :ARG1-of (g / good-02))
            :time (t2 / then)
            :time (s / seek-01
                  :ARG0 t
                  :ARG1 (e / ensure-01
                        :ARG0 t
                        :ARG1 (f2 / fair-01
                              :ARG1 (t3 / tool
                                    :instrument-of (m / make-01
                                          :ARG1 (d / decide-01))
                                    :mod (a2 / algorithm)))))))


# ::id 85
# ::snt Review into bias in algorithmic decision-making: Enabling fair innovation
Centre for Data Ethics and Innovation89138 Kilbertus, N.; Gascon, A.; Censor, M.; Veale, M.; Gummadi, K. P.; and Weller, A.; ‘Blind Justice: Fairness with Encrypted Sensitive Attributes’.
(m / multi-sentence
      :snt1 (r / review-01
            :ARG1 (b / bias-01
                  :ARG1 (m2 / make-01
                        :ARG1 (d / decide-01
                              :mod (a / algorithm))))
            :ARG0-of (e / enable-01
                  :ARG1 (ii / innovate-01
                        :ARG1-of (f / fair-01)))
            :ARG1-of (p / publication-91
                  :ARG4 (o / organization
                        :name (n / name
                              :op1 "Centre"
                              :op2 "for"
                              :op3 "Data"
                              :op4 "Ethics"
                              :op5 "and"
                              :op6 "Initiative"))
                  :time (d2 / date-entity
                        :year 1989
                        :month 13
                        :day 4)))
      :snt2 (a2 / and
            :op1 (p2 / person
                  :name (n2 / name
                        :op1 "Kilbertus")
                  :mod (c / country
                        :name (n3 / name
                              :op1 "Netherlands")))
            :op2 (p3 / person
                  :name (n4 / name
                        :op1 "Gascon")
                  :mod c)
            :op3 (p4 / person
                  :name (n5 / name
                        :op1 "Censor")
                  :mod c)
            :op4 (p5 / person
                  :name (n6 / name
                        :op1 "Veale")
                  :mod c)
            :op5 (p6 / person
                  :name (n7 / name
                        :op1 "Gummadi")
                  :mod c
                  :name (n8 / name
                        :op1 "Korea")))
      :op6 (p7 / person
            :name (n9 / name
                  :op1 "Weller")
            :mod c)
      :snt3 (p8 / publication-91
            :ARG1 (j / justice
                  :ARG1-of (b2 / blind-02)
                  :ARG1-of (m3 / mean-01
                        :ARG2 (f2 / fairness
                              :ARG1-of (e2 / encrypt-01)
                              :accompanier (a3 / attribute
                                    :mod (s / sensitive)))))))


# ::id 85
# ::snt But, in order to assess the  overall outcome (and hence assess the risk of indirect  discrimination), data on protected characteristics is  required.138 There have been calls for wider data collection, reflecting  an acceptance that doing so helps promote fairness and  equality in areas where bias could occur.139 CDEI supports  these calls; we think that greater collection of protected  characteristic data would allow for fairer algorithmic  decision-making in many circumstances.
(m / multi-sentence
      :li 139
      :snt1 (c / contrast-01
            :ARG2 (r / require-01
                  :ARG1 (d / data
                        :ARG2-of (c2 / characteristic-02)
                        :ARG1-of (p / protect-01))
                  :purpose (a / assess-01
                        :ARG1 (o / outcome
                              :mod (o2 / overall))
                        :ARG0-of (c3 / cause-01
                              :ARG1 (a2 / assess-01
                                    :ARG1 (r2 / risk-01
                                          :ARG2 (d2 / discriminate-02
                                                :ARG1-of (d3 / direct-02
                                                      :polarity -))))))))
      :snt2 (c4 / call-03
            :li 134
            :ARG1 (c5 / collect-01
                  :ARG1 (d4 / data)
                  :ARG1-of (h / have-degree-91
                        :ARG2 (w / wide-02
                              :ARG1 c5)
                        :ARG3 (m2 / more)))
            :ARG1-of (r3 / reflect-01
                  :ARG2 (a3 / accept-01
                        :ARG1 (h2 / help-01
                              :ARG0 c5
                              :ARG1 (p2 / promote-02
                                    :ARG0 c5
                                    :ARG1 (a4 / and
                                          :op1 (f / fairness)
                                          :op2 (e / equal-01)
                                          :location (a5 / area
                                                :location-of (p3 / possible-01
                                                      :ARG1 (b / bias)))))))))
      :snt3 (t / think-01
            :ARG0 (w2 / we)
            :ARG1 (a6 / allow-01
                  :ARG0 (c6 / collect-01
                        :ARG1 (d5 / data
                              :ARG2-of (c7 / characteristic-02)
                              :ARG1-of (p4 / protect-01))
                        :ARG1-of (h3 / have-degree-91
                              :ARG2 (g / great)
                              :ARG3 (m3 / more)))
                  :ARG1 (d6 / decide-01
                        :manner (a7 / algorithm))
                  :time (c8 / circumstance
                        :quant (m4 / many)))))


# ::id 85
# ::snt Notably,  this provision also specifically mentions equality rather  than discrimination, which allows for this data to address  broader fairness and equality considerations rather than  just discrimination as defined by equality or human   rights law.
(m / mention-01
      :ARG0 (p / provision
            :mod (t / this))
      :ARG1 (ii / instead-of-91
            :ARG1 (e / equal-01)
            :ARG2 (d / discriminate-02))
      :ARG1-of (s / specific-02)
      :ARG0-of (a / allow-01
            :ARG1 (a2 / address-02
                  :ARG0 (d2 / data
                        :mod t)
                  :ARG1 (c / consider-02
                        :ARG1 (a3 / and
                              :op1 (f / fairness)
                              :op2 (e2 / equal-01))
                        :ARG1-of (ii2 / instead-of-91
                              :ARG2 (d3 / discriminate-02
                                    :mod (j / just)
                                    :ARG1-of (d4 / define-01
                                          :ARG0 (o / or
                                                :op1 e2
                                                :op2 (l / law
                                                      :topic (r / right-05
                                                            :ARG1 (h / human)))))))
                        :ARG1-of (h2 / have-degree-91
                              :ARG2 (b / broad-02
                                    :ARG1 c)
                              :ARG3 (m2 / more)))))
      :ARG1-of (n / notable-04)
      :mod (a4 / also))


# ::id 85
# ::snt Without guidance or the PSED, private sector  organisations have to manage different expectations  from customers, employees, investors and the public  about how to measure and manage the risks of  algorithmic bias.There are also concerns about balancing the trade-off  between fairness and privacy.
(m / multi-sentence
      :snt1 (o / obligate-01
            :ARG1 (o2 / organization
                  :mod (s / sector
                        :ARG1-of (p / private-03)))
            :ARG2 (m2 / manage-01
                  :ARG0 o2
                  :ARG1 (e / expect-01
                        :ARG0 (a / and
                              :op1 (c / customer)
                              :op2 (p2 / person
                                    :ARG1-of (e2 / employ-01))
                              :op3 (p3 / person
                                    :ARG0-of (ii / invest-01))
                              :op4 (p4 / public))
                        :ARG1 (t / thing
                              :manner-of (a2 / and
                                    :op1 (m3 / measure-01
                                          :ARG0 o2
                                          :ARG1 (r / risk-01
                                                :ARG2 (b / bias-01
                                                      :mod (a3 / algorithm))))
                                    :op2 (m4 / manage-01
                                          :ARG0 o2
                                          :ARG1 r)))
                        :ARG1-of (d / differ-02)))
            :manner (o3 / or
                  :op1 (g / guide-01
                        :polarity -)
                  :op2 t
                  :polarity -
                  :name (n / name
                        :op1 "PSED")))
      :snt2 (c2 / concern-01
            :ARG0 (b2 / balance-01
                  :ARG1 (t2 / trade-off-02
                        :ARG1 (f / fairness)
                        :ARG2 (p5 / privacy)))
            :mod (a4 / also)))


# ::id 85
# ::snt 146 Kilbertus, N., Gascon, A., Kusner, M., Veale, M., Gummadi, K. P., and Weller, A., ‘Blind Justice: Fairness with Encrypted Sensitive Attributes’.
(p / publication-91
      :li 146
      :ARG0 (a / and
            :op1 (p2 / person
                  :name (n / name
                        :op1 "Kilbertus")
                  :mod (n2 / north))
            :op2 (p3 / person
                  :name (n3 / name
                        :op1 "Gascon")
                  :mod (s / south))
            :op3 (p4 / person
                  :name (n4 / name
                        :op1 "Kusner")
                  :mod n2))
      :op4 (p5 / person
            :name (n5 / name
                  :op1 "Veale")
            :mod s)
      :op5 (p6 / person
            :name (n6 / name
                  :op1 "Gummadi")
            :mod n2)
      :op6 (p7 / person
            :name (n7 / name
                  :op1 "Weller")
            :mod s)
      :ARG1 (p8 / publication
            :name (n8 / name
                  :op1 "Blind"
                  :op2 "Justice"
                  :op3 ":"
                  :op4 "Fairness"
                  :op5 "with"
                  :op6 "Enchanting"
                  :op7 "Sensitive"
                  :op8 "Attributes")))


# ::id 85
# ::snt In the International Conference on Machine  Learning (ICML), 2018; https://arxiv.org/pdf/1806.03281.pdf 147 ICO, ‘What do we need to do to ensure lawfulness, fairness, and transparency in AI systems’; https://ico.org.uk/for-organisations/guide-to-data-protection/key-data-protection-themes/ guidance-on-ai-and-data-protection/what-do-we-need-to-do-to-ensure-lawfulness-fairness-and-transparency-in-ai-systems/ 148 Wachter, Sandra, and Mittelstadt, Brent; ‘A right to reasonable inferences: Re-thinking data protection law in the age of big data and AI’, Columbia Business Law Review, 2019, p494; https:// www.researchgate.net/publication/328257891_A_Right_to_Reasonable_Inferences_Re-Thinking_Data_Protection_Law_in_the_Age_of_Big_Data_and_AI 149 NHS Research Scotland, ‘Data Safe Haven’; https://www.nhsresearchscotland.org.uk/research-in-scotland/data/safe-havens 150 The Alan Turing Institute, ‘Enabling trust models for differential privacy’, ongoing; https://www.turing.ac.uk/research/research-projects/enabling-trust-models-differential-privacy If a successful model could be developed, private sector companies would be able to  audit their algorithms for bias without individuals being required to hand over their  sensitive data to multiple organisations.
(m / multi-sentence
      :snt1 (p / publication-91
            :ARG0 (a / and
                  :op1 (p2 / person
                        :name (n / name
                              :op1 "Wachter"))
                  :op2 (p3 / person
                        :name (n2 / name
                              :op1 "Mittelstadt"))
                  :op3 (p4 / person
                        :name (n3 / name
                              :op1 "Brent")))
            :ARG1 (p5 / publication
                  :name (n4 / name
                        :op1 "A"
                        :op2 "Right"
                        :op3 "to"
                        :op4 "Reasonable_Inferences_Re-Thinking_Data_Protection_Law_in_the_Age_of_Big_Data_and_AI"))
            :ARG4 (c / conference
                  :name (n5 / name
                        :op1 "International"
                        :op2 "Conference"
                        :op3 "on"
                        :op4 "Machine"
                        :op5 "Learning")
                  :time (d / date-entity
                        :year 2018))
            :ARG6 (p6 / publication
                  :name (n6 / name
                        :op1 "The"
                        :op2 "Alan"
                        :op3 "Turing"
                        :op4 "Institute"))
            :ARG7 (p7 / publication
                  :name (n7 / name
                        :op1 "Enabling"
                        :op2 "Trust"
                        :op3 "Model"
                        :op4 "for"
                        :op5 "Difference"
                        :op6 "Privacy"))
            :ARG8 (u / url-entity
                  :value "https://www.researchgate.org/research/research-projects/enabling-trust-models-differential-privacy.pdf"))
      :snt2 (p8 / possible-01
            :ARG1 (a2 / audit-01
                  :ARG0 (c2 / company
                        :mod (s / sector
                              :ARG1-of (p9 / private-03)))
                  :ARG1 (a3 / algorithm
                        :poss c2)
                  :ARG2 (b / bias-01
                        :ARG1 a3)
                  :manner (r / require-01
                        :polarity -
                        :ARG1 (h / hand-over-02
                              :ARG0 (ii / individual)
                              :ARG1 (d2 / data
                                    :poss ii
                                    :ARG0-of (s2 / sensitive-03))
                              :ARG2 (o / organization
                                    :quant (m2 / multiple)))
                        :ARG2 ii))))


# ::id 85
# ::snt Some  organisations are uncertain of how they should approach  issues of fairness, including associated reputational, legal  and commercial issues.
(k / know-01
      :polarity -
      :ARG0 (o / organization
            :quant (s / some))
      :ARG1 (r / recommend-01
            :ARG1 (a / approach-02
                  :ARG0 o
                  :ARG1 (ii / issue-02
                        :ARG0 (f / fairness)
                        :ARG2-of (ii2 / include-01
                              :ARG1 (a2 / and
                                    :op1 (ii3 / issue-02
                                          :ARG0 (r2 / repute-01)
                                          :ARG1-of (a3 / associate-01))
                                    :op2 (ii4 / issue-02
                                          :ARG0 (l / legal-02))
                                    :op3 (ii5 / issue-02
                                          :ARG0 (c / commerce))))))))


# ::id 85
# ::snt There is a high level of focus  on this area in the academic literature,  and an increasing number of practical  algorithmic fairness tools have appeared in  the last three years.
(a / and
      :op1 (f / focus-01
            :ARG1 (a2 / area
                  :mod (t / this))
            :ARG2 (l / literature
                  :mod (a3 / academia))
            :degree (l2 / level
                  :ARG1-of (h / high-02)))
      :op2 (a4 / appear-01
            :ARG1 (t2 / tool
                  :purpose (f2 / fairness
                        :mod (a5 / algorithm))
                  :ARG1-of (p / practical-03)
                  :quant (n / number
                        :ARG1-of (ii / increase-01)))
            :time (b / before
                  :op1 (n2 / now)
                  :duration (t3 / temporal-quantity
                        :quant 3
                        :unit (y / year)))))


# ::id 85
# ::snt Had that medical school been equipped with a current  understanding of how to assess an algorithm for bias, and  been motivated to do so, perhaps they would have been  able to use their algorithm to reduce bias rather than  propagate it.For most organisations, bias monitoring and analysis are a necessary part of their decisionmaking (whether algorithmic or not).Review into bias in algorithmic decision-making: Enabling fair innovation
Centre for Data Ethics and Innovation98166 Note that in the machine learning literature on fairness, some terms used throughout this report take on specific, often narrower, definitions.
(m / multi-sentence
      :snt1 (a / and
            :op1 (r / review-01
                  :ARG1 (b / bias
                        :location (m2 / make-01
                              :ARG1 (d / decide-01)
                              :manner (a2 / algorithm))))
            :op2 (e / enable-01
                  :ARG1 (ii / innovate-01
                        :ARG1-of (f / fair-01)))
            :ARG1-of (p / publication-91
                  :ARG4 (o / organization
                        :name (n / name
                              :op1 "Centre"
                              :op2 "for"
                              :op3 "Data"
                              :op4 "Ethics"
                              :op5 "and"
                              :op6 "Innovation"))
                  :time (d2 / date-entity
                        :year 1998
                        :month 4
                        :day 16)))
      :snt2 (p2 / possible-01
            :ARG1 (u / use-01
                  :ARG0 (s / school
                        :mod (m3 / medicine)
                        :mod (t / that))
                  :ARG1 (a3 / algorithm)
                  :ARG2 (r2 / reduce-01
                        :ARG0 s
                        :ARG1 (b2 / bias)
                        :ARG1-of (ii2 / instead-of-91
                              :ARG2 (p3 / propagate-01
                                    :ARG0 s
                                    :ARG1 b2))))
            :condition (e2 / equip-01
                  :ARG1 s
                  :ARG2 (u2 / understand-01
                        :ARG1 (t2 / thing
                              :manner-of (a4 / assess-01
                                    :ARG0 s
                                    :ARG1 (b3 / bias)))
                        :time (c / current))))
      :snt3 (n2 / note-01
            :mode imperative
            :ARG0 (y / you)
            :ARG1 (t3 / take-on-09
                  :ARG0 (t4 / term
                        :quant (s2 / some)
                        :ARG1-of (u3 / use-01
                              :ARG2 (t5 / thing
                                    :ARG1-of (r3 / report-01)
                                    :mod (t6 / this))))
                  :ARG1 (d3 / define-01
                        :ARG1-of (s3 / specific-02)
                        :ARG1-of (h / have-degree-91
                              :ARG2 (n3 / narrow-02
                                    :ARG1 d3)
                              :ARG3 (m4 / more
                                    :frequency (o2 / often))))
                  :location (l / literature
                        :topic (f2 / fair-01)
                        :mod (m5 / machine))))
      :snt4 (n4 / need-01
            :ARG0 (o3 / organization
                  :quant (m6 / most))
            :ARG1 (a5 / and
                  :op1 (m7 / monitor-01
                        :ARG1 (b4 / bias))
                  :op2 (a6 / analyze-01
                        :ARG1 b4))
            :part-of (d4 / decide-01
                  :ARG0 o3
                  :manner (o4 / or
                        :op1 (a7 / algorithm)
                        :op2 (a8 / algorithm
                              :polarity -)))))


# ::id 85
# ::snt The field of study of how to create a mathematical system that is unbiased, is called “algorithmic fairness”.
(c / call-01
      :ARG1 (f / field
            :ARG1-of (s / study-01)
            :topic (t / thing
                  :manner-of (c2 / create-01
                        :ARG1 (s2 / system
                              :mod (m / mathematics)
                              :ARG1-of (b / bias-01
                                    :polarity -)))))
      :ARG2 (f2 / fairness
            :mod (a / algorithm)))


# ::id 85
# ::snt In this report we use  “discrimination” and “bias” in the common language sense as defined in Chapter 2 (rather than their statistical meanings), and note that the concept of “fairness” discussed in this section is  narrower than that described above.Statistical definitions of fairness  If we want model development to include  a definition of fairness, we must tell the  relevant model what that definition is, and  then measure it.
(m / multi-sentence
      :snt1 (a / and
            :op1 (u / use-01
                  :ARG0 (w / we)
                  :ARG1 (a2 / and
                        :op1 (d / discriminate-02)
                        :op2 (b / bias-01))
                  :manner (s / sense
                        :mod (l / language
                              :mod (c / common))
                        :ARG1-of (d2 / define-01
                              :ARG0 (c2 / chapter
                                    :mod 2))
                        :ARG1-of (ii / instead-of-91
                              :ARG2 (m2 / mean-01
                                    :ARG1 a2
                                    :ARG3 (s2 / statistical))))
                  :location (r / report
                        :mod (t / this)))
            :op2 (n / note-01
                  :ARG0 w
                  :ARG1 (h / have-degree-91
                        :ARG1 (c3 / concept
                              :mod (f / fairness)
                              :ARG1-of (d3 / discuss-01
                                    :ARG0 (s3 / section
                                          :mod (t2 / this))))
                        :ARG2 (n2 / narrow-02
                              :ARG1 c3)
                        :ARG3 (m3 / more)
                        :ARG4 (t3 / that
                              :ARG1-of (d4 / describe-01
                                    :location (a3 / above))))))
      :snt2 (h2 / have-condition-91
            :ARG1 (o / obligate-01
                  :ARG1 (a4 / and
                        :op1 (t4 / tell-01
                              :ARG0 (w2 / we)
                              :ARG1 (t5 / thing
                                    :ARG2-of (d5 / define-01
                                          :ARG1 (f2 / fairness))
                                    :ARG1-of (r2 / relevant-01))
                              :ARG2 (m4 / model))
                        :op2 (m5 / measure-01
                              :ARG0 w2
                              :ARG1 t5
                              :time (t6 / then)))
                  :ARG2 (w3 / want-01
                        :ARG0 w2
                        :ARG1 (ii2 / include-01
                              :ARG1 t5
                              :ARG2 (d6 / develop-02
                                    :ARG1 m4))))))


# ::id 85
# ::snt There is, however, no single mathematical definition  of fairness that can apply to all contexts.166 As a result,  the academic literature has seen dozens of competing  notions of fairness introduced, each with their own  merits and drawbacks, and many different terminologies  for categorising these notions, none of which are  complete.
(m / multi-sentence
      :snt1 (c / contrast-01
            :ARG2 (t / thing
                  :polarity -
                  :ARG2-of (d / define-01
                        :ARG1 (f / fairness)
                        :mod (m2 / mathematics))
                  :ARG1-of (s / single-02)
                  :ARG1-of (a / apply-02
                        :ARG2 (c2 / context
                              :mod (a2 / all))
                        :ARG1-of (p / possible-01))))
      :snt2 (c3 / cause-01
            :li166 (s2 / see-01
                  :ARG0 (l / literature
                        :mod (a3 / academia))
                  :ARG1 (ii / introduce-02
                        :ARG1 (n / notion
                              :ARG0-of (c4 / compete-01)
                              :quant (m3 / multiple
                                    :op1 12)
                              :ARG0-of (h / have-03
                                    :ARG1 (a4 / and
                                          :op1 (m4 / merit)
                                          :op2 (d2 / drawback)
                                          :poss n))
                              :ARG2-of (c5 / categorize-01
                                    :ARG1 (n2 / notion
                                          :ARG1-of (c6 / complete-02
                                                :polarity -)))
                              :ARG0-of h
                              :ARG1 (t2 / terminology
                                    :ARG1-of (d3 / differ-02)
                                    :quant (m5 / many)))))))


# ::id 85
# ::snt Ultimately, humans must choose which  notions of fairness an algorithm will work to, taking  wider notions and considerations into account, and  recognising that there will always be aspects of  fairness outside of any statistical definition.
(o / obligate-01
      :ARG1 (h / human)
      :ARG2 (c / choose-01
            :ARG0 h
            :ARG1 (h2 / have-manner-91
                  :ARG1 (w / work-09
                        :ARG0 (a / algorithm)
                        :ARG1 (n / notion
                              :topic (f / fairness)))
                  :ARG2 (a2 / and
                        :op1 (t / take-into-account-04
                              :ARG0 h
                              :ARG1 (a3 / and
                                    :op1 (n2 / notion)
                                    :op2 (c2 / consideration)
                                    :ARG1-of (h3 / have-degree-91
                                          :ARG2 (w2 / wide-02
                                                :ARG1 n2)
                                          :ARG3 (m / more))))
                        :op2 (r / recognize-02
                              :ARG0 h
                              :ARG1 (a4 / aspect
                                    :topic (f2 / fairness)
                                    :time (a5 / always)
                                    :location (o2 / outside
                                          :op1 (d / define-01
                                                :ARG0 (s / statistic
                                                      :mod (a6 / any)))))))))
      :mod (u / ultimate))


# ::id 85
# ::snt Fairness definitions can be grouped by notion of  fairness sought and stage of development involved.
(p / possible-01
      :ARG1 (g / group-01
            :ARG1 (d / define-01
                  :ARG1 (f / fairness))
            :ARG2 (a / and
                  :op1 (n / notion
                        :topic (f2 / fairness
                              :ARG1-of (s / seek-01)))
                  :op2 (s2 / stage
                        :mod (d2 / develop-01)
                        :ARG1-of (ii / involve-01)))))


# ::id 85
# ::snt In  the first instance, these fall into the broad categories of  procedural and outcome fairness  discussed in Section  2.5.
(f / fall-04
      :ARG1 (t / this)
      :ARG2 (c / category
            :ARG1-of (b / broad-02)
            :mod (f2 / fairness
                  :mod (p / procedure)
                  :mod (o / outcome))
            :ARG1-of (d / discuss-01
                  :location (s / section
                        :mod 2.5)))
      :prep-in (ii / instance
            :ord (o2 / ordinal-entity
                  :value 1)))


# ::id 85
# ::snt Within the technical aspects of machine learning,  procedural fairness approaches often concern the  information used by a system, and thus include “Fairness  Through Unawareness”, which is rarely an effective  strategy.
(c / concern-02
      :ARG0 (a / approach-02
            :ARG1 (f / fairness
                  :mod (p / procedure)))
      :ARG1 (ii / information
            :ARG1-of (u / use-01
                  :ARG0 (s / system)))
      :frequency (o / often)
      :ARG0-of (c2 / cause-01
            :ARG1 (ii2 / include-01
                  :ARG1 (f2 / fairness
                        :manner (a2 / awareness
                              :polarity -)
                        :ARG1-of (s2 / strategy-00
                              :ARG0-of (e / effective-04)
                              :ARG1-of (r / rare-02)))
                  :ARG2 a))
      :location (a3 / aspect
            :mod (t / technical)
            :poss (l / learn-01
                  :manner (m / machine))))


# ::id 85
# ::snt The statistical concept of fairness as applied  to algorithms is then focused on achieving unbiased  outcomes, rather than other concepts of fairness.
(f / focus-01
      :ARG1 (c / concept
            :mod (s / statistical)
            :topic (f2 / fair-01)
            :ARG1-of (a / apply-02
                  :ARG2 (a2 / algorithm)))
      :ARG2 (a3 / achieve-01
            :ARG1 (o / outcome
                  :ARG1-of (b / bias-01
                        :polarity -))
            :ARG1-of (ii / instead-of-91
                  :ARG2 (c2 / concept
                        :mod (o2 / other)
                        :topic (f3 / fair-01))))
      :time (t / then))


# ::id 85
# ::snt Within Outcome Fairness we can make additional  distinctions, between Causal and Observational notions  of fairness, as well as Individual and Group notions.
(p / possible-01
      :ARG1 (d / distinguish-01
            :ARG0 (w / we)
            :ARG2 (a / and
                  :op1 (n / notion
                        :topic (f / fairness)
                        :mod (r / responsible-01))
                  :op2 (n2 / notion
                        :topic (f2 / fairness)
                        :mod (o / observe-01))
                  :op3 (n3 / notion
                        :topic (ii / individual))
                  :op4 (n4 / notion
                        :topic (g / group)))
            :mod (a2 / additional))
      :topic (f3 / fairness
            :mod (o2 / outcome)))


# ::id 85
# ::snt It is important to recognise  that the growing literature and toolsets on algorithmic  fairness often only address part of the issue (that which  can be quantified), and wider interventions to promote  fairness and equality remain key to success.
(ii / important-01
      :ARG1 (r / recognize-02
            :ARG1 (a / and
                  :op1 (a2 / address-02
                        :ARG0 (a3 / and
                              :op1 (l / literature
                                    :ARG1-of (g / grow-01)
                                    :topic (f / fairness
                                          :mod (a4 / algorithmic)))
                              :op2 (t / toolset
                                    :topic f))
                        :ARG1 (p / part
                              :part-of (ii2 / issue-02)
                              :ARG1-of (q / quantify-01
                                    :ARG1-of (p2 / possible-01)))
                        :frequency (o / often)
                        :mod (o2 / only))
                  :op2 (r2 / remain-01
                        :ARG1 (ii3 / intervene-01
                              :ARG1 (p3 / promote-02
                                    :ARG1 (a5 / and
                                          :op1 (f2 / fairness)
                                          :op2 (e / equal-01)))
                              :ARG1-of (h / have-degree-91
                                    :ARG2 (w / wide-02
                                          :ARG1 ii3)
                                    :ARG3 (m / more)))
                        :ARG3 (k / key-02
                              :ARG1 ii3
                              :ARG2 (s / succeed-01))))))


# ::id 85
# ::snt Ultimately, humans must choose which  notions of fairness an algorithm will work to, taking wider notions and considerations  into account, and recognising that there will always be aspects of fairness outside of any statistical definition.Review into bias in algorithmic decision-making: Enabling fair innovation
Centre for Data Ethics and Innovation99An example of how these different definitions play out in practice can be seen in the US criminal justice system, as per the  following case study.Group Individual • Demographic parity   - outcomes for different protected groups are equally distributed, and statistically  independent.
(m / multi-sentence
      :snt1 (o / obligate-01
            :ARG1 (h / human)
            :ARG2 (c / choose-01
                  :ARG0 h
                  :ARG1 (h2 / have-manner-91
                        :ARG1 (w / work-09
                              :ARG0 (a / algorithm)
                              :ARG1 (n / notion
                                    :topic (f / fairness)))
                        :ARG2 (a2 / and
                              :op1 (t / take-into-account-04
                                    :ARG0 h
                                    :ARG1 (a3 / and
                                          :op1 (n2 / notion
                                                :ARG1-of (h3 / have-degree-91
                                                      :ARG2 (w2 / wide-02
                                                            :ARG1 n2)
                                                      :ARG3 (m2 / more)))
                                          :op2 (c2 / consider-02
                                                :ARG1 n2)))
                              :op2 (r / recognize-02
                                    :ARG0 h
                                    :ARG1 (a4 / aspect
                                          :topic (f2 / fairness)
                                          :location (o2 / outside
                                                :op1 (d / define-01
                                                      :mod (s / statistical)
                                                      :mod (a5 / any)))
                                          :time (a6 / always))))))
            :mod (u / ultimate))
      :snt2 (e / equal-01
            :ARG1 (o3 / outcome
                  :poss (g / group
                        :ARG1-of (p / protect-01)
                        :ARG1-of (d2 / differ-02
                              :ARG1 (g2 / group
                                    :mod (ii / individual))))))
      :snt3 (o4 / organization
            :name (n3 / name
                  :op1 "Centre"
                  :op2 "for"
                  :op3 "Data"
                  :op4 "Ethics"
                  :op5 "and"
                  :op6 "Innovation"
                  :op7 "99"))
      :snt4 (p2 / possible-01
            :ARG1 (s2 / see-01
                  :ARG1 (t2 / thing
                        :manner-of (p3 / play-out-03
                              :ARG1 (d3 / define-01
                                    :ARG1-of d2))
                        :ARG1-of (p4 / practice-01)))
            :location (s3 / system
                  :mod (j / justice
                        :ARG0-of (c3 / criminal-03))
                  :mod (c4 / country
                        :name (n4 / name
                              :op1 "US")))
            :ARG1-of (f3 / follow-01
                  :ARG2 (s4 / study-01
                        :mod (c5 / case)))))


# ::id 85
# ::snt • Putting structures in place to gather data and  monitor outcomes for fairness.
(p / put-03
      :ARG1 (s / structure)
      :ARG2 (ii / in-place)
      :purpose (a / and
            :op1 (g / gather-01
                  :ARG0 s
                  :ARG1 (d / data))
            :op2 (m / monitor-01
                  :ARG0 s
                  :ARG1 (o / outcome)
                  :purpose (f / fairness))))


# ::id 85
# ::snt Most decision processes (whether using algorithms or not) exhibit bias in some form  and will fail certain tests of fairness.
(a / and
      :op1 (e / exhibit-01
            :ARG0 (p / process-02
                  :ARG1 (d / decide-01)
                  :quant (m / most))
            :ARG1 (b / bias-01
                  :ARG1 p
                  :mod (f / form
                        :mod (s / some)))
            :ARG1-of (r / regardless-91
                  :ARG2 (o / or
                        :op1 (u / use-01
                              :ARG1 (a2 / algorithm))
                        :op2 (u2 / use-01
                              :polarity -
                              :ARG1 (a3 / algorithm)))))
      :op2 (f2 / fail-01
            :ARG1 p
            :ARG2 (t / test-01
                  :ARG2 (f3 / fair-01)
                  :mod (c / certain))))


# ::id 85
# ::snt How should fairness have been interpreted in this case?
(r / recommend-01
      :ARG1 (ii / interpret-01
            :ARG1 (f / fairness)
            :ARG2 (a / amr-unknown)
            :prep-in (c / case-04
                  :ARG1 (t / this))))


# ::id 85
# ::snt There were a number of different notions of fairness to  consider, including:23 • Fairness between year groups: Achieve a similar  distribution of grades to previous and future year  groups.
(ii / include-01
      :li 23
      :ARG1 (f / fair-01
            :mod (b / between
                  :op1 (g / group
                        :mod (y / year)))
            :ARG1-of (m / mean-01
                  :ARG2 (a / achieve-01
                        :ARG1 (d / distribute-01
                              :ARG1 (g2 / grade)
                              :ARG1-of (r / resemble-01
                                    :ARG2 (a2 / and
                                          :op1 (g3 / group
                                                :time (y2 / year
                                                      :mod (p / previous)))
                                          :op2 (g4 / group
                                                :time (f2 / future))))))))
      :ARG2 (n / notion
            :ARG1-of (d2 / differ-02)
            :topic (f3 / fair-01)
            :ARG1-of (c / consider-02)
            :quant (n2 / number)))


# ::id 85
# ::snt • Group fairness between different schools: Attempt   to standardise teacher assessed grades, given the  different levels of strictness/optimism in grading  between different schools to be fair to individual  students from different schools.
(m / multi-sentence
      :snt1 (f / fairness
            :mod (g / group)
            :location (b / between
                  :op1 (s / school
                        :ARG1-of (d / differ-02))))
      :snt2 (a / attempt-01
            :ARG1 (s2 / standardize-01
                  :ARG1 (g2 / grade-01
                        :ARG1 (p / person
                              :ARG0-of (t / teach-01))
                        :ARG1-of (a2 / assess-01)))
            :ARG1-of (c / cause-01
                  :ARG0 (l / level
                        :ARG1-of (d2 / differ-02)
                        :mod (s3 / slash
                              :op1 (s4 / strict)
                              :op2 (o / optimism))
                        :topic (g3 / grade-01)
                        :location (b2 / between
                              :op1 (s5 / school
                                    :ARG1-of (d3 / differ-02)))
                        :ARG0-of (f2 / fair-01
                              :ARG2 (p2 / person
                                    :ARG0-of (s6 / study-01)
                                    :mod (ii / individual)
                                    :source s5))))))


# ::id 85
# ::snt • Group fairness and discrimination: Avoid exacerbating  differences in outcomes correlated with protected  characteristics; particularly sex and race.
(a / and
      :op1 (f / fair-01
            :ARG1 (g / group))
      :op2 (d / discriminate-02
            :ARG1 g)
      :ARG1-of (m / mean-01
            :ARG2 (a2 / avoid-01
                  :ARG1 (e / exacerbate-01
                        :ARG1 (d2 / differ-02
                              :ARG1 (o / outcome)
                              :ARG1-of (c / correlate-01
                                    :ARG2 (t / thing
                                          :ARG2-of (c2 / characteristic-02)
                                          :ARG1-of (p / protect-01)
                                          :example (a3 / and
                                                :op1 (s / sex)
                                                :op2 (r / race)
                                                :mod (p2 / particular)))))))))


# ::id 85
# ::snt How should fairness have been interpreted in this case?
(r / recommend-01
      :ARG1 (ii / interpret-01
            :ARG1 (f / fairness)
            :ARG2 (a / amr-unknown)
            :prep-in (c / case-04
                  :ARG1 (t / this))))


# ::id 85
# ::snt This principle sets out some core terms for what  we mean by fairness in an algorithmic decisionmaking process.
(s / set-out-06
      :ARG0 (p / principle
            :mod (t / this))
      :ARG1 (t2 / term
            :ARG1-of (c / core-02)
            :quant (s2 / some)
            :ARG2-of (m / mean-01
                  :ARG0 (w / we)
                  :ARG1 (f / fairness
                        :part-of (p2 / process-02
                              :ARG1 (d / decide-01
                                    :manner (a / algorithm)))))))


# ::id 85
# ::snt In Chapter 8  we consider the ability of existing UK  legal and regulatory structures to ensure fairness  in this area, especially data protection and equality  legislation, and how they will need to evolve to adapt  to an algorithmic world.
(c / consider-02
      :ARG0 (w / we)
      :ARG1 (a / and
            :op1 (p / possible-01
                  :ARG1 (e / ensure-01
                        :ARG0 (a2 / and
                              :op1 (s / structure
                                    :ARG1-of (l / legal-02))
                              :op2 (s2 / structure
                                    :ARG0-of (r / regulate-01))
                              :ARG1-of (e2 / exist-01)
                              :mod (c2 / country
                                    :name (n / name
                                          :op1 "UK")))
                        :ARG1 (f / fair-01
                              :location (a3 / area
                                    :mod (t / this)
                                    :ARG2-of (ii / include-91
                                          :ARG1 (a4 / and
                                                :op1 (p2 / protect-01
                                                      :ARG1 (d / data))
                                                :op2 (l2 / legislate-01
                                                      :ARG1 (e3 / equal-01))
                                                :mod (e4 / especially)))))))
            :op2 (n2 / need-01
                  :ARG0 a2
                  :ARG1 (e5 / evolve-01
                        :ARG1 a2
                        :purpose (a5 / adapt-01
                              :ARG1 a2
                              :ARG2 (w2 / world
                                    :mod (a6 / algorithm))))))
      :location (c3 / chapter
            :mod 8))


# ::id 85
# ::snt Organisations that are introducing algorithms into  decisions that were previously purely made by humans  should be looking to achieve at least equivalent  standards of fairness, accountability and transparency,  and in many cases should look to do better.
(r / recommend-01
      :ARG1 (a / and
            :op1 (l / look-04
                  :ARG0 (o / organization
                        :ARG0-of (ii / introduce-02
                              :ARG1 (a2 / algorithm)
                              :ARG2 (d / decide-01
                                    :ARG1-of (m / make-01
                                          :ARG0 (h / human)
                                          :ARG1-of (p / pure-02)
                                          :time (p2 / previous)))))
                  :ARG1 (a3 / achieve-01
                        :ARG0 o
                        :ARG1 (s / standard-02
                              :ARG1 (a4 / and
                                    :op1 (f / fairness)
                                    :op2 (a5 / accountable-02)
                                    :op3 (t / transparency))
                              :ARG1-of (e / equivalent-00
                                    :mod (a6 / at-least)))))
            :op2 (l2 / look-04
                  :ARG0 o
                  :ARG1 (d2 / do-02
                        :ARG0 o
                        :ARG1 (g / good-02
                              :ARG2-of (h2 / have-degree-91
                                    :ARG1 d2
                                    :ARG3 (m2 / more))))
                  :time (c / case-04
                        :quant (m3 / many)))))


# ::id 85
# ::snt • While it is important to test the outputs of algorithms  and assess their fairness, the key measure of the  fairness of an algorithm is the impact it has on the  whole decision process.
(c / contrast-01
      :ARG1 (ii / important-01
            :ARG1 (a / and
                  :op1 (t / test-01
                        :ARG1 (o / output
                              :poss (a2 / algorithm)))
                  :op2 (a3 / assess-01
                        :ARG1 (f / fair-01
                              :ARG1 o))))
      :ARG2 (m / measure-01
            :ARG1 (f2 / fair-01
                  :ARG1 a2)
            :ARG2 (ii2 / impact-01
                  :ARG0 a2
                  :ARG1 (p / process-02
                        :ARG1 (d / decide-01)
                        :mod (w / whole)))
            :ARG1-of (k / key-02)))


# ::id 85
# ::snt In some cases, resolving  fairness issues may only be possible outside of the  actual decision-making process, e.g.
(p / possible-01
      :ARG1 (r / resolve-01
            :ARG1 (ii / issue-02
                  :ARG0 (f / fairness))
            :manner (o / outside
                  :op1 (p2 / process-02
                        :ARG1 (d / decide-01)
                        :ARG1-of (a / actual-02)
                        :example (e / example))))
      :mod (o2 / only)
      :time (c / case-04
            :mod (s / some)))


# ::id 85
# ::snt (see Section 7.2) Recent focus on data protection (due to the arrival of GDPR), and especially privacy  and security aspects of this, risks de-prioritisation of fairness and equality issues (even  though these are also required in GDPR).
(s / see-01
      :ARG0 (y / you)
      :ARG1 (s2 / section
            :mod 7.2
            :topic (r / risk-01
                  :ARG0 (f / focus-01
                        :ARG2 (a / and
                              :op1 (p / protect-01
                                    :ARG1 (d / data))
                              :op2 (a2 / aspect
                                    :topic (a3 / and
                                          :op1 (p2 / privacy)
                                          :op2 (s3 / security))
                                    :mod (e / especially)
                                    :poss p))
                        :time (r2 / recent)
                        :ARG1-of (c / cause-01
                              :ARG0 (a4 / arrive-01
                                    :ARG1 (l / law
                                          :name (n / name
                                                :op1 "GDPR")))))
                  :ARG2 (p3 / prioritize-01
                        :polarity -
                        :ARG1 (ii / issue-02
                              :ARG0 (a5 / and
                                    :op1 (f2 / fairness)
                                    :op2 (e2 / equality)))
                        :concession (r3 / require-01
                              :ARG0 l
                              :ARG1 a5
                              :mod (a6 / also))))))


# ::id 85
# ::snt Humans  ‘over the loop’ monitoring the fairness of the whole  decision process are also needed, with responsibility  for the whole process.
(n / need-01
      :ARG1 (m / monitor-01
            :ARG0 (h / human
                  :mod (o / over-the-loop))
            :ARG1 (f / fair-01
                  :ARG1 (p / process-02
                        :ARG1 (d / decide-01)
                        :mod (w / whole))))
      :mod (a / also)
      :prep-with (r / responsible-03
            :ARG0 h
            :ARG1 p))


